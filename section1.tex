\section{序論}
\subsection{背景}
音声は基本的なコミュニケーション手段として，人々の日常生活において重要な役割を果たしている．しかし、癌などの病気で喉頭を摘出すると、声帯振動による音声生成が不可能になり、従来の発声手段を失ってしまう。このような場合の代用音声手法として、電気式人工喉頭や食道発声、シャント発声がある。しかし、これらには人工的な音声になる、習得に訓練が必要である、器具の交換のため定期的な手術を要するなどの課題がある。そこで本研究では，新たな代用音声手法として，ビデオカメラで撮影された口唇動画を入力として音声を合成する、深層学習モデルを利用したアプローチを提案する。この手法により，訓練や手術を必要とせずとも，自然な声でのコミュニケーションを可能にすることを目指す．

従来の動画音声合成では，動画からの予測対象にメルスペクトログラムが選択されることが多かった．例えば，テキスト音声合成で高い品質を達成した系列変換モデル\cite{shen2018natural}を動画音声合成に応用したLip2Wav\cite{prajwal2020learning}や，GAN（Generative Adversarial Network）を活用したVCA-GAN\cite{kim2021lip}，畳み込み層とTransformer\cite{vaswani2017attention}から構成されるConformer\cite{gulati2020conformer}を利用したSVTS\cite{mira2022svts}などがある．これらでは，損失関数としてMAE LossやMSE Lossが用いられ，原音声から計算されるメルスペクトログラムと，予測されたメルスペクトログラムの間の距離を最小化するよう学習が行われた．これに対し，\cite{kim2023lip_multitask}では，メルスペクトログラムは発話内容だけでなく、話者性やイントネーションといった音響的特徴を含むため、発話内容に対する正確な制約を十分に与えられないことを課題として指摘した．この課題に対し，モデルの中間特徴量を用いたテキストの予測によって計算されるCTC（Connectionist Temporal Classification） Lossと，メルスペクトログラムを事前学習済み音声認識モデルによって変換した特徴量におけるMSE Lossを損失に加え，発話内容に関する制約を強化した．これにより，客観評価において従来手法を上回る結果が得られた．これに続き，\cite{choi2023intelligible}では手法\cite{kim2023lip_multitask}が教師ラベルとしてテキストを必要とするため，テキストアノテーションされていないデータで用いることができない点を課題として指摘した．この課題に対し，言語情報を中心とした特徴量として扱うことが可能な，HuBERT\cite{hsu2021hubert}を利用して得られた離散特徴量をテキストに代わる予測対象として導入した．また，モデルから予測されるメルスペクトログラムがノイジーになってしまう課題に対し，言語情報であるHuBERT離散特徴量も入力として情報を補完する，Multi-input Vocoderが合わせて提案された．これにより，客観評価と主観評価の両面において従来手法を上回る結果が得られた．加えて，\cite{choi2023intelligible}ではAVHuBERT\cite{shi2022learning}のFineTuningも検討された．AVHuBERTは，英語動画音声データセットであるLRS3\cite{afouras2018lrs3}と，多言語動画音声データセットであるVoxCeleb2\cite{chung2018voxceleb2}の英語データを用いて，動画と音声の間の複雑な関係をMasked Predictionという自己教師あり学習方法によって学習したモデルである．動画音声合成に対するFineTuningの結果，AVHuBERTを用いない場合と比較して，客観評価指標における改善が確認された．

\subsection{目的}
本研究では，動画音声合成モデルによって得られる合成音声の品質が依然として低く，自然音声に迫る合成音の実現に至っていない点を課題とする．この課題に対し，近年高い精度を達成した，「AVHuBERTを利用したメルスペクトログラムとHuBERT離散特徴量を予測対象とするマルチタスク学習手法」をベースラインとして採用し，この手法を上回る新たなモデルを提案することで、自然音声に迫る合成音声の実現に近づくことを目的とする。

ここで，近年有効性の示された，テキストやHuBERT離散特徴量を利用するマルチタスク学習手法は，動画を入力とするモデルの学習方法に対する工夫だったと考えられる．しかし，口唇動画と発話内容の間に一意な対応がないこと，すなわち，同じような口唇の動きでも場合によって異なる音素となる場合があることが，特に動画音声合成を困難にしていると考える．これに対して本研究では，動画を入力として最終予測値を出力する従来のネットワークとは異なる新たなネットワークを導入し，これらを組み合わせることで精度改善を狙った.

\subsection{本論文の構成}
本論文は本章を含め，全5章から構成される．2章では，音声データを取り扱う上で必要な信号処理について述べる．3章では，動画から音声を予測するために用いた深層学習について述べる．4章では，本研究の提案手法とベースラインとの比較について述べる．これを踏まえ，5章では本研究を通した結論を述べる．
