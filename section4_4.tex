\subsection{考察}
\subsubsection{ネットワークB（Randomized）について}
まず，ネットワークB（Randomized）は客観評価において，ネットワークB（Randomized・Mel-HuB）を上回る性能を示した．これより，ネットワークBへの入力として，HuBERT中間特徴量はメルスペクトログラムとHuBERT離散特徴量に対するロジットを組み合わせた特徴量よりも優れていたと考えられる．HuBERT中間特徴量は，音声波形を入力とする畳み込みエンコーダの出力であり，Masked PredictionにおいてTransformerの入力となる．従って，畳み込みエンコーダは音声波形の系列長を圧縮しつつ，文脈構造を考慮するための情報を効率的に抽出するよう最適化されると考えられる．よって，ネットワークB（Randomized）におけるHuBERT Transformerはランダム初期化されてはいたものの，HuBERT中間特徴量自体が文脈的構造の考慮に適した情報を含んでいるために，メルスペクトログラムとHuBERT離散特徴量に対するロジットを入力特徴量とする場合よりも高い性能に達したと考えられる．

次に，ネットワークB（Randomized・A-SingleTask）は客観評価と主観評価の両面において，ネットワークB（Randomized）を上回る性能を示した．これより，HuBERT中間特徴量を予測するネットワークAは，メルスペクトログラムとHuBERT離散特徴量を同時に予測するマルチタスク学習を行わず，HuBERT中間特徴量のみを予測するよう学習した方が，ネットワークBに対するより良い入力特徴量を与えられると考えられる．マルチタスク学習では，複数タスクの損失を考慮させることが相互作用的に良い影響を及ぼす場合もあるが，シングルタスクの場合より悪い局所最適解に収束する場合もある\cite{crawshaw2020multi}．本実験条件下では，HuBERT中間特徴量の損失に対し，メルスペクトログラムおよびHuBERT離散特徴量の損失の干渉が悪影響を与えたと考えられる．

最後に，ネットワークE2E（Pretrained）は客観評価において，ネットワークB（Randomized・A-SingleTask）を上回ることはできなかった．これより，HuBERT中間特徴量を継ぎ目として2段階で学習したことは，ネットワーク全体としての表現力を損なうものではなかったと考えられる．HuBERT中間特徴量は元々DNNの中間表現であったため，2つのネットワーク間の継ぎ目として利用しても，十分な表現力を維持できたと考えられる．

\subsubsection{ネットワークB（Pretrained）について}
ネットワークB（Pretrained）は客観評価において，ネットワークB（Randomized）に劣る結果となった．これに対し，ネットワークB（Pretrained）が有効性を示さなかった理由は2つ考えられる．1つ目は，損失関数の重み係数$\lossWeightHubDisc$の適切な値が，グリッドサーチした範囲に存在しなかったことである．ネットワークB（Randomized）では適切な値が含まれていたが，グリッドサーチの候補は決め打ちであるから，ネットワークB（Randomized）にとって適した値を探索すれば，高い性能を発揮した可能性がある．2つ目は，事前学習時との入力特徴量のギャップである．本実験では，HuBERTの事前学習に合わせてHuBERT中間特徴量をTransformerへの入力とした．しかし，事前学習時に用いられる，マスクベクトルおよび原音声から得られる特徴量が存在しなかったことがギャップとなり，転移学習の妨げになった可能性がある．

\subsection{今後の課題}
本実験を通して得られた今後の課題は2つある．

1つ目は，損失関数の重み係数$\lossWeightHubDisc$におけるグリッドサーチである．本実験では5種類の値によるグリッドサーチを行い，客観評価の結果から，検討した手法の性能は$\lossWeightHubDisc$に大きく左右されることがわかった．これに対し，各タスクに対する損失関数の勾配のノルムを求め，損失の下がり具合によって勾配のノルムを調整するように重み係数を最適化するGradNorm\cite{chen2018gradnorm}や，損失の下がり具合のみを用いて重み係数を変化させるDynamic Weight Averaging\cite{liu2019end}がある．これらのように，重み係数を動的に調節し，グリッドサーチよりも柔軟なチューニングを可能にすることで，モデルのさらなる精度改善および，実験時間の短縮が期待できる．

2つ目は，HuBERT Transformerの転移学習方法である．本実験では，事前学習時との入力特徴量のギャップが大きかったことが課題の1つとして考えられる．これに対し，学習初期にあえて原音声から得られる特徴量を混入させて事前学習時の条件に近づけ，徐々に混入率を下げてネットワークAによる予測結果のみを入力とするようスケジューリングする方法が考えられる．混入率を下げて事前学習時から徐々に離していくことは，学習難易度が徐々に高まっていくことに相当する．このような方法はカリキュラム学習と呼ばれ，難易度を下げた場合において収束した局所最適解が，その後難易度を上げた場合における良い初期値となることで，初めから難易度の高い学習を行うよりも良い局所最適解に収束させられる可能性がある\cite{wang2021survey}．
