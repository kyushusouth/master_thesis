\subsection{考察}
\subsubsection{ネットワークB（Randomized）について}
\label{sec4:sec:consideration_b_randomized}
ネットワークB（Randomized）は客観評価において，メルスペクトログラムとHuBERT離散特徴量に対するロジットを入力特徴量としたネットワークB（Randomized・Mel-HuB）を上回る性能を示した．これより，ネットワークBへの入力として，HuBERT中間特徴量はメルスペクトログラムとHuBERT離散特徴量に対するロジットを組み合わせた特徴量よりも優れていたと言える．HuBERT中間特徴量は，音声波形をHuBERTの畳み込みエンコーダによって変換した結果であった．これはHuBERTの事前学習時に行われるMasked PredictionにおいてTransformer層の入力となる特徴量であり，Transformer層ではHuBERT中間特徴量のうち観測できる区間のみを利用して，マスクされた区間における教師ラベルを予測する必要がある．これより，畳み込みエンコーダは，音声波形の系列長を圧縮する過程で，Transformer層によって行われる文脈的構造の考慮に役立つ情報を，限られた次元の中にできるだけ詰め込むように最適化されると考える．よって，ネットワークB（Randomized）におけるHuBERT Transformer層はランダム初期化されてはいたものの，HuBERT中間特徴量自体が文脈的構造の考慮に適した情報を含んでいるから，メルスペクトログラムとHuBERT離散特徴量に対するロジットを入力特徴量とする場合よりも特徴抽出が行いやすくなり，結果として高い性能に達したと考えられる．

次に，ネットワークB（Randomized・A-SingleTask）は客観評価と主観評価の両面において，ネットワークB（Randomized）を上回る性能を示した．これより，HuBERT中間特徴量を予測するネットワークAは，メルスペクトログラムとHuBERT離散特徴量を同時に予測するマルチタスク学習を行わず，HuBERT中間特徴量のみを予測する方がネットワークBにとって適していると考えられる．ここで，ネットワークB（Randomized）とネットワークB（Randomized・Mel-HuB）の結果より，HuBERT中間特徴量は，メルスペクトログラムとHuBERT離散特徴量のロジットから構成した特徴量と比較して，ネットワークBの推定精度改善につながるより良い入力特徴量であった．よって，HuBERT中間特徴量と，メルスペクトログラムおよびHuBERT離散特徴量は，異なる情報を含んだ特徴量だと考えられる．この時，メルスペクトログラムおよびHuBERT離散特徴量についての損失がHuBERT中間特徴量についての損失と干渉することが，ネットワークBへの入力として用いるHuBERT中間特徴量を予測する上で，悪影響を及ぼしたと考えられる．

次に，ネットワークE2E（Pretrained）では，ネットワークB（Randomized・A-SingleTask）における重みを初期値としてネットワーク全体を再学習させることによる改善を狙ったが，これは効果的ではなかった．前述したように，HuBERT中間特徴量自体が文脈的構造の考慮に適した情報を含んでいたと考えると，HuBERT中間特徴量を継ぎ目として二段階で学習することはネットワークの表現力を損なうものではなく，ネットワーク全体を一度に最適化するのと変わらない品質での学習が可能だったと考えられる．

最後に，本実験では損失関数の重み係数$\lossWeightHubDisc$について五種類の値によるグリッドサーチを一貫して行い，検討した手法の性能は$\lossWeightHubDisc$に大きく依存することがわかった．特に，$\lossWeightHubDisc$の値を大きくすることによってHuBERT離散特徴量に対するCross Entropy Lossの低下が促進される一方で、メルスペクトログラムのMAE Lossが十分に下がらないままEarly Stoppingが発生するケースが散見され，マルチタスク学習において両方の損失をバランスよく最小化することの難しさが明らかとなった．実数値をとる重み係数についてのグリッドサーチには限界があるから，これを行うこと自体がモデルの性能の妨げになると考えられる．これに対し，マルチタスク学習の先行研究では，損失の下がり具合に基づいて適応的に重みを調整する方法\cite{chen2018gradnorm,liu2019end}が提案されている．これらは，各タスクの損失の勾配のノルムの調整にあたる．また，マルチタスク学習においては各タスクの損失が競合するNegative Transfer\cite{crawshaw2020multi}が発生する可能性があるが，これに対して各タスクの損失の勾配の向きを調整する方法\cite{yu2020gradient}も提案されている．こういった手法により，グリッドサーチおよび静的な重み係数を脱却することで，ネットワークの性能改善および実験時間の短縮が期待できる．

\subsubsection{ネットワークB（Pretrained）について}
ネットワークB（Pretrained）は、HuBERTの事前学習済み重みを用いて学習を行ったが、WERを低下させつつ話者類似度を維持することができなかったため，ネットワークB（Randomized）に劣る結果となった．しかし，これは単に事前学習済み重みが話者性の抽出に適していなかったわけではないと考える．実際，話者識別の先行研究\cite{chen2022large}では，音声波形をHuBERTによって変換した特徴量を入力として話者識別モデルを学習させた場合，窓長25 ms，シフト幅10 msで40次元のメルスペクトログラムを入力とする場合よりも，Equal Error Rateが低いことが示されている．ここで，HuBERTは事前学習時のまま重みを固定し，単なる特徴抽出器として用いられた．話者性の含まれるメルスペクトログラムに対し，話者識別をより高い精度で行える入力特徴量を音声波形から抽出できたことになるから，HuBERTが話者性抽出に適していないわけではないと考える．また，本研究ではHuBERT Transformer層の出力に話者ベクトルを結合する構成としたため，仮に話者性が損なわれていたとしてもそれを補完できるのではないかと考える．

これに対し，ネットワークB（Pretrained）が有効性を示さなかった理由は二つ考えられる．一つ目は，損失関数の重み係数$\lossWeightHubDisc$の適切な値が，グリッドサーチした範囲に存在しなかったことである．ネットワークB（Randomized）では適切な値が含まれていたが，グリッドサーチの候補は決め打ちであるから，その探索範囲に限界があった．この課題に対しては，\ref{sec4:sec:consideration_b_randomized}節で挙げた動的な重み係数の調整が有効だと考える．二つ目は，事前学習時との入力特徴量のギャップである．本実験では，HuBERTの事前学習に合わせてHuBERT中間特徴量をTransformer層への入力とした．しかし，事前学習時に用いられるマスクベクトルが存在しなかったこと，原音声から得られる特徴量ではないことが異なっており，こういったギャップが転移学習の妨げになった可能性がある．この課題に対しては，ネットワークAから予測されたHuBERT中間特徴量の一部区間に，マスクベクトルや原音声から得られたHuBERT中間特徴量を混入させることで，事前学習時の条件に近づける方法が考えられる．具体的には，学習開始時に高い混入率に設定しておいて，学習の進行に伴って徐々に混入率を低下させていき，最終的にネットワークAから予測されたHuBERT中間特徴量のみが入力となるようにスケジューリングする方法が考えられる．
