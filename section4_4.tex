\subsection{考察}
\subsubsection{ネットワークB（Randomized）について}
ネットワークB（Randomized）は、メルスペクトログラムとHuBERT離散特徴量を予測するタスクにおいて、メルスペクトログラムとHuBERT離散特徴量に対するロジットを入力特徴量としたネットワークB（Randomized・Mel-HuB）や、動画を入力特徴量としたベースラインを上回る性能を示した．これより，動画から推定されたHuBERT中間特徴量は，元々の入力である動画よりもメルスペクトログラムとHuBERT離散特徴量に適した入力特徴量であったと考えられる．HuBERT中間特徴量は768次元であったのに対し，メルスペクトログラムとHuBERT離散特徴量の複合特徴量は261次元であったから，より高次元で冗長性が高いことがネットワークBによる特徴抽出の対象に適していたのではないかと考える．

また，HuBERT中間特徴量を予測するネットワークAは，メルスペクトログラムとHuBERT離散特徴量を同時に予測するマルチタスク学習を行わず，HuBERT中間特徴量のみを予測するよう学習した方が，ネットワークBの性能改善につながることがわかった．マルチタスク学習を行う場合，メルスペクトログラムやHuBERT離散特徴量が損失に加わるため，それらの損失を小さくするための勾配も考慮した重みの更新が行われるが，特にネットワークB（Randomized）への入力となるHuBERT中間特徴量を推定する上では，悪影響を与えていたと考えられる．実際，ネットワークB（Randomized・Mel-HuB）がネットワークB（Randomized）に対して性能が下がったことから，HuBERT中間特徴量とその他二つの特徴量は性質の異なる特徴量であることが示唆される．

\subsubsection{ネットワークB（Pretrained）について}
ネットワークB（Pretrained）は、HuBERTの事前学習済み重みを用いて学習を行ったが、WERを低下させつつ話者類似度を維持することができなかったため，ネットワークB（Randomized）に劣る結果となった．しかし，これは単に事前学習済み重みが話者性の抽出に適していなかったわけではないと考える．実際，話者識別の先行研究\cite{chen2022large}では，HuBERTから得られる特徴量を入力として話者識別モデルを学習させた場合，メルスペクトログラムを入力とする場合よりもEqual Error Rateが低いことが示されているため，HuBERTが話者性抽出に適さないわけではないと考えられる．また，本研究ではHuBERT Transformer層の出力に話者ベクトルを結合する構成としたため，仮に話者性が損なわれていたとしてもそれを補完できるのではないかと考える．これに対し，本実験における課題は二つ考えられる．一つ目は，損失関数の重み係数$\lossWeightHubDisc$の適切な値が，グリッドサーチした範囲に存在しなかったことである．ネットワークB（Randomized）では適切な値が含まれていたが，グリッドサーチの候補は決め打ちであるから，その探索範囲に限界がある．ネットワークB（Pretrained）に適した値を探索すれば，より高い性能を発揮した可能性がある．二つ目は，事前学習時との入力特徴量のギャップである．本タスクのFineTuning時は，事前学習時に存在するマスクベクトルや，原音声から得られた特徴量が含まれない．この違いが大きかったために，転移学習が効果的でなかった可能性がある．この課題に対し，Visual Speech Recognition（VSR）の先行研究\cite{djilali2023lip2vec}では，本研究と同様に動画からHuBERT中間特徴量を推定し，これをHuBERT Transformer層に入力する方法が提案されている．

\subsubsection{損失関数の重み係数$\lossWeightHubDisc$の影響について}
本実験では，$\lossWeightHubDisc$について五種類の値によるグリッドサーチを一貫して行い，この値に性能が著しく依存するネットワークが多いことが分かった．特に、HuBERT離散特徴量に対するCross Entropy Lossが急峻に低下する一方で、メルスペクトログラムのMAE Lossが十分に下がらないままEarly Stoppingが発生するケースが散見され，マルチタスク学習において両方の損失をバランスよく最小化することの難しさが明らかとなった．こういったタスクごとの損失の重み付け方法について，マルチタスク学習の先行研究では，損失の下がり具合に基づき適応的に重みを調整する方法\cite{chen2018gradnorm,liu2019end}や、各タスクの損失の勾配を調整する方法\cite{yu2020gradient}が有効であることが示されている．こうした手法を導入し，学習時に適応的に重み係数の値を変化させられるようにすることで，実験回数を減らしながらより優れたモデルを得られる可能性がある．