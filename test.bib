@article{taguchi,
  author  = {田口史郎},
  journal = {九州大学大学院芸術工学府芸術工学専攻 博士論文},
  number  = {},
  title   = {"深層学習を用いたデータ駆動型調音・音声間変換に関する研究"},
  volume  = {},
  year    = {2021}
}
@article{esaki,
  author  = {江崎蓮},
  journal = {九州大学大学院芸術工学府芸術工学専攻 修士論文},
  number  = {},
  title   = {"深層学習を用いた口唇動画・音声変換に関する調査"},
  volume  = {},
  year    = {2022}
}
@inproceedings{atr,
  title={A large-scale Japanese speech database.},
  author={Sagisaka, Yoshinori and Takeda, Kazuya and Abel, M and Katagiri, Shigeru and Umeda, Tetsuo and Kuwabara, Hisao},
  booktitle={ICSLP},
  pages={1089--1092},
  year={1990}
}
@misc{okamoto2023hi,
  title={Hi-Fi-CAPTAIN: High-fidelity and high-capacity conversational speech synthesis corpus developed by NICT},
  author={Okamoto, T and Shiga, Y and Kawai, H},
  year={2023}
}
@article{takamichi2019jvs,
  title={JVS corpus: free Japanese multi-speaker voice corpus},
  author={Takamichi, Shinnosuke and Mitsui, Kentaro and Saito, Yuki and Koriyama, Tomoki and Tanji, Naoko and Saruwatari, Hiroshi},
  journal={arXiv preprint arXiv:1908.06248},
  year={2019}
}
@inproceedings{wan2018generalized,
  title={Generalized end-to-end loss for speaker verification},
  author={Wan, Li and Wang, Quan and Papir, Alan and Moreno, Ignacio Lopez},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={4879--4883},
  year={2018},
  organization={IEEE}
}
@misc{rinna-japanese-hubert-base,
    title = {rinna/japanese-hubert-base},
    author = {Hono, Yukiya and Mitsui, Kentaro and Sawada, Kei},
    url = {https://huggingface.co/rinna/japanese-hubert-base}
}
@inproceedings{sawada2024release,
    title = {Release of Pre-Trained Models for the {J}apanese Language},
    author = {Sawada, Kei and Zhao, Tianyu and Shing, Makoto and Mitsui, Kentaro and Kaga, Akio and Hono, Yukiya and Wakatsuki, Toshiaki and Mitsuda, Koh},
    booktitle = {Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
    month = {5},
    year = {2024},
    pages = {13898--13905},
    url = {https://aclanthology.org/2024.lrec-main.1213},
    note = {Available at: https://arxiv.org/abs/2404.01657}
}
@inproceedings{radford2023robust,
  title={Robust speech recognition via large-scale weak supervision},
  author={Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  booktitle={International conference on machine learning},
  pages={28492--28518},
  year={2023},
  organization={PMLR}
}
@inproceedings{kim2023lip,
  title={Lip-to-speech synthesis in the wild with multi-task learning},
  author={Kim, Minsu and Hong, Joanna and Ro, Yong Man},
  booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}
@article{choi2023intelligible,
  title={Intelligible lip-to-speech synthesis with speech units},
  author={Choi, Jeongsoo and Kim, Minsu and Ro, Yong Man},
  journal={arXiv preprint arXiv:2305.19603},
  year={2023}
}
@inproceedings{hsu2023revise,
  title={Revise: Self-supervised speech resynthesis with visual input for universal and generalized speech regeneration},
  author={Hsu, Wei-Ning and Remez, Tal and Shi, Bowen and Donley, Jacob and Adi, Yossi},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18795--18805},
  year={2023}
}
@inproceedings{sahipjohn2023robustl2s,
  title={RobustL2S: Speaker-Specific Lip-to-Speech Synthesis exploiting Self-Supervised Representations},
  author={Sahipjohn, Neha and Shah, Neil and Tambrahalli, Vishal and Gandhi, Vineet},
  booktitle={2023 Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)},
  pages={1492--1499},
  year={2023},
  organization={IEEE}
}
@inproceedings{kim2024let,
  title={Let There Be Sound: Reconstructing High Quality Speech from Silent Videos},
  author={Kim, Ji-Hoon and Kim, Jaehun and Chung, Joon Son},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={3},
  pages={2759--2767},
  year={2024}
}
@inproceedings{djilali2023lip2vec,
  title={Lip2Vec: Efficient and Robust Visual Speech Recognition via Latent-to-Latent Visual to Audio Representation Mapping},
  author={Djilali, Yasser Abdelaziz Dahou and Narayan, Sanath and Boussaid, Haithem and Almazrouei, Ebtessam and Debbah, Merouane},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={13790--13801},
  year={2023}
}
@article{yeo2024akvsr,
  title={Akvsr: Audio knowledge empowered visual speech recognition by compressing audio knowledge of a pretrained model},
  author={Yeo, Jeong Hun and Kim, Minsu and Choi, Jeongsoo and Kim, Dae Hoe and Ro, Yong Man},
  journal={IEEE Transactions on Multimedia},
  year={2024},
  publisher={IEEE}
}
@article{cheng2023opensr,
  title={Opensr: Open-modality speech recognition via maintaining multi-modality alignment},
  author={Cheng, Xize and Jin, Tao and Li, Linjun and Lin, Wang and Duan, Xinyu and Zhao, Zhou},
  journal={arXiv preprint arXiv:2306.06410},
  year={2023}
}
@inproceedings{liu2023synthvsr,
  title={Synthvsr: Scaling up visual speech recognition with synthetic supervision},
  author={Liu, Xubo and Lakomkin, Egor and Vougioukas, Konstantinos and Ma, Pingchuan and Chen, Honglie and Xie, Ruiming and Doulaty, Morrie and Moritz, Niko and Kolar, Jachym and Petridis, Stavros and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18806--18815},
  year={2023}
}
@inproceedings{ma2023auto,
  title={Auto-avsr: Audio-visual speech recognition with automatic labels},
  author={Ma, Pingchuan and Haliassos, Alexandros and Fernandez-Lopez, Adriana and Chen, Honglie and Petridis, Stavros and Pantic, Maja},
  booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}
@inproceedings{chang2024conformer,
  title={Conformer is All You Need for Visual Speech Recognition},
  author={Chang, Oscar and Liao, Hank and Serdyuk, Dmitriy and Shahy, Ankit and Siohan, Olivier},
  booktitle={ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={10136--10140},
  year={2024},
  organization={IEEE}
}
@article{yeo2024visual,
  title={Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing},
  author={Yeo, Jeong Hun and Han, Seunghee and Kim, Minsu and Ro, Yong Man},
  journal={arXiv preprint arXiv:2402.15151},
  year={2024}
}
@inproceedings{zinonos2023learning,
  title={Learning cross-lingual visual speech representations},
  author={Zinonos, Andreas and Haliassos, Alexandros and Ma, Pingchuan and Petridis, Stavros and Pantic, Maja},
  booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}
@article{yeo2023visual,
  title={Visual speech recognition for low-resource languages with automatic labels from whisper model},
  author={Yeo, Jeong Hun and Kim, Minsu and Watanabe, Shinji and Ro, Yong Man},
  journal={arXiv preprint arXiv:2309.08535},
  year={2023}
}