\begin{thebibliography}{10}

\bibitem{afouras2018lrs3}
Triantafyllos Afouras, Joon~Son Chung, and Andrew Zisserman.
\newblock Lrs3-ted: a large-scale dataset for visual speech recognition.
\newblock {\em arXiv preprint arXiv:1809.00496}, 2018.

\bibitem{chung2018voxceleb2}
Joon~Son Chung, Arsha Nagrani, and Andrew Zisserman.
\newblock Voxceleb2: Deep speaker recognition.
\newblock {\em arXiv preprint arXiv:1806.05622}, 2018.

\bibitem{shi2022learning}
Bowen Shi, Wei-Ning Hsu, Kushal Lakhotia, and Abdelrahman Mohamed.
\newblock Learning audio-visual speech representation by masked multimodal
  cluster prediction.
\newblock {\em arXiv preprint arXiv:2201.02184}, 2022.

\bibitem{kim2023lip_multitask}
Minsu Kim, Joanna Hong, and Yong~Man Ro.
\newblock Lip-to-speech synthesis in the wild with multi-task learning.
\newblock In {\em ICASSP 2023-2023 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pp. 1--5. IEEE, 2023.

\bibitem{choi2023intelligible}
Jeongsoo Choi, Minsu Kim, and Yong~Man Ro.
\newblock Intelligible lip-to-speech synthesis with speech units.
\newblock {\em arXiv preprint arXiv:2305.19603}, 2023.

\bibitem{maas2013rectifier}
Andrew~L Maas, Awni~Y Hannun, Andrew~Y Ng, et~al.
\newblock Rectifier nonlinearities improve neural network acoustic models.
\newblock In {\em Proc. icml}, Vol.~30, p.~3. Atlanta, GA, 2013.

\bibitem{he2015delving}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pp. 1026--1034, 2015.

\bibitem{hendrycks2016gaussian}
Dan Hendrycks and Kevin Gimpel.
\newblock Gaussian error linear units (gelus).
\newblock {\em arXiv preprint arXiv:1606.08415}, 2016.

\bibitem{hochreiter1997long}
S~Hochreiter.
\newblock Long short-term memory.
\newblock {\em Neural Computation MIT-Press}, 1997.

\bibitem{cho2014learning}
Kyunghyun Cho.
\newblock Learning phrase representations using rnn encoder-decoder for
  statistical machine translation.
\newblock {\em arXiv preprint arXiv:1406.1078}, 2014.

\bibitem{ioffe2015batch}
Sergey Ioffe.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock {\em arXiv preprint arXiv:1502.03167}, 2015.

\bibitem{ba2016layer}
Jimmy~Lei Ba.
\newblock Layer normalization.
\newblock {\em arXiv preprint arXiv:1607.06450}, 2016.

\bibitem{salimans2016weight}
Tim Salimans and Durk~P Kingma.
\newblock Weight normalization: A simple reparameterization to accelerate
  training of deep neural networks.
\newblock {\em Advances in neural information processing systems}, Vol.~29, ,
  2016.

\bibitem{vaswani2017attention}
A~Vaswani.
\newblock Attention is all you need.
\newblock {\em Advances in Neural Information Processing Systems}, 2017.

\bibitem{zhang2019gradient}
Jiawei Zhang.
\newblock Gradient descent based optimization algorithms for deep learning
  models training.
\newblock {\em arXiv preprint arXiv:1903.03614}, 2019.

\bibitem{srivastava2014dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock {\em The journal of machine learning research}, Vol.~15, No.~1, pp.
  1929--1958, 2014.

\bibitem{kingma2014adam}
Diederik~P Kingma.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{loshchilov2017decoupled}
I~Loshchilov.
\newblock Decoupled weight decay regularization.
\newblock {\em arXiv preprint arXiv:1711.05101}, 2017.

\bibitem{hsu2021hubert}
Wei-Ning Hsu, Benjamin Bolte, Yao-Hung~Hubert Tsai, Kushal Lakhotia, Ruslan
  Salakhutdinov, and Abdelrahman Mohamed.
\newblock Hubert: Self-supervised speech representation learning by masked
  prediction of hidden units.
\newblock {\em IEEE/ACM transactions on audio, speech, and language
  processing}, Vol.~29, pp. 3451--3460, 2021.

\bibitem{wan2018generalized}
Li~Wan, Quan Wang, Alan Papir, and Ignacio~Lopez Moreno.
\newblock Generalized end-to-end loss for speaker verification.
\newblock In {\em 2018 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, pp. 4879--4883. IEEE, 2018.

\bibitem{kong2020hifi}
Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae.
\newblock Hifi-gan: Generative adversarial networks for efficient and high
  fidelity speech synthesis.
\newblock {\em Advances in neural information processing systems}, Vol.~33, pp.
  17022--17033, 2020.

\bibitem{taguchi}
田口史郎.
\newblock "深層学習を用いたデータ駆動型調音・音声間変換に関する研究".
\newblock 九州大学大学院芸術工学府芸術工学専攻 博士論文, 2021.

\bibitem{esaki}
江崎蓮.
\newblock "深層学習を用いた口唇動画・音声変換に関する調査".
\newblock 九州大学大学院芸術工学府芸術工学専攻 修士論文, 2022.

\bibitem{atr}
Yoshinori Sagisaka, Kazuya Takeda, M~Abel, Shigeru Katagiri, Tetsuo Umeda, and
  Hisao Kuwabara.
\newblock A large-scale japanese speech database.
\newblock In {\em ICSLP}, pp. 1089--1092, 1990.

\bibitem{okamoto2023hi}
T~Okamoto, Y~Shiga, and H~Kawai.
\newblock Hi-fi-captain: High-fidelity and high-capacity conversational speech
  synthesis corpus developed by nict, 2023.

\bibitem{takamichi2019jvs}
Shinnosuke Takamichi, Kentaro Mitsui, Yuki Saito, Tomoki Koriyama, Naoko Tanji,
  and Hiroshi Saruwatari.
\newblock Jvs corpus: free japanese multi-speaker voice corpus.
\newblock {\em arXiv preprint arXiv:1908.06248}, 2019.

\bibitem{bulat2017far}
Adrian Bulat and Georgios Tzimiropoulos.
\newblock How far are we from solving the 2d \& 3d face alignment problem?(and
  a dataset of 230,000 3d facial landmarks).
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pp. 1021--1030, 2017.

\bibitem{rinna-japanese-hubert-base}
Yukiya Hono, Kentaro Mitsui, and Kei Sawada.
\newblock rinna/japanese-hubert-base.

\bibitem{sawada2024release}
Kei Sawada, Tianyu Zhao, Makoto Shing, Kentaro Mitsui, Akio Kaga, Yukiya Hono,
  Toshiaki Wakatsuki, and Koh Mitsuda.
\newblock Release of pre-trained models for the {J}apanese language.
\newblock In {\em Proceedings of the 2024 Joint International Conference on
  Computational Linguistics, Language Resources and Evaluation (LREC-COLING
  2024)}, pp. 13898--13905, 5 2024.
\newblock Available at: https://arxiv.org/abs/2404.01657.

\bibitem{pasad2023comparative}
Ankita Pasad, Bowen Shi, and Karen Livescu.
\newblock Comparative layer-wise analysis of self-supervised speech models.
\newblock In {\em ICASSP 2023-2023 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pp. 1--5. IEEE, 2023.

\bibitem{radford2023robust}
Alec Radford, Jong~Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and
  Ilya Sutskever.
\newblock Robust speech recognition via large-scale weak supervision.
\newblock In {\em International conference on machine learning}, pp.
  28492--28518. PMLR, 2023.

\bibitem{kirkland2023stuck}
Ambika Kirkland, Shivam Mehta, Harm Lameris, Gustav~Eje Henter, Eva
  Sz{\'e}kely, and Joakim Gustafson.
\newblock Stuck in the mos pit: A critical analysis of mos test methodology in
  tts evaluation.
\newblock In {\em 12th Speech Synthesis Workshop (SSW) 2023}, 2023.

\end{thebibliography}
