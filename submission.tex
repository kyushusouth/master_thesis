\documentclass[12pt]{jarticle}
\usepackage[utf8]{inputenc}
\usepackage[top=30truemm, bottom=30truemm, left=20truemm, right=20truemm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{siunitx}
\usepackage{bm}
\usepackage[dvipdfmx]{graphicx}
\usepackage[dvipdfmx]{color}
\usepackage[subrefformat=parens]{subcaption}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{tocloft}
\usepackage{enumerate}
\usepackage{url}
\usepackage{multirow}
\usepackage{caption}
\usepackage{enumitem}
\usepackage{makecell}
\usepackage{array}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{physics}

\numberwithin{equation}{section}    % 数式番号にセクション番号をつける
\numberwithin{figure}{section}      % 図番号にセクション
\numberwithin{table}{section}      % 図番号にセクション

% \renewcommand{\figurename}{Fig.}    % 図 -> Fig.
% \renewcommand{\tablename}{Table }   % 表 -> Table

\renewcommand{\baselinestretch}{1.1}

\newlength{\figcaptionskip}
\setlength{\figcaptionskip}{5pt} % 図のキャプション間隔
\newlength{\tabcaptionskip}
\setlength{\tabcaptionskip}{-5pt} % 表のキャプション間隔
\captionsetup[figure]{skip=\figcaptionskip}
\captionsetup[table]{skip=\tabcaptionskip}

\setlist[enumerate]{topsep=5pt, partopsep=5pt, itemsep=0pt, parsep=0pt}
\setlength{\topsep}{5pt}
\setlength{\partopsep}{5pt}

\input{vars.tex}

\allowdisplaybreaks[4]

\begin{document}

\begin{titlepage}
    \begin{center}
        {\Large 2024年度後期　修士論文}
        \vspace{120truept}

        {\huge 深層学習による口唇音声変換に関する研究}
        \vspace{30truept}

        {\huge A Study on the Conversion from Lip-Video to Speech Using a Deep Learning Technique}
        \vspace{120truept}

        {\Large 2025年月日}
        \vspace{10truept}

        {\Large 九州大学芸術工学府音響設計コース}
        \vspace{70truept}

        {\Large 2DS23095M}
        \vspace{10truept}

        {\Large 南 汰翼}
        \vspace{10truept}

        {\Large MINAMI Taisuke}
        \vspace{30truept}

        {\Large 研究指導教員　鏑木 時彦　教授}
    \end{center}
\end{titlepage}

\section*{概要}
\thispagestyle{empty}
\clearpage

\setcounter{tocdepth}{3}
\tableofcontents
\thispagestyle{empty}
\clearpage

\pagestyle{plain}
\setcounter{page}{1}

\input{section1.tex}
\clearpage

\input{section2.tex}
\clearpage

\input{section3.tex}
\clearpage

\section{動画音声合成モデルの検討}
\subsection{音声合成法}
\input{section4.tex}

% \subsubsection{前のやつ}
% 提案手法の構築手順は3段階に分かれる．ネットワークの構造を図~\ref{sec4:fig:network}に示す．一段階目では，動画を入力として，メルスペクトログラムとHuBERT離散特徴量，HuBERT中間特徴量を推定するネットワークAを学習する（図~\ref{sec4:fig:network}のA）．ここで，HuBERT離散特徴量はHuBERT Transformer層から得られる特徴量を k-means法によってクラスタリングすることで離散化した値，HuBERT中間特徴量はHuBERTにおける畳み込み層出力で，HuBERTの事前学習時にマスク対象となる値のことを指す．図~\ref{sec4:fig:hubert}にこれらの取得位置を示す．第一段階では，AVHuBERTを動画からの特徴抽出に利用した．これにより，動画の空間情報は完全に圧縮され，768次元の一次元系列となる．その後，事前学習済みの話者識別モデル\cite{wan2018generalized}によって音声波形から得られる256次元の話者Embeddingを，各フレームでチャンネル方向に結合する．これによって特徴量は1024次元に拡張され，全結合層によって再度768次元に圧縮する．その後，畳み込み層と全結合層からなるデコーダを通すことによって，話者Embeddingを結合した特徴量に対する変換を施した．これにより，特にメルスペクトログラムにおいて話者性が正しく反映されることを狙った．複数話者モデルであっても，入力である動画の見た目から話者性を判別できる可能性があったが，将来的な未知話者対応への拡張性も考慮して，本研究では補助特徴量として入力することとした．デコーダは残差結合を利用したブロック単位で構成され，各ブロックに2層の畳み込み層を設けた．各畳み込み層のチャンネル数は768，カーネルサイズは3であり，3ブロック積み重ねた．最後に全結合層を通し，所望の次元に変換することで予測対象を得た．ネットワークAの役割は，続くネットワークBの入力であるHuBERT中間特徴量を提供することである．これに対し，メルスペクトログラムとHuBERT離散特徴量の推定を同時に行った理由は，先行研究においてマルチタスク学習の有効性が確認されていることを考慮し，ネットワークAでもマルチタスク学習を採用しておこうと考えたからである．

% 二段階目では，一段階目に学習されたネットワークAの重みを固定した状態でHuBERT中間特徴量を推定し，それを入力としてメルスペクトログラムとHuBERT離散特徴量を推定する，HuBERT Transformer層を中心としたネットワークBの学習を行う（図~\ref{sec4:fig:network}のB）．HuBERT Transformer層出力はAVHuBERT出力と同じ768次元の特徴量となるため，これに対してネットワークAと同様に話者Embeddingを結合し，デコーダを通すことで予測値を得た．ネットワークBの役割は，音声波形への変換に必要となるメルスペクトログラムとHuBERT離散特徴量の予測である．HuBERT Transformer層の転移学習を検討した狙いについて，HuBERTは自己教師あり学習時，畳み込み層出力にマスクを適用し，Transformer層を通すことによってマスクされた部分を推定しようとする．これにより，音声の文脈を考慮するのに長けた学習済み重みが，特にTransformer層で獲得されると仮定した．これに基づき，本研究ではHuBERT Transformer層を動画音声合成にFine Tuningすることにより，動画を入力としたAVHuBERTを中心とするネットワークAにおける推定残差を，音声自体の文脈を考慮することによって軽減し，動画から直接推定しきれなかった部分を補うことでの精度改善を狙った．

% \begin{figure}[bt]
%     \centering
%     \includegraphics[height=90mm]{./figure/sec4/model/network.drawio.png}
%     \caption{提案するネットワークの構造}
%     \label{sec4:fig:network}
% \end{figure}

% \begin{figure}[bt]
%     \centering
%     \includegraphics[height=90mm]{./figure/sec4/model/hubert.png}
%     \caption{HuBERT中間特徴量とHuBERT離散特徴量の取得位置}
%     \label{sec4:fig:hubert}
% \end{figure}

% 三段階目では，二段階目までに学習されたネットワークAとネットワークBの重みを固定した状態で，AVHuBERTから得られる特徴量と，HuBERT Transformer層から得られる特徴量の二つを結合し，それらを入力として再びメルスペクトログラムとHuBERT離散特徴量の予測を行うネットワークCを学習した（図~\ref{sec4:fig:network}のC）．ネットワークCでは，はじめに前述した二つの特徴量をチャンネル方向に結合することで，1536次元の入力特徴量を得る．これに対して全結合層を施すことで再度768次元に圧縮し，4層のTransformer層を通すことで系列全体を考慮した特徴抽出を改めて行った．その後，ネットワークA,Bと同様に話者Embeddingを結合し，デコーダを通すことによって予測値を得た．ここで，ネットワークCのTransformer層におけるパラメータについては，AVHuBERTやHuBERTと同様にチャンネル数を768，ヘッド数は12とした．ネットワークCの役割は，ネットワークBと同様に音声波形への変換に必要な特徴量の予測である．ここでの狙いについて，まず，AVHuBERTから得られる特徴量とHuBERT Transformer層から得られる特徴量は，どちらもデコーダへの入力となる点で同じである．一方，AVHuBERTは動画を入力，HuBERT Transformer層はHuBERT中間特徴量を入力とするため，これら特徴量の元となる入力は異なっている．ここでは，概ね同じ予測対象のために利用される二つの特徴量（ネットワークAではHuBERT中間特徴量の予測も行っているため，全く同じではない）が，入力の違いに依存して内部のSelf Attentionにより注意される部分が変化し，何らかの異なった情報を持っている可能性があると仮定した．よって，両方の特徴量を考慮し，単一特徴量への依存を解消することで，汎化性能向上による予測精度の改善を狙った．

% 以上が提案手法の全体像であるが，今回ベースラインとする先行研究\cite{choi2023intelligible}に基づいたマルチタスク学習手法は，本研究におけるネットワークAで，HuBERT中間特徴量を推定しないものに当たる．

% 以上のモデルにより，動画からメルスペクトログラムとHuBERT離散特徴量が推定可能となる．その後，先行研究\cite{choi2023intelligible}に基づくMulti-input Vocoderを用い，メルスペクトログラムとHuBERT離散特徴量を入力として音声波形に変換することで，最終的な合成音声を得た．Multi-input VocoderはHiFi-GAN\cite{kong2020hifi}をベースとしたモデルであり，音声波形を生成するGeneratorと，Multi-Period Discriminator（MPD）およびMulti-Scale Discriminator（MSD）という二つのDiscriminatorによって構成される．

% Generatorの構造を図~\ref{sec4:fig:multi-input_vocoder}に示す．左の特徴量予測モデルは，動画からメルスペクトログラムとHuBERT離散特徴量を予測する，本研究において主な検討対象となる部分を表す．Generatorの内部構造について，まず，前処理層はメルスペクトログラムとHuBERT離散特徴量を入力として受け取り，その後のレイヤーに入力するための形状に変換する役割を持つ．メルスペクトログラムに対しては，時間方向に隣接した2フレームを次元方向に縦積みすることによって，100 Hz・80次元の特徴量から50 Hz・160次元の特徴量に変換した後，全結合層によって128次元の特徴量に変換する．一方，HuBERT離散特徴量は50 Hzのインデックス系列であり，インデックスから128次元のベクトルへと変換する．その後，これらをチャンネル方向に結合することで50 Hz・256次元の特徴量を構成し，これをその後のレイヤーへの入力とする．この特徴量は，転置畳み込み層と複数種類の畳み込み層から構成されるブロックを通過していく．各ブロックについて，まず，転置畳み込み層は特徴量を時間方向にアップサンプリングする役割を果たす．実際，本研究では50 Hzの入力特徴量から16 kHzの音声波形まで，時間方向に320倍のアップサンプリングを行う必要がある．Generatorでは，これを複数のブロックを通して段階的に行っている．また，各ブロックの転置畳み込み層に積まれた複数種類の畳み込み層は，そのカーネルサイズとダイレーションが全て異なっている．複数の時間的な受容野を持つ畳み込み層からの出力をすべて加算することで，アップサンプリング後の特徴量からの特徴抽出を行う仕組みとなっている．表~\ref{sec4:tab:multi-input_vocoder_parameter}に，Generatorの各ブロックにおけるパラメータとブロックごとの出力特徴量の形状を示す．カラム名の右にある括弧がきが数値の意味を表しており，Kはカーネルサイズ，Sはストライド，Dはダイレーション，Cは次元（チャンネル数），Tは系列長である．畳み込み層についてはカーネルサイズとダイレーションを集合として表記しているが，実際はこれらの直積の元，すなわち\lr{3, 1}や\lr{3, 3}，\lr{3, 5}をパラメータとする畳み込み層が存在することを表す．すなわち，転置畳み込み層一層に対し，その後の特徴量抽出は15種類の異なるカーネルサイズ，ダイレーションを設定した畳み込み層によって行われる．
% \begin{figure}[bt]
%     \centering
%     \includegraphics[height=120mm]{./figure/sec4/model/multi-input_vocoder.png}
%     \caption{Multi-input Vocoderの構造}
%     \label{sec4:fig:multi-input_vocoder}
% \end{figure}
% \begin{table*}[bt]
%     \centering
%     \caption{Generatorの各ブロックにおけるパラメータ}
%     \label{sec4:tab:multi-input_vocoder_parameter}
%     \begin{center}
%         \renewcommand{\arraystretch}{0.9} % 行の高さ調整
%         \setlength{\tabcolsep}{8pt}      % 列の幅調整
%         \scalebox{1.0}{
%             \begin{tabular}{|c|c|c|c|}
%                 \hline
%                   & \multicolumn{1}{c|}{転置畳み込み層 \lr{K, S}} & \multicolumn{1}{c|}{畳み込み層 \lr{K, D}}     & \multicolumn{1}{c|}{出力特徴量の形状 \lr{C, T}} \\
%                 \hline
%                 1 & \lr{11, 5}                             & \lr{$\{3, 5, 7, 9, 11\}$, $\{1, 3, 5\}$} & \lr{1024, 250}                          \\
%                 2 & \lr{8, 4}                              & \lr{$\{3, 5, 7, 9, 11\}$, $\{1, 3, 5\}$} & \lr{512, 1000}                          \\
%                 3 & \lr{4, 2}                              & \lr{$\{3, 5, 7, 9, 11\}$, $\{1, 3, 5\}$} & \lr{256, 2000}                          \\
%                 4 & \lr{4, 2}                              & \lr{$\{3, 5, 7, 9, 11\}$, $\{1, 3, 5\}$} & \lr{128, 4000}                          \\
%                 5 & \lr{4, 2}                              & \lr{$\{3, 5, 7, 9, 11\}$, $\{1, 3, 5\}$} & \lr{64, 8000}                           \\
%                 6 & \lr{4, 2}                              & \lr{$\{3, 5, 7, 9, 11\}$, $\{1, 3, 5\}$} & \lr{32, 16000}                          \\
%                 \hline
%             \end{tabular}
%         }
%     \end{center}
% \end{table*}

% Generatorの学習時に用いられるのが，MPDおよびMSDという二つのDiscriminatorである．これらの概要を図~\ref{sec4:fig:multi-input_vocoder_mpd_msd}に示す．MPDでは，一次元の音声波形を指定した周期をもとにReshapeすることで二次元に変換し，これに対して二次元畳み込みを適用することで，入力された音声波形の原音声らしさを判定するDiscriminatorである．異なる周期を設定したDiscriminatorを複数利用することで，時間的な特徴を考慮できるように構成されている．一方，MSDでは，音声波形に対してAverage Poolingを適用することでダウンサンプリングし，これに一次元畳み込みを適用することで入力された音声波形の原音声らしさを判定するDiscriminatorである．MSDはAverage Poolingによって系列長を抑えつつ，時間方向の連続的な特徴を考慮する狙いがある．これら二つのDiscriminatorについては，HiFi-GANと同様のパラメータで用いた．
% \begin{figure}[bt]
%     \centering
%     \begin{subfigure}{0.45\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{./figure/sec4/model/mpd.png}
%         \caption{Multi-Period Discriminator（MPD）}
%         \label{sec4:fig:multi-input_vocoder_mpd}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.45\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{./figure/sec4/model/msd.png}
%         \caption{Multi-Scale Discriminator（MSD）}
%         \label{sec4:fig:multi-input_vocoder_msd}
%     \end{subfigure}
%     \caption{Multi-Period Discriminator（MPD）とMulti-Scale Discriminator（MSD）の概要}
%     \label{sec4:fig:multi-input_vocoder_mpd_msd}
% \end{figure}

\subsection{実験方法}
\subsubsection{利用したデータセット}
動画音声データセットには，男女二人ずつから収録した合計4人分のデータセット\cite{taguchi,esaki}を用いた．これはATR音素バランス文\cite{atr}から構成され，全話者共通でAからHセットを学習データ，Iセットを検証データ，Jセットをテストデータとして利用した．各分割ごとの文章数を表~\ref{sec4:tab:dataset_info}に示す．

Multi-input Vocoderの学習に利用する音声データセットには，Hi-Fi-Captain（日本語話者二名分）\cite{okamoto2023hi}とJVS（parallel100とnonpara30）\cite{takamichi2019jvs}を利用した．Hi-Fi-Captainはtrain-parallelおよびtrain-non-parallelを学習データ，valを検証データ，evalをテストデータとして分割した．各分割ごとの文章数を表~\ref{sec4:tab:dataset_info}に示す．JVSには話者に対して1から100まで番号が割り振られており，本実験では1から80番の話者を学習データ，81番から90番の話者を検証データ，91番から100番までの話者をテストデータとした．各分割ごとの文章数を表~\ref{sec4:tab:dataset_info}に示す．また，JVSには読み上げ音声のparallel100およびnonpara30と，裏声のfalset10，囁き声のwhisper10が含まれる．本研究では，parallel100とnonpara30のみを利用した．

\begin{table*}[bt]
    \centering
    \caption{利用したデータセットの文章数}
    \label{sec4:tab:dataset_info}
    \begin{center}
        \renewcommand{\arraystretch}{0.9} % 行の高さ調整
        \setlength{\tabcolsep}{8pt}      % 列の幅調整
        \scalebox{1.0}{
            \begin{tabular}{|l|r|r|r|}
                \hline
                              & \multicolumn{1}{c|}{学習} & \multicolumn{1}{c|}{検証} & \multicolumn{1}{c|}{テスト} \\
                \hline
                動画音声データセット    & 1598                    & 200                     & 212                      \\
                Hi-Fi-Captain & 37714                   & 200                     & 200                      \\
                JVS           & 10398                   & 1299                    & 1300                     \\
                \hline
            \end{tabular}
        }
    \end{center}
\end{table*}

\subsubsection{データの前処理}
動画データは60 FPSで収録されたものをffmpegにより25 FPSに変換して用いた．その後，手法\cite{bulat2017far}により動画に対してランドマーク検出を適用した．このランドマークを利用することで口元のみを切り取り，画像サイズを（96, 96）にリサイズした．モデル入力時は動画をグレースケールに変換し，各フレームに対する正規化および標準化を適用した．正規化では，グレースケールが0から255までの値を取るため，最大値の255で割り，標準化では，AVHuBERTのプログラムで利用されていた平均値0.421と，標準偏差0.165をそのまま利用して，平均値を引いた後に標準偏差で割った．全体として，今回は事前学習済みのAVHuBERTの転移学習を行うため，そこでの前処理に合わせている．学習時は，ランダムクロップ，左右反転，Time Masking（一時停止）をデータ拡張として適用した．ランダムクロップは，（96, 96）で与えられる画像から（88, 88）をランダムに切り取る処理である．検証およびテスト時は，必ず画像中央を切り取るよう実装した．左右反転は，50\%の確率で左右が反転されるよう実装した．Time Maskingは，連続する画像の時間平均値を利用することによって，一時停止させるような効果を与えるデータ拡張手法である．動画1秒あたり0から0.5秒の間でランダムに停止区間を定め，その区間における動画の時間方向平均値を計算し，区間内のすべてのフレームをこの平均値で置換した．

音声データは16 kHzにダウンサンプリングして用いた．それから，窓長25 msのハニング窓を用いて，シフト幅10 msでSTFTを適用することでフレームレート100 Hzのスペクトログラムに変換した．さらに，パワースペクトログラムに対して80次のメルフィルタバンクを適用し，メルスペクトログラムを得た上で対数スケールに変換した．
また，Multi-input Vocoderの学習に利用したHi-Fi-CaptainとJVSについては，無音区間のトリミング（-40 dBFS未満かつ500 ms継続する区間を100 msまでカット）を適用した．なぜなら，Multi-input Vocoderの学習時は，全体から1秒分をランダムサンプリングするように実装しており，元データに存在する無音区間を除去することによって，学習の安定化および得られる音声の品質改善に繋がったからである．
また，話者Embeddingの取得には事前学習済みの話者識別モデル\cite{wan2018generalized}を利用した．動画音声データセット，Hi-Fi-Captain，JVSともに，各話者学習データの中から100文章をランダムサンプリングし，各発話に対して得られたベクトルの平均値を用いた．この値を学習・検証・テストで一貫して用いるため，学習外の検証データやテストデータには非依存な値となっている．モデルに入力する際には，ベクトルの大きさで割って正規化した．

HuBERTは，HuggingFaceに公開されているReazonSpeechというデータセットによって学習されたモデル\cite{rinna-japanese-hubert-base,sawada2024release}を利用した．ReazonSpeechは約19000時間の日本語音声からなるデータセットであり，日本語音声のコンテキストを大量のデータから学習したモデルである．今回用いるデータセットが日本語であることから，本研究の検討対象としては日本語音声に関する事前知識を豊富に有するモデルが適していると考え，このモデルを選択した．本研究で用いるHuBERT中間特徴量およびHuBERT離散特徴量について，HuBERT中間特徴量は，音声波形に対して畳み込み層を適用した出力で，HuBERTの事前学習時にはマスクの対象となる特徴量を利用した．一方，HuBERT離散特徴量は，HuBERT Transformer層の8層目出力に，k-means法によるクラスタリングを適用することで得た．あえて8層目出力を選択した理由は，HuBERTのレイヤーごとの特徴量について，音素のOne-hotベクトルおよび単語のOne-hotベクトルとの相関を，Canonical Correlation Analysis（CCA）によって調べた先行研究\cite{pasad2023comparative}より，8層目出力がそのどちらとも相関が高く言語的な情報に近いと判断したからである．クラスタ数については決め打ちとなるが，今回は離散化した結果が言語的な情報を持つことを狙い，比較的少ない100とした．k-means法の学習には動画音声データセットにおける学習用データ全てを利用し，これを用いて動画音声データ，Hi-Fi-Captain，JVSに対するクラスタリングを実施した．

\subsubsection{学習方法}
一段階目について，損失関数はメルスペクトログラムのMAE Loss $L_{mel}$ とHuBERT離散特徴量のCross Entropy Loss $L_{ssl^{d}}$ ，HuBERT中間特徴量のMAE Loss $L_{ssl^{i}}$ の重み付け和とした．それぞれの重み係数を$\lambda_{mel}, \lambda_{ssl^{d}}, \lambda_{ssl^{i}}$とすると，
\begin{equation}
    \label{sec4:eq:loss}
    L = \lambda_{mel} * L_{mel} + \lambda_{ssl^{d}} * L_{ssl^{d}} + \lambda_{ssl^{i}} * L_{ssl^{i}}
\end{equation}
となる．最適化手法にはAdamW\cite{loshchilov2017decoupled}を利用し，$\beta_{1} = 0.9$，$\beta_{2} = 0.98$，$\lambda = 0.01$とした．学習率に対するスケジューラには，Cosine Annealing with Warmupを利用した．開始時の学習率は\num{1.0e-6}として，最大エポック数の10\%に至るまでは学習率を\num{1.0e-3}まで線形に増加させ，その後のエポックではcosine関数に基づいて\num{10.e-6}まで減少させた．バッチサイズはメモリの都合上4としたが，学習の安定化のため，Gradient Accumulationによって各イテレーションにおける勾配を累積させ，8イテレーションに一回重みを更新するようにした．モデルに入力する動画の秒数は10秒を上限とし，それを超える場合はランダムにトリミング，それに満たない場合はゼロパディングした．ゼロパディングした部分は損失の計算からは除外した．勾配のノルムは3.0を上限としてクリッピングすることで，過度に大きくなることを防止した．最大エポック数は50とし，10エポック連続して検証データに対する損失が小さくならない場合には，学習を中断するようにした（Early Stopping）．また，学習終了時には検証データに対する損失が最も小さかったエポックにおけるチェックポイントを保存し，これをテストデータに対する評価に用いた．

第二段階について，損失関数はメルスペクトログラムのMAE LossとHuBERT離散特徴量のCross Entropy Lossの重み付け和とした．これは式~\eqref{sec4:eq:loss}において，$\lambda_{ssl^{i}} = 0.0$と固定した場合に相当する．最適化手法にはAdamWを利用し，$\beta_{1} = 0.9$，$\beta_{2} = 0.98$，$\lambda = 0.01$とした．学習率に対するスケジューラには，Cosine Annealing with Warmupを利用した．開始時の学習率は\num{1.0e-6}として，最大エポック数の10\%に至るまでは学習率を\num{5.0e-4}まで線形に増加させ，その後のエポックではcosine関数に基づいて\num{10.e-6}まで減少させた．学習率の最大値は第一段階の$1/2$となっているが，これは値を半減させることによって学習を安定させることができたためである．その他のパラメータは，第一段階における値と同じである．

第三段階について，Cosine Annealing with Warmupにおける最大学習率のみ\num{1.0e-3}としたが，それ以外は第二段階と同様である．

Multi-input Vocoderの学習では，Hi-Fi-CaptainとJVSを用いた．はじめにHi-Fi-Captainのみを用いて学習させ，その後学習済みモデルをJVSによって再学習した．損失関数はHiFi-GANと同様である．二つのデータセットを用いた理由について，Hi-Fi-Captainは男女一人ずつの文章数が豊富なデータセットであるため，高品質なモデルを構築可能であった．しかし，学習できる話者数が少ない分，学習外話者に対する合成音声の品質が低かった．そのため，一人当たりの文章数は100文章程度と少ないながらも，100人分の話者からなるJVSを利用して再学習することによって，学習外話者に対する合成音声の品質を向上させた．最適化手法にはAdamWを利用し，$\beta_{1} = 0.8$，$\beta_{2} = 0.99$，$\lambda = \num{1.0e-5}$とした．学習率は\num{2.0e-4}から開始し，1エポック経過するごとに0.99かけて徐々に減衰させた．バッチサイズは16とし，ここではGradient Accumulationは利用しなかった．モデルへの入力は1秒を上限とし，それを超える場合はランダムにトリミング，それに満たない場合はゼロパディングした．勾配のノルムは3.0を上限としてクリッピングすることで，過度に大きくなることを防止した．最大エポック数は30とし，ここではEarly Stoppingは適用しなかった．また，学習終了時には検証データに対する損失（メルスペクトログラムに対するL1 Loss）が最も小さかったエポックにおけるチェックポイントを保存し，これをテストデータに対する評価に用いた．また，Multi-input Vocoderの提案された先行研究\cite{choi2023intelligible}においては，学習時にあえてメルスペクトログラムにノイズをかけることによって，合成音声に対する汎化性能を向上させる学習方法が提案されている．本研究では，動画から推定されるメルスペクトログラムとHuBERT離散特徴量の推定精度向上に焦点を当てたため，Multi-input Vocoderの学習は原音声から計算される特徴量そのもので行い，ボコーダ自体の汎化性能向上による精度改善は追求しなかった．

実装に用いた深層学習ライブラリはPyTorchおよびPyTorch Lightningである．GPUにはNVIDIA RTX A4000を利用し，計算の高速化のためAutomatic Mixed Precisionを適用した．

\subsubsection{客観評価}
合成音声の客観評価には，二種類の指標を用いた．
一つ目は，音声認識の結果から算出した単語誤り率（Word Error Rate; WER）である．WERの計算方法について，まず，正解文字列$s_{1}$と音声認識モデルによる予測文字列$s_{2}$に対し，レーベンシュタイン距離によってその差分を測る．レーベンシュタイン距離は，二つの文字列を一致させるために必要なトークンの挿入数$I$，削除数$D$，置換数$R$の和の最小値として定義される．WERは，レーベンシュタイン距離を測ることによって得られた$I, D, R$を利用し，
\begin{equation}
    \text{WER}\lr{s_{1}, s_{2}} = \frac{I + D + R}{|s_{1}|}
\end{equation}
で与えられる．ここで，$|s_{1}|$は正解文字列$s_{1}$のトークン数を表す．実際には，音声認識モデルにWhisper\cite{radford2023robust}を利用し，出力される漢字仮名交じり文に対してMeCabを用いて分かち書きを行った上で，jiwerというライブラリを用いて算出した．WhisperはLargeモデルを利用し，MeCabの辞書にはunidicを利用した．WERの値は0\%から100\%であり，この値が低いほど音声認識の誤りが少ないため，より聞き取りやすい音声であると判断した．

二つ目は，話者Embeddingから計算したコサイン類似度である．モデルへの入力値を計算するのに用いた話者識別モデルを同様に利用し，サンプルごとに評価対象音声の話者Embeddingと原音声の話者Embeddingのペアでコサイン類似度を計算した．今回構築するモデルは4人の話者に対応するモデルとなるため，原音声に似た声質の合成音声が得られているかをこの指標で評価した．値は0から1であり，高いほど原音声と類似した合成音声だと判断できる．

\subsubsection{主観評価}
\label{sec4:sec:sbj_explanation}
合成音声の主観評価では，音声の明瞭性と類似性の二点を評価した．今回はクラウドワークスというクラウドソーシングサービスおよび，自作の実験用Webサイトを利用してオンラインで実験を実施した．被験者の条件は，日本語母語話者であること，聴覚に異常がないこと，イヤホンあるいはヘッドホンを用いて静かな環境で実験を実施可能であることとした．被験者の方に行っていただいた項目は，以下の五つである．
\begin{enumerate}
    \item アンケート
    \item 練習試行（明瞭性）
    \item 本番試行（明瞭性）
    \item 練習試行（類似性）
    \item 本番試行（類似性）
\end{enumerate}

一つ目のアンケートでは，被験者についての基本的な統計を取ることを目的として，性別・年齢・実験に利用した音響機器について回答してもらった．性別は，男性，女性，無回答の三つからの選択式とした．年齢は被験者の方に直接数値を入力してもらう形式とした．実験に使用した音響機器は，イヤホン，ヘッドホンの二つからの選択式とした．

二つ目の練習試行（明瞭性）および三つ目の本番試行（明瞭性）では，音声の明瞭性の評価を実施した．初めに練習試行を行っていただくことで実験内容を把握してもらい，その後本番施行を行っていただく流れとした．ここで，練習施行は何度でも実施可能とし，本番試行は一回のみ実施可能とした．
評価項目について，明瞭性は「話者の意図した発話内容を，一回の発話でどの程度聞き取ることができたか」を評価するものとした．実際の評価プロセスは以下の二段階で構成した．
\begin{enumerate}
    \item 音声サンプルのみを一回再生し，発話内容を聞き取ってもらう．
    \item 本来の発話内容を確認してもらい，聴取者が想定していた発話内容と本来の発話内容を照らし合わせ，音声の聞き取りやすさを5段階評価してもらう．
\end{enumerate}
5段階評価の回答項目は以下のようにした．
\begin{enumerate}
    \item 全く聞き取れなかった
    \item ほとんど聞き取れなかった
    \item ある程度聞き取れた
    \item ほとんど聞き取れた
    \item 完全に聞き取れた
\end{enumerate}

実験に利用した音声サンプルについて，練習試行では検証データ，本番試行ではテストデータを用いた．被験者ごとの評価サンプルの割り当て方法をアルゴリズム\ref{sec4:algo:sample-assignment}に示す．$\text{sentences}$は文章のリスト，$\text{method\_names}$が手法名のリスト，$\text{speaker\_names}$が話者名のリスト，$\text{num\_total\_respondents}$が被験者総数である．各被験者について，まず$\text{sentences}$と$\text{method\_names}$をランダムにシャッフルし，それからランダムサンプリングした$\text{speaker\_name}$を合わせて，一つのサンプルを決定するようになっている．この選択方法では，二つのことに注意した．一つ目は，各被験者がユニークな53文章を評価することである．評価の際に本来の発話内容を知ることになるため，それを知った上で同じ発話内容のサンプルが出現した場合，音声自体の明瞭性に関わらず発話内容がわかってしまう可能性があると考え，これを避けるようにした．二つ目は，各手法がなるべく均等な回数出現することである．今回は手法の比較が実験の目的となるため，各被験者がすべての手法を評価することが望ましいと判断した．また，評価に際し音声サンプルを一回だけ聞いてもらうようにした理由は，代用音声をコミュニケーションツールとして利用する場面を想定したとき，会話において何度も聞き返されることはストレスになると考えられるため，一回の発話で意図した発話内容をどの程度聞き取ってもらえるかをその手法の聞き取りやすさとして評価するべきだと考えたからである．
\begin{algorithm}
    \caption{Sample Assignment Algorithm}
    \label{sec4:algo:sample-assignment}
    \begin{algorithmic}[1]
        \Require \text{sentences}: List of sentences
        \Require \text{method\_names}: List of method names
        \Require \text{speaker\_names}: List of speaker names
        \Require \text{num\_total\_respondents}: Total number of respondents
        \State Initialize \text{all\_assignments} $\gets$ []
        \For{\text{respondent\_id} $\gets 1$ to \text{num\_total\_respondents}}
        \State Initialize \text{assignments} $\gets$ []
        \State Randomly shuffle \text{sentences}
        \State Randomly shuffle \text{method\_names}
        \For{$i \gets 0$ to $\text{len}\lr{\text{sentences}} - 1$}
        \State \text{sentence} $\gets \text{sentences}[i]$
        \State \text{method\_name} $\gets \text{method\_names}[i \bmod \text{len}\lr{\text{method\_names}}]$
        \State \text{speaker\_name} $\gets$ Randomly select from \text{speaker\_names}
        \State Append $\lr{\text{respondent\_id}, \text{sentence}, \text{method\_name}, \text{speaker\_name}}$ to \text{assignments}
        \EndFor
        \State \text{all\_assignments} $\gets \text{all\_assignments} \cup \text{assignments}$
        \EndFor
        \State \Return \text{all\_assignments}
    \end{algorithmic}
\end{algorithm}

四つ目の練習試行（類似性）および五つ目の本番試行（類似性）では，評価対象の音声と同一話者の原音声の類似性の評価を実施した．ここでも初めに練習試行を行っていただくことで実験内容を把握してもらい，その後本番施行を行っていただく流れとした．ここで，練習施行は何度でも実施可能とし，本番試行は一回のみ実施可能とした．
評価項目について，類似性は「評価対象の音声が同一話者の原音声とどれくらい似ているか」を評価するものとした．実際の評価プロセスは以下の二段階で構成した．
\begin{enumerate}
    \item 評価対象の音声と原音声を聞き比べてもらう．
    \item 評価対象の音声が原音声にどれくらい似ていたかを五段階評価してもらう．
\end{enumerate}
5段階評価の回答項目は以下のようにした．
\begin{enumerate}
    \item 全く似ていなかった
    \item あまり似ていなかった
    \item やや似ていた
    \item かなり似ていた
    \item 同じ話者に聞こえた
\end{enumerate}
実験に利用した音声サンプルおよび，被験者ごとの評価サンプルの割り当て方法は明瞭性の評価実験と同様にした．ただし，類似性評価においては，評価対象となる音声に対して同一話者の原音声をランダムに選択し，評価対象となる音声とペアで提示できるようにした．また，明瞭性評価では音声サンプルを一回だけ聞いて評価してもらうようにしたが，類似性評価では何度でも聞けるようにした．聞き取りやすさのようにコミュニケーションを想定した評価というより，単に原音声とどの程度似ているかを評価したいと考えたからである．さらに，明瞭性評価では評価時に発話内容を提示したが，類似性評価は発話内容に依存しないため，提示しなかった．類似性評価は発話内容に依存しないと考えらえるからである．加えて，類似性評価は明瞭性評価を完了した後にしか実施できないようにした．類似性評価と明瞭性評価に用いるサンプルは同一の発話内容のパターンであり，特に類似性評価では原音声を聴取できることから，本来の発話内容を完全に把握できると予想される．その上で明瞭性評価を行った場合，音声自体の明瞭性に関わらず発話内容がわかってしまう可能性があると考えられ，望ましくない．一方，類似性評価は発話内容に依存しない評価であるため，発話内容を知っていることが評価に影響を与えないと考えられる．よって，今回は明瞭性評価の後に類似性評価を行うことにした．

また，オンラインでの評価は効率よく数多くの方に評価していただけるという点でメリットがあるが，オフラインでの評価と比較して実験環境を制御することが難しく，評価品質が低下する恐れがある．これに対して，本実験では先行研究\cite{kirkland2023stuck}を参考に，評価サンプル中にダミー音声を混入させることで対策を講じた．ダミー音声は本研究で得られた合成音声とは無関係に，gTTSというライブラリを用いて生成したサンプルである．具体例として，明瞭性評価では
\begin{quote}
    これはダミー音声です．明瞭性は「3: ある程度聞き取れた」を選択してください．
\end{quote}
のような発話内容の音声を，類似性評価では
\begin{quote}
    これはダミー音声です．類似性は「1: 全く同じ話者には聞こえなかった」を選択してください．
\end{quote}
のような発話内容の音声を提示した．この時，その音声自体の明瞭性や類似性とは無関係に，必ずこの音声によって指定された評価値を選択するよう説明を与えた．本番試行においてダミー音声で指定された評価値を誤って選んだ場合は，すべての回答を無効にする旨を被験者に伝えた．実際，実験終了後にはそのようにデータを処理した．

被験者数は75人とし，謝礼は実験一回あたり40分程度要すると予想し，650円とした．

\subsection{結果}
\subsubsection{客観評価1: ベースラインと提案手法の比較}
\label{sec4:sec:obj_1}
本節では，ベースラインと提案手法の比較を行う．比較手法は，以下の五つである．
\begin{enumerate}
    \item ベースライン
    \item ネットワークA
    \item ネットワークB（Not-Pretrained）
    \item ネットワークC（Not-Pretrained）
    \item ネットワークB（Pretrained）
    \item ネットワークC（Pretrained）
\end{enumerate}
ベースラインは，提案手法（図\ref{sec4:fig:network}）におけるネットワークAで，メルスペクトログラムとHuBERT離散特徴量を予測するマルチタスク学習手法である．ネットワークAは提案手法において，ネットワークBおよびCに入力を与える役割を果たすネットワークである．提案手法自体ではないが，その構成要素とはなっているため，ここでも客観評価指標を確認することとした．ネットワークB（Not-Pretrained）は，HuBERT Transformer層で事前学習済み重みを読み込まなかった場合のネットワークBである．これに関連して，ネットワークC（Not-Pretrained）はネットワークB（Not-Pretrained）を利用したネットワークCである．これらに対し，ネットワークB（Pretrained）およびネットワークC（Pretrained）は，ネットワークBの学習時にHuBERT Transformer層で事前学習済み重みを読み込んだ点のみ異なる．

まず，損失関数~\eqref{sec4:eq:loss}の重み係数$\lambda_{ssl^{d}}$を変化させた時の，客観評価指標の全テストデータに渡る平均値を表~\ref{sec4:tab:obj_weights}に示す．各手法ごとに$\lambda_{ssl^{d}}$を0.0001から1.0まで，10倍刻みで5段階検討するグリッドサーチを行い，各手法の客観評価指標ごとに，最も優れた値を下線で示している．ここで，提案手法ではネットワークAを学習し，その後Aを固定してBを学習し，最後にAとBを固定してCを学習するように実装している．これに対し，$\lambda_{ssl}^{d}$のグリッドサーチでは，この一連の流れに対して一つの値を検討した．例えば，ネットワークB（Not-Pretrained）で$\lambda_{ssl^{d}}$が0.0001の場合，ネットワークAには$\lambda_{ssl^{d}}$が0.0001の場合を用いている．ネットワークC（Not-Pretrained）で$\lambda_{ssl^{d}}$が0.0001の場合，ネットワークA，Bともに$\lambda_{ssl^{d}}$が0.0001の場合を用いている．また，この後手法ごとの比較を行う際には，各手法ごとにグリッドサーチで得られた最適な場合同士を比較するため，最適だと判断した場合を太字で示している．

\begin{table*}[bt]
    \centering
    \caption{損失関数の重み係数$\lambda_{ssl^{d}}$による客観評価指標の比較}
    \label{sec4:tab:obj_weights}
    \begin{center}
        \renewcommand{\arraystretch}{1.0} % 行の高さ調整
        \setlength{\tabcolsep}{8pt}      % 列の幅調整
        \scalebox{1.0}{
            \begin{tabular}{|c|l|rr|}
                \hline
                \multicolumn{1}{|c|}{手法}         & \multicolumn{1}{c|}{$\lambda_{ssl^{d}}$} & \multicolumn{1}{c}{WER [\%]} & \multicolumn{1}{c|}{話者類似度} \\
                \hline
                ベースライン                           & 0.0001                                   & 57.3                         & 0.833                      \\
                \textbf{ベースライン}                  & \textbf{0.001}                           & \underline{\textbf{54.6}}    & \textbf{0.836}             \\
                ベースライン                           & 0.01                                     & 56.2                         & \underline{0.837}          \\
                ベースライン                           & 0.1                                      & 58.2                         & 0.784                      \\
                ベースライン                           & 1                                        & 59.0                         & 0.685                      \\
                \hline
                ネットワークA                          & 0.0001                                   & 55.4                         & 0.841                      \\
                ネットワークA                          & 0.001                                    & 55.1                         & 0.842                      \\
                ネットワークA                          & 0.01                                     & \underline{53.4}             & \underline{0.843}          \\
                ネットワークA                          & 0.1                                      & 54.6                         & 0.809                      \\
                ネットワークA                          & 1                                        & 58.7                         & 0.698                      \\
                \hline
                ネットワークB（Not-Pretrained）          & 0.0001                                   & 60.6                         & \underline{0.852}          \\
                ネットワークB（Not-Pretrained）          & 0.001                                    & 56.7                         & 0.829                      \\
                ネットワークB（Not-Pretrained）          & 0.01                                     & 54.4                         & 0.847                      \\
                \textbf{ネットワークB（Not-Pretrained）} & \textbf{0.1}                             & \underline{\textbf{45.3}}    & \textbf{0.840}             \\
                ネットワークB（Not-Pretrained）          & 1                                        & 45.5                         & 0.712                      \\
                \hline
                ネットワークC（Not-Pretrained）          & 0.0001                                   & 58.5                         & \underline{0.860}          \\
                ネットワークC（Not-Pretrained）          & 0.001                                    & 55.0                         & 0.850                      \\
                ネットワークC（Not-Pretrained）          & 0.01                                     & 56.0                         & 0.848                      \\
                \textbf{ネットワークC（Not-Pretrained）} & \textbf{0.1}                             & \underline{\textbf{45.8}}    & \textbf{0.848}             \\
                ネットワークC（Not-Pretrained）          & 1                                        & 46.9                         & 0.763                      \\
                \hline
                ネットワークB（Pretrained）              & 0.0001                                   & 60.1                         & 0.841                      \\
                ネットワークB（Pretrained）              & 0.001                                    & 57.1                         & 0.839                      \\
                ネットワークB（Pretrained）              & 0.01                                     & 56.8                         & \underline{0.860}          \\
                \textbf{ネットワークB（Pretrained）}     & \textbf{0.1}                             & \underline{\textbf{44.2}}    & \textbf{0.778}             \\
                ネットワークB（Pretrained）              & 1                                        & 48.1                         & 0.685                      \\
                \hline
                ネットワークC（Pretrained）              & 0.0001                                   & 57.8                         & 0.861                      \\
                ネットワークC（Pretrained）              & 0.001                                    & 57.2                         & 0.865                      \\
                ネットワークC（Pretrained）              & 0.01                                     & 55.7                         & \underline{0.870}          \\
                \textbf{ネットワークC（Pretrained）}     & \textbf{0.1}                             & \underline{\textbf{45.5}}    & \textbf{0.849}             \\
                ネットワークC（Pretrained）              & 1                                        & 46.2                         & 0.753                      \\
                \hline
            \end{tabular}
        }
    \end{center}
\end{table*}

\begin{figure}[bt]
    \centering
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[height=55mm]{./figure/sec4/learning_curves/0/mel_loss.png}
        \caption{メルスペクトログラムのMAE Loss（式~\eqref{sec4:eq:loss}の$L_{mel}$）}
        \label{sec4:fig:learning_curve_baseline_val_mel_loss}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[height=55mm]{./figure/sec4/learning_curves/0/ssl_feature_cluster_loss.png}
        \caption{HuBERT離散特徴量のCross Entropy Loss（式~\eqref{sec4:eq:loss}の$L_{ssl^{d}}$）}
        \label{sec4:fig:learning_curve_baseline_val_ssl_feature_cluster_loss}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[height=55mm]{./figure/sec4/learning_curves/0/total_loss.png}
        \caption{損失の合計値（式~\eqref{sec4:eq:loss}の$L$）}
        \label{sec4:fig:learning_curve_baseline_val_total_loss}
    \end{subfigure}
    \caption{ベースラインにおける学習曲線}
    \label{sec4:fig:learning_curve_baseline_val_losses}
\end{figure}

\begin{figure}[bt]
    \centering
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[height=55mm]{./figure/sec4/learning_curves/2/mel_loss.png}
        \caption{メルスペクトログラムのMAE Loss（式~\eqref{sec4:eq:loss}の$L_{mel}$）}
        \label{sec4:fig:learning_curve_method_2_val_mel_loss}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[height=55mm]{./figure/sec4/learning_curves/2/ssl_feature_cluster_loss.png}
        \caption{HuBERT離散特徴量のCross Entropy Loss（式~\eqref{sec4:eq:loss}の$L_{ssl^{d}}$）}
        \label{sec4:fig:learning_curve_method_2_val_ssl_feature_cluster_loss}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[height=55mm]{./figure/sec4/learning_curves/2/total_loss.png}
        \caption{損失の合計値（式~\eqref{sec4:eq:loss}の$L$）}
        \label{sec4:fig:learning_curve_method_2_val_total_loss}
    \end{subfigure}
    \caption{ネットワークB（Not-Pretrained）における学習曲線}
    \label{sec4:fig:learning_curve_method_2_val_losses}
\end{figure}

ベースラインでは，$\lambda_{ssl^{d}}$の値が0.001のときにWERが最も低く，0.01のときに話者類似度が最も高くなった．現状WERの高さが特に課題であり，話者類似度はほとんど同じであったため，今回は0.001が最適であると判断した．次に，図~\ref{sec4:fig:learning_curve_baseline_val_losses}にベースラインにおける学習曲線の結果を示す．横軸がエポック数，縦軸が損失の値を表す．損失の値は各エポックにおける平均値である．実線は検証データに対する損失，点線は学習データに対する損失を表しており，線の色は$\lambda_{ssl^{d}}$の違いを表す．また，丸いマーカーは表~\ref{sec4:tab:obj_method_comp}に示した最良エポック時における検証データに対する損失の値を表す．学習曲線より，$\lambda_{ssl^{d}}$の値を変化させることによって，特に$L_{ssl^{d}}$の傾向が変化していることがわかる．具体的には，$\lambda_{ssl^{d}}$の値を0.0001から1.0へと増加させるのに伴って，学習初期における損失の下がり方が急峻になっており，達する最小値自体が小さくなっていることがわかる．また，$\lambda_{ssl^{d}}$の値が0.1以上の場合，検証データに対する$L_{ssl^{d}}$は早いうちから増加傾向に転じている．これに伴い，今回は検証データに対する$L$の値を監視し，Early Stoppingの適用と最良エポックの決定を行なったため，$L_{mel}$が下がり切らない状態で学習が中断される結果となった．離散化して冗長性を排除したHuBERT離散特徴量と比較して，メルスペクトログラムが特に話者性を反映するために必要な特徴量だと考えられるため，$\lambda_{ssl^{d}}$が0.1以上の場合に見られた話者類似度の顕著な低下は，$L_{mel}$を下げきれなかったことが原因だと考えられる．

ネットワークAでは，提案手法の構成要素であるため，特に最適な手法の選択は行なっていない．ベースラインとの違いは$L_{ssl^{i}}$が損失に含まれることであるが，客観評価指標より，ベースラインと性能は概ね同等であることがわかる．学習曲線の傾向については，ベースラインと同様であったため省略する．

ネットワークB（Not-Pretrained）では，$\lambda_{ssl^{d}}$の値が0.1の時にWERが最も低く，0.0001の時に話者類似度が最も高くなった．ここではWERの低さを優先し，0.1が最適だと判断した．次に，図~\ref{sec4:fig:learning_curve_method_2_val_losses}にネットワークB（Not-Pretrained）における学習曲線の結果を示す．ベースラインと同様に，$\lambda_{ssl^{d}}$の値を0.0001から1.0へと増加させるのに伴って，学習初期における$L_{ssl^{d}}$の下がり方が急峻になっており，達する最小値自体が小さくなっていることがわかる．また，$\lambda_{ssl^{d}}$が1.0の場合に$L_{mel}$を下げきれなくなる傾向が見られ，ベースラインと同様にこのとき話者類似度の顕著な低下が確認された．加えて，ネットワークB（Not-Pretrained）では$\lambda_{ssl^{d}}$が0.1の場合における$L_{ssl^{d}}$の増加が緩やかであり，$L_{mel}$も十分下げられていることがわかる．さらに，$\lambda_{ssl^{d}}$が0.1の場合，$L_{mel}$の達する最小値自体が，$\lambda_{ssl^{d}}$が0.01以下の場合と比較して小さくなっていることがわかる．これはベースラインでは見られなかった新たな傾向であった．最適な$\lambda_{ssl^{d}}$の値は客観評価指標から0.1としたが，学習曲線の挙動より，この時他の値の場合と比較して$L_{mel}$と$L_{ssl^{d}}$の両方をバランスよく下げられていたと言える．

ネットワークC（Not-Pretrained）では，$\lambda_{ssl^{d}}$が0.1の場合が最適だと判断した．判断理由はネットワークB（Not-Pretrained）と同様である．学習曲線の傾向については，ネットワークB（Not-Pretrained）と同様であったため省略する．

ネットワークB（Pretrained）では，$\lambda_{ssl^{d}}$を0.1としたときにWERが最低となる一方で，0.01以上の場合と比較したときの話者類似度の低下が顕著であり，事前学習済み重みを読み込まなかったネットワークB（Not-Pretrained）とは異なる傾向であった．今回はWERが低いことを優先して，0.1が最適であると判断した．学習曲線の傾向については，ネットワークB（Not-Pretrained）と同様であったため省略する．学習曲線が同様であるにも関わらず結果の傾向が異なったことについては，重みの初期値が異なっていれば損失の値が同様だとしても異なる局所最適解に収束する可能性があるため，妥当だと判断した．

ネットワークC（Pretrained）では，$\lambda_{ssl^{d}}$の値が0.1の時にWERが最も低く，0.01の時に話者類似度が最も高くなった．ここでもWERが最低であることを優先して，0.1が最適だと判断した．学習曲線の傾向はネットワークB（Not-Pretrained）と同様であったため省略する．

次に，最適なチューニングをした場合における，手法ごとの客観評価指標の全テストデータに渡る平均値を表~\ref{sec4:tab:obj_method_comp}に示す．分析合成は，原音声から計算した特徴量を入力として，Multi-input Vocoderで逆変換した合成音声であり，本実験下において合成音声により達成され得る上限値を表す．ベースラインからネットワークC（Pretrained）については，表~\ref{sec4:tab:obj_weights}において太字としたもの，すなわち最適なチューニングだと判断されたものを選択している．また，ベースラインからネットワークC（Pretrained）の中で，最も優れた値を下線で示している．これより，提案手法であるネットワークB（Not-Pretrained），ネットワークC（Not-Pretrained），ネットワークC（Pretrained）の三つは，ベースラインに対してWERと話者類似度の両方を改善したことがわかる．一方，ネットワークB（Pretrained）はWERの改善を達成したが，話者類似度については悪化したことがわかる．ここで，HuBERTの事前学習済み重みを初期値とすることの効果について，ネットワークB（Not-Pretrained）とネットワークB（Pretrained）を比較すると，ネットワークB（Pretrained）の方がWERは1.1\%低いが，話者類似度も0.062低いことがわかる．実際に音声を聞いてみると，音声に不自然なノイズが含まれており，原音声に対する類似性が下がっていることが確認された．よって，HuBERTの事前学習済み重みを初期値としたHuBERT Transformer層の転移学習は，本タスクにおいて有効でないと考えられる．また，ネットワークCの導入効果について，ネットワークB（Not-Pretrained）とネットワークC（Not-Pretrained）を比較すると，ほとんど結果が変わらないことがわかる．一方，ネットワークB（Pretrained）とネットワークC（Pretrained）を比較すると，特に話者類似度についてネットワークC（Pretrained）はネットワークB（Pretrained）よりも0.071高いことがわかる．これより，ネットワークCの効果は，ベースとなるネットワークBの性能に依存して変化すると考えられる．

\begin{table*}[bt]
    \centering
    \caption{最適なチューニングをした場合における手法ごとの比較}
    \label{sec4:tab:obj_method_comp}
    \begin{center}
        \renewcommand{\arraystretch}{1.0} % 行の高さ調整
        \setlength{\tabcolsep}{8pt}      % 列の幅調整
        \scalebox{1.0}{
            \begin{tabular}{|c|l|rr|}
                \hline
                \multicolumn{1}{|c|}{手法} & \multicolumn{1}{c|}{$\lambda_{ssl^{d}}$} & \multicolumn{1}{c}{WER [\%]} & \multicolumn{1}{c|}{話者類似度} \\
                \hline
                ベースライン                   & 0.001                                    & 54.6                         & 0.836                      \\
                ネットワークB（Not-Pretrained）  & 0.1                                      & 45.3                         & 0.840                      \\
                ネットワークC（Not-Pretrained）  & 0.1                                      & 45.8                         & 0.848                      \\
                ネットワークB（Pretrained）      & 0.1                                      & \underline{44.2}             & 0.778                      \\
                ネットワークC（Pretrained）      & 0.1                                      & 45.5                         & \underline{0.849}          \\
                \hline
                分析合成                     & \multicolumn{1}{c|}{-}                   & 3.7                          & 0.956                      \\
                原音声                      & \multicolumn{1}{c|}{-}                   & 3.7                          & 1.000                      \\
                \hline
            \end{tabular}
        }
    \end{center}
\end{table*}

\subsubsection{客観評価2: 提案手法のさらなる検討}
\label{sec4:sec:obj_2}
本節では，\ref{sec4:sec:obj_1}節の客観評価によって得られた知見をもとに，さらなる提案手法の検討を行った結果を述べる．まず，\ref{sec4:sec:obj_1}節より，これまでに検討した提案手法の中で最も優れているのはネットワークB（Not-Pretrained）だと判断した．選択理由について，まずネットワークC（Not-Pretrained）とネットワークB（Not-Pretrained）はほとんど差がないため，ネットワークCの導入効果はほとんどないと判断しネットワークC（Not-Pretrained）を省いた．次に，ネットワークB（Pretrained）はネットワークB（Not-Pretrained）と比較してWERは1.2\%低いがその差は小さく，一方で話者類似度では顕著な低下が見られた．よって，ネットワークB（Pretrained）は本実験下においては効果的でないと判断した．最後に，ネットワークC（Pretrained）について，客観評価指標の値はネットワークB（Not-Pretrained）と同等であり，元となっているネットワークB（Pretrained）を省いたことも考慮して，ネットワークC（Pretrained）も省くこととした．

ネットワークB（Not-Pretrained）に対し，まずはHuBERT Transformer層への入力特徴量について検討した．\ref{sec4:sec:obj_1}節では事前学習済み重みを用いることの有効性を調べる目的があったため，入力は事前学習時に揃えてHuBERT中間特徴量としていた．しかし，選択したネットワークB（Not-Pretrained）は事前学習済み重みを利用しないため，HuBERT中間特徴量を入力とすることが意味をなしているか不明である．これに対し，今回はネットワークAからマルチタスク学習によって同時に予測される，メルスペクトログラムとHuBERT離散特徴量を入力とする場合を比較した．入力の際，メルスペクトログラムは連続したフレームをチャンネル方向に積んで，100Hz・80次元から50Hz・160次元に形状を変換した．HuBERT離散特徴量は，モデルから出力される予測確率分布をそのまま利用した．HiFi-GAN入力のように，一度トークンに変換してからベクトル表現を得るという手段も考えられたが，トークンに変換するためには予測確率を元にトークンをサンプリングする必要があり，これは微分不可能な操作であるため学習が不可能となる．そのため，ここでは予測確率分布をそのまま特徴量として利用することにした．パディング部のトークンを含むため，予測確率分布の次元は101次元である．これらを結合して$160 + 101 = 261$次元の特徴量とした上で，全結合層を通してHuBERT中間特徴量と同じ768次元までチャンネル数を上げ，HuBERT Transformer層への入力とした．これまでと同様に，五種類の$\lambda_{ssl^{d}}$でグリッドサーチを行った結果を表\ref{sec4:tab:obj_weights_networkb_input_comparison}に示す．新たに検討した手法はネットワークB（Not-Pretrained・Mel-Hub）とした．客観評価指標ごとに最も優れた値を下線で示しており，ネットワークB（Not-Pretrained・Mel-Hub）については最適だと判断した場合を太字で示している．これより，ネットワークB（Not-Pretrained・Mel-Hub）では，ネットワークB（Not-Pretrained）で見られたようなWERが下がる重みが確認されないことがわかる．最適だと判断した$\lambda_{ssl^{d}}$が0.001の場合においても，話者類似度はネットワークB（Not-Pretrained）よりも0.005とわずかに高いが，WERは10.1\%高い．これより，ネットワークB（Not-Pretrained・Mel-Hub）は，ネットワークB（Not-Pretrained）に劣ると判断した．この結果より，HuBERT中間特徴量はメルスペクトログラムとHuBERT離散特徴量の複合特徴量と比較して，ネットワークBの推定精度改善につながるより良い入力特徴量であったと考えられる．HuBERT中間特徴量は768次元であったのに対し，メルスペクトログラムとHuBERT離散特徴量の複合特徴量は261次元であるから，より高次元で冗長性が高い方が，ネットワークBによる特徴抽出の対象として優れていたのではないかと考える．

\begin{table*}[bt]
    \centering
    \caption{HuBERT Transformer層への入力特徴量を変化させた場合の比較}
    \label{sec4:tab:obj_weights_networkb_input_comparison}
    \begin{center}
        \renewcommand{\arraystretch}{1.0} % 行の高さ調整
        \setlength{\tabcolsep}{8pt}      % 列の幅調整
        \scalebox{0.98}{
            \begin{tabular}{|c|l|rr|}
                \hline
                \multicolumn{1}{|c|}{手法}                 & \multicolumn{1}{c|}{$\lambda_{ssl^{d}}$} & \multicolumn{1}{c}{WER [\%]} & \multicolumn{1}{c|}{話者類似度} \\
                \hline
                ネットワークB（Not-Pretrained・Mel-HuB）          & 0.0001                                   & 59.5                         & 0.840                      \\
                \textbf{ネットワークB（Not-Pretrained・Mel-HuB）} & \textbf{0.001}                           & \textbf{55.4}                & \underline{\textbf{0.845}} \\
                ネットワークB（Not-Pretrained・Mel-HuB）          & 0.01                                     & 56.2                         & 0.803                      \\
                ネットワークB（Not-Pretrained・Mel-HuB）          & 0.1                                      & 57.7                         & 0.795                      \\
                ネットワークB（Not-Pretrained・Mel-HuB）          & 1                                        & 58.1                         & 0.711                      \\
                \hline
                ネットワークB（Not-Pretrained）                  & 0.1                                      & \underline{45.3}             & 0.840                      \\
                \hline
            \end{tabular}
        }
    \end{center}
\end{table*}

次に，HuBERT中間特徴量をネットワークBに与えている，ネットワークAの学習方法を検討した．これまでの提案手法では，先行研究\cite{kim2023lip_multitask, choi2023intelligible}において報告されたマルチタスク学習の有効性を考慮し，ネットワークAにおいてHuBERT中間特徴量だけでなく，メルスペクトログラム，HuBERT離散特徴量を同時に予測するマルチタスク学習を採用していた．これに対し，ネットワークAにおいてHuBERT中間特徴量のみを推定するよう学習する場合を検討することで，マルチタスク学習の有効性を調べた．結果を表\ref{sec4:tab:obj_weights_networka_multitask}に示す．新たに検討した手法はネットワークB（Not-Pretrained・A-SingleTask）とした．客観評価指標ごとに最も優れた値を下線で示しており，ネットワークB（Not-Pretrained・A-SingleTask）については最適だと判断した場合を太字で示している．これより，ネットワークB（Not-Pretrained・A-SingleTask）で最適な場合を見ると，ネットワークB（Not-Pretrained）よりもWERが2.8\%低く，話者類似度は0.007高いことがわかる．従って，ネットワークAではネットワークBの入力に必要なHuBERT中間特徴量のみを推定する方が，メルスペクトログラムとHuBERT離散特徴量を同時に予測するマルチタスク学習を行うよりも，ネットワークBにより良い入力特徴量を与えられると考えられる．マルチタスク学習を行う場合，メルスペクトログラムやHuBERT離散特徴量が損失に加わるため，それらの損失を小さくするための勾配も考慮した重みの更新が行われるが，これがネットワークBに入力するHuBERT中間特徴量を推定する上では，悪影響を与えていると考えられる．

\begin{table*}[bt]
    \centering
    \caption{ネットワークAにおけるマルチタスク学習の有無による比較}
    \label{sec4:tab:obj_weights_networka_multitask}
    \begin{center}
        \renewcommand{\arraystretch}{1.0} % 行の高さ調整
        \setlength{\tabcolsep}{8pt}      % 列の幅調整
        \scalebox{0.96}{
            \begin{tabular}{|c|l|rr|}
                \hline
                \multicolumn{1}{|c|}{手法}                      & \multicolumn{1}{c|}{$\lambda_{ssl^{d}}$} & \multicolumn{1}{c}{WER [\%]} & \multicolumn{1}{c|}{話者類似度} \\
                \hline
                ネットワークB（Not-Pretrained・A-SingleTask）          & 0.0001                                   & 54.0                         & \underline{0.867}          \\
                ネットワークB（Not-Pretrained・A-SingleTask）          & 0.001                                    & 52.2                         & 0.865                      \\
                ネットワークB（Not-Pretrained・A-SingleTask）          & 0.01                                     & 51.8                         & 0.843                      \\
                \textbf{ネットワークB（Not-Pretrained・A-SingleTask）} & \textbf{0.1}                             & \underline{\textbf{42.5}}    & \textbf{0.847}             \\
                ネットワークB（Not-Pretrained・A-SingleTask）          & 1                                        & 43.0                         & 0.768                      \\
                \hline
                ネットワークB（Not-Pretrained）                       & 0.1                                      & 45.3                         & 0.840                      \\
                \hline
            \end{tabular}
        }
    \end{center}
\end{table*}

\subsubsection{主観評価}
主観評価実験は，本実験で検討した提案手法がベースラインに対する改善を達成したか否かを確認するために行った．\ref{sec4:sec:obj_1}節および\ref{sec4:sec:obj_2}節の検討結果より，本研究の提案手法の代表としては，ネットワークB（Not-Pretrained）と，ネットワークB（Not-Pretrained・SingleTask）を選択した．なぜなら，客観評価指標の結果から，最良だと考えられる提案手法はネットワークB（Not-Pretrained・SingleTask）であるが，ネットワークB（Not-Pretrained）もそれに次ぐ性能であり，ネットワークAにおけるマルチタスク学習の有無が主観評価において有意な差であるかも確かめたいと考えたからである．結果として，主観評価実験において比較する音声は以下の五種類である．
\begin{enumerate}
    \item ベースライン
    \item ネットワークB（Not-Pretrained）
    \item ネットワークB（Not-Pretrained・A-SingleTask）
    \item 分析合成
    \item 原音声
\end{enumerate}
\ref{sec4:sec:sbj_explanation}節で述べたように，主観評価では音声の明瞭性と類似性を五段階評価した．総被験者数は75人としたが，アプリの不具合によるエラーを理由に一名のデータを除き，それ以外の被験者についてはダミーサンプルの回答が正しかったためそのまま用いて，74名分のデータで統計処理を実施した．被験者の年齢層は21歳から62歳に渡った．年齢層の箱ひげ図を図\ref{sec4:fig:age}に示す．被験者の性別は男性32名，女性42名であった．また，実験に利用した音響機器はヘッドホンが18名，イヤホンが56名であった．
\begin{figure}[bt]
    \centering
    \includegraphics[height=50mm]{./figure/sec4/sbj/age.png}
    \caption{主観評価実験における被験者の年齢層}
    \label{sec4:fig:age}
\end{figure}

明瞭性と類似性の評価値について，手法ごとに平均値と95\%信頼区間を計算した結果を表\ref{sec4:tab:sbj_mean_ci}に示す．また，各手法の組み合わせについて，平均値の差の検定（片側検定）を行った結果を表\ref{sec4:tab:sbj_int_p}, \ref{sec4:tab:sbj_sim_p}に示す．表における$\lr{i, j}$成分は，$i$行目の手法に対する評価値の母平均を$\mu_{i}$，$j$列目の手法に対する評価値の母平均を$\mu_{j}$とするとき，帰無仮説を$\mu_{i} = \mu_{j}$，対立仮説を$\mu_{i} > \mu_{j}$とした片側検定で計算されたp値に対し，Benjamini/Hochberg法による多重比較のための補正を行った結果である．ここで，表の文字列はそれぞれの以下の略称とする．
\begin{enumerate}
    \item GT: 原音声（Ground Truth）
    \item AbS: 分析合成（Analysis by Synthesis）
    \item B（N-P, A-S）: ネットワークB（Not-Pretrained・A-SingleTask）
    \item B（N-P）: ネットワークB（Not-Pretrained）
    \item Baseline: ベースライン
\end{enumerate}
また，本実験における有意水準は5\%とする．

まず，表\ref{sec4:tab:sbj_int_p}より，提案したネットワークB（Not-Pretrained）およびネットワークB（Not-Pretrained・A-SingleTask）は，ベースラインに対して明瞭性の評価値が有意に高いことがわかる．これより，二つの提案手法はどちらもベースラインより話者の想定した発話内容を正確に反映した合成音声を実現できたと考えられる．一方，提案手法二つの間には有意差がないことも分かる．これより，ネットワークAの学習方法の違いは明瞭性に有意な差をもたらさなかったと言える．次に，表\ref{sec4:tab:sbj_sim_p}より，提案したネットワークB（Not-Pretrained）およびネットワークB（Not-Pretrained・A-SingleTask）は，ベースラインに対して類似性の評価値が有意に高いことがわかる．これより，二つの提案手法はどちらもベースラインより原音声に似た合成音声を実現できたと考えられる．また，ここでは提案手法二つの間にも有意差があることが分かる．これより，ネットワークAの学習方法の違いは明瞭性に有意な差をもたらさなかったが，類似性には有意な差をもたらしたと言える．

以上のことから，ネットワークB（Not-Pretrained・A-SingleTask）が明瞭性・類似性の両面において，最も優れた合成音声を実現したと考えられる．一方，ネットワークB（Not-Pretrained・A-SingleTask）と，合成音声の性能上限を表す分析合成の間には未だ大きな差があり，特に自然音声に迫る合成音の実現は達成されていない．従って，本実験におけるベースラインからの改善は達成したものの，今後もさらなるネットワークの改善が必要だと考えられる．

\begin{table*}[bt]
    \centering
    \caption{主観評価実験の結果より計算した標本平均と95\%信頼区間}
    \label{sec4:tab:sbj_mean_ci}
    \begin{center}
        \renewcommand{\arraystretch}{1.0} % 行の高さ調整
        \setlength{\tabcolsep}{8pt}      % 列の幅調整
        \scalebox{0.95}{
            \begin{tabular}{|c|l|rr|}
                \hline
                \multicolumn{1}{|c|}{手法}             & \multicolumn{1}{c|}{$\lambda_{ssl^{d}}$} & \multicolumn{1}{c}{明瞭性} & \multicolumn{1}{c|}{類似性} \\
                \hline
                ベースライン                               & 0.001                                    & $2.371 \pm 0.072$       & $2.933 \pm 0.080$        \\
                ネットワークB（Not-Pretrained）              & 0.1                                      & $2.753 \pm 0.075$       & $3.052 \pm 0.085$        \\
                ネットワークB（Not-Pretrained・A-SingleTask） & 0.1                                      & $2.818 \pm 0.079$       & $3.182 \pm 0.082$        \\
                \hline
                分析合成                                 & \multicolumn{1}{c|}{-}                   & $4.749 \pm 0.040$       & $4.316 \pm 0.071$        \\
                原音声                                  & \multicolumn{1}{c|}{-}                   & $4.866 \pm 0.032$       & $4.705 \pm 0.052$        \\
                \hline
            \end{tabular}
        }
    \end{center}
\end{table*}

\begin{table*}[bt]
    \centering
    \caption{主観評価実験の結果より計算した平均値の差の検定におけるp値（明瞭性）}
    \label{sec4:tab:sbj_int_p}
    \begin{center}
        \renewcommand{\arraystretch}{1.0} % 行の高さ調整
        \setlength{\tabcolsep}{8pt}      % 列の幅調整
        \scalebox{0.95}{
            \begin{tabular}{|c|rrrr|}
                \hline
                \multicolumn{1}{|c|}{} & \multicolumn{1}{c}{AbS} & \multicolumn{1}{c}{B（N-P, A-S）} & \multicolumn{1}{c}{B（N-P）} & \multicolumn{1}{c|}{Baseline} \\
                \hline
                GT                     & $5.25 \times 10^{-6}$   & $2.14 \times 10^{-259}$         & $2.82 \times 10^{-285}$    & $0$                           \\
                AbS                    & \multicolumn{1}{c}{-}   & $2.23 \times 10^{-240}$         & $4.56 \times 10^{-266}$    & 0                             \\
                B（N-P, A-S）            & \multicolumn{1}{c}{-}   & \multicolumn{1}{c}{-}           & $1.23 \times 10^{-1}$      & $3.61 \times 10^{-16}$        \\
                B（N-P）                 & \multicolumn{1}{c}{-}   & \multicolumn{1}{c}{-}           & \multicolumn{1}{c}{-}      & $5.77 \times 10^{-13}$        \\
                \hline
            \end{tabular}
        }
    \end{center}
\end{table*}

\begin{table*}[bt]
    \centering
    \caption{主観評価実験の結果より計算した平均値の差の検定におけるp値（類似性）}
    \label{sec4:tab:sbj_sim_p}
    \begin{center}
        \renewcommand{\arraystretch}{1.0} % 行の高さ調整
        \setlength{\tabcolsep}{8pt}      % 列の幅調整
        \scalebox{0.95}{
            \begin{tabular}{|c|rrrr|}
                \hline
                \multicolumn{1}{|c|}{} & \multicolumn{1}{c}{AbS} & \multicolumn{1}{c}{B（N-P, A-S）} & \multicolumn{1}{c}{B（N-P）} & \multicolumn{1}{c|}{Baseline} \\
                \hline
                GT                     & $1.07 \times 10^{-17}$  & $1.93 \times 10^{-156}$         & $1.17 \times 10^{-169}$    & $1.01 \times 10^{-200}$       \\
                AbS                    & \multicolumn{1}{c}{-}   & $1.31 \times 10^{-82}$          & $5.25 \times 10^{-96}$     & $3.53 \times 10^{-118}$       \\
                B（N-P, A-S）            & \multicolumn{1}{c}{-}   & \multicolumn{1}{c}{-}           & $1.70 \times 10^{-2}$      & $1.34 \times 10^{-5}$         \\
                B（N-P）                 & \multicolumn{1}{c}{-}   & \multicolumn{1}{c}{-}           & \multicolumn{1}{c}{-}      & $2.29 \times 10^{-2}$         \\
                \hline
            \end{tabular}
        }
    \end{center}
\end{table*}

\clearpage

\subsection{まとめ}

\clearpage

\section{結論}

\clearpage

\section*{謝辞}
\addcontentsline{toc}{section}{謝辞}

\clearpage

\bibliographystyle{junsrt}
\addcontentsline{toc}{section}{参考文献}
\bibliography{library}

\end{document}