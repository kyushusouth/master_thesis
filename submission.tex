\documentclass[12pt]{jarticle}
\usepackage[utf8]{inputenc}
\usepackage[top=30truemm, bottom=30truemm, left=20truemm, right=20truemm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{siunitx}
\usepackage{bm}
\usepackage[dvipdfmx]{graphicx}
\usepackage[dvipdfmx]{color}
\usepackage[subrefformat=parens]{subcaption}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{tocloft}
\usepackage{enumerate}
\usepackage{url}
\usepackage{multirow}
\usepackage{caption}
\usepackage{enumitem}
\usepackage{makecell}
\usepackage{array}

\numberwithin{equation}{section}    % 数式番号にセクション番号をつける
\numberwithin{figure}{section}      % 図番号にセクション
\numberwithin{table}{section}      % 図番号にセクション

% \renewcommand{\figurename}{Fig.}    % 図 -> Fig.
% \renewcommand{\tablename}{Table }   % 表 -> Table

\renewcommand{\baselinestretch}{1.1}

\newlength{\figcaptionskip}
\setlength{\figcaptionskip}{5pt} % 図のキャプション間隔
\newlength{\tabcaptionskip}
\setlength{\tabcaptionskip}{-5pt} % 表のキャプション間隔
\captionsetup[figure]{skip=\figcaptionskip}
\captionsetup[table]{skip=\tabcaptionskip}

\setlist[enumerate]{topsep=5pt, partopsep=5pt, itemsep=0pt, parsep=0pt}
\setlength{\topsep}{5pt}
\setlength{\partopsep}{5pt}


\begin{document}

\begin{titlepage}
    \begin{center}
        {\Large 2024年度後期　修士論文}
        \vspace{120truept}

        {\huge 深層学習による口唇音声変換に関する研究}
        \vspace{30truept}

        {\huge A Study on the Conversion from Lip-Video to Speech Using a Deep Learning Technique}
        \vspace{120truept}

        {\Large 2025年月日}
        \vspace{10truept}

        {\Large 九州大学芸術工学府音響設計コース}
        \vspace{70truept}

        {\Large 2DS23095M}
        \vspace{10truept}

        {\Large 南 汰翼}
        \vspace{10truept}

        {\Large MINAMI Taisuke}
        \vspace{30truept}

        {\Large 研究指導教員　鏑木 時彦　教授}
    \end{center}
\end{titlepage}

\section*{概要}
\thispagestyle{empty}
\clearpage

\setcounter{tocdepth}{2}
\tableofcontents
\thispagestyle{empty}
\clearpage

\pagestyle{plain}
\setcounter{page}{1}

\section{序論}
\subsection{背景}
音声は基本的なコミュニケーションの手段であり、人と人とのコミュニケーションの場面において、重要な役割を果たしている。音声は、肺からの呼気流による声帯の振動が音源波を生成し、声道特性に伴ったフィルタリングと口唇からの放射特性に従って生成される。これより、音声の生成には音源を作り出す声帯やその制御のための喉頭、舌や口唇といった調音器官の働きが重要となる。しかし、癌などの重い病気で喉頭を摘出した場合、音源波を生成することができなくなるため、これまで通り発声を行うことが不可能になってしまう。このようなコミュニケーション機能の喪失に対し、現在でも電気式人工喉頭や食道発声、シャント発声といった代用音声手法が存在する。電気式人工喉頭では、専用の発振器を顎下に当てて振動を加えることにより、それを音源とした発声を行う。発振器を用意すれば容易に発声することが可能であるが、生成される音声のピッチが発振器の振動に依存してしまうため、抑揚のない単調な音声になってしまう。食道発声では、まず口や鼻から食道内に空気を取り込み、その空気を逆流させることで食道入口部の粘膜を振動させることによって発声する。電気式人工喉頭と違って道具を必要とせず、ピッチも本人が調節できるが、その習得に長期間の訓練を要する。シャント発声では、手術によって気管と食道を繋ぐ管を設ける。これにより、息を吐き出す際に設けられた喉の穴を手で塞ぐことによって肺からの呼気流が食道に流れる。そのため、食道発声と同様に粘膜の振動を音源とし、発声することが可能となる。習得は容易であり、比較的自然に話すことが可能となるが、設けられた管を交換するための定期的な手術が必要となる。このように、現在用いられている代用音声手法にはそれぞれデメリットが存在する。

そのため、本研究ではビデオカメラで撮影した口唇の動きから音声合成を行うことによる、新たな代用音声手法を検討する。本来、音声は声帯の振動や声道の形状に依存して生成されるものであり、口唇の動きのみから音声波形を直接推定することは困難である。そこで、近年画像や自然言語処理、音声といった分野において成果を上げている深層学習を使用し、データ駆動型の方法で口唇の動きと音声の間の関係を学習することで推定を行う。これにより、従来の代用音声手法よりも自然性の高い音声を、訓練や定期的な手術の必要なく提供することを目指す。

これまでの動画音声合成は英語が中心に検討が進んでおり、近年ではYouTube上のデータを収集、処理することによって構築した大規模データセット~\cite{afouras2018lrs3,chung2018voxceleb2}を用いることで、大規模で表現力の高いモデルが構築可能となっている。これにより、従来行われてきた教師あり学習のみならず、動画と音声の関係性を自己教師あり学習（Self Supervised Learning; SSL）によって学習し、そのモデルを動画音声合成や、動画からテキストを推定するVisual Speech Recognition（VSR）にFineTuningするアプローチが提案され、その有効性が示されている。自己教師あり学習モデルにもいくつかの種類があり、近年多くの研究で応用例のあるAVHuBERT~\cite{shi2022learning}は、動画・音声の入力領域においてマスクされた区間の予測と、予測対象の更新を繰り返して学習を進めていくモデルである。予測対象の更新は5回行われ、1回目は音声波形から計算されるMFCCをクラスタリングした結果を利用するが、2回目以降はモデルの中間特徴量をクラスタリングした結果を新たな予測対象に設定する。更新のたびに再度モデルをランダム初期化して再学習するが、その予測対象の複雑さが増していくことによって学習を促進するようなメカニズムとなっている。また、これに類似したVATLM~\cite{zhu2023vatlm}は、動画と音声のみならずテキストも加えた学習によって、精度改善を達成した。その他、StudentとTeacherという二つのネットワークを利用し、Teacherから出力される特徴量をStudentがマスクされた入力から予測することによって学習を進めるRAVEn~\cite{haliassos2022jointly}やAV-data2vec~\cite{lian2023av}、RAVEnの改善版として提案されたBRAVEn~\cite{haliassos2024braven}など、多くのモデルが提案されている。

近年の動画音声合成やVSRでは、こういったSSLモデルを動画からの特徴抽出器として活用しつつ、さらなる工夫によって精度改善を達成している。動画音声合成について、~\cite{kim2023lip_multitask}では予測対象として従来用いられてきたメルスペクトログラムに加え、テキストを予測するマルチタスク学習手法を提案した。損失においては上記の二つに加え、予測したメルスペクトログラムを事前学習済みのASR（Automatic Speech Recognition）モデルに入力して得られる特徴表現も採用した。音声波形はメルスペクトログラムに対してGriffin-Limアルゴリズムを適用することで獲得しており、従来のメルスペクトログラムのみを損失とする手法に対して客観評価指標における改善を達成した。これに続き、~\cite{choi2023intelligible}では前述した手法がテキストアノテーションされたデータのみにしか用いることができないという課題を解消するため、テキストと同様に言語的な情報を持つと考えられている、音声SSLモデルのHuBERT\cite{hsu2021hubert}から得られた離散特徴量（HuBERT離散特徴量とする）を利用する手法を提案した。また、予測されたメルスペクトログラムとHuBERT離散特徴量の両方を入力とするMulti-input Vocoder、Multi-input Vocoderの学習時にメルスペクトログラムにノイズをかけるデータ拡張を合わせて提案し、客観評価と主観評価の両方で改善を達成した。加えて、ここではAVHuBERTの転移学習についても合わせて検討が行われ、これによってさらに性能を改善できることを示した。手法~\cite{choi2023intelligible}に関連して、上記のようなマルチタスク学習手法以外にも、HuBERT離散特徴量やHuBERT連続特徴量（離散化しない場合を指す）を音声波形までの中間特徴量として扱う手法は提案されている。例えば、~\cite{hsu2023revise}ではメルスペクトログラムの推定を行わず、HuBERT離散特徴量のみを推定して音声波形に変換する手法が提案された。~\cite{choi2023intelligible}では離散化におけるクラスタ数を200にしていたのに対して、~\cite{hsu2023revise}ではクラスタ数を2000と大きく取っている点で実装が異なっている。メルスペクトログラムを省略する分情報圧縮の程度を軽減することで、音声波形への変換に十分な情報を保持する目的があると考えられる。また、\cite{choi2023intelligible}や\cite{hsu2023revise}ではAVHuBERTを直接動画音声合成にFineTuningしていた一方で、~\cite{sahipjohn2023robustl2s}ではAVHuBERTをVSRによってFineTuningし、その後重みを固定した上で特徴抽出器として利用するアプローチを提案している。動画音声合成モデルは、VSRでFineTuningしたAVHuBERTから得られる動画特徴量を入力とし、HuBERT特徴量を予測するネットワークを導入して、HuBERT特徴量のみから音声波形に変換するボコーダを利用することで構築される。ここではHuBERT特徴量として連続値および離散値の両方が検討され、連続値を用いる場合の方が客観評価指標が改善することを報告している。検討された離散値のクラスタ数が100であったため、\cite{hsu2023revise}の結果と合わせると、HuBERT離散特徴量のみで音声波形に変換するアプローチを取るのであれば、クラスタ数を十分大きく取る必要があることが予想される。

一方VSRについて、~\cite{yeo2024akvsr}では音声認識を利用して言語情報を格納したメモリを用意し、メモリと動画特徴量の間でアテンションをとることによって、ネットワーク内部で言語情報との関連を考慮する構成を提案した。また、~\cite{cheng2023opensr}ではAVHuBERTが動画あるいは音声のどちらを入力とした場合でもクロスモーダルな特徴量を返すことに着目し、音声認識デコーダに組み合わせるAVHuBERTのFew-shot Learning、Zero-shot Learningによる転移学習を検討した。加えて、同様に音声認識デコーダを転移学習するアプローチであるが、事前学習済みモデルの重みを固定し、動画特徴量から音声認識モデルの中間特徴量を予測するネットワークのみを新たに学習することで、両者を合併するようなアプローチ~\cite{djilali2023lip2vec}も提案されている。さらに、静止画像と音声から動画を合成するネットワークを構築し、音声認識用のデータセットを用いてVSRの学習データを大量に合成するデータ拡張手法~\cite{liu2023synthvsr}や、事前学習済みの音声認識モデルによって教師なしデータにラベリングを行うデータ拡張手法~\cite{ma2023auto}、10万時間分の教師ありデータを新たに増強した研究~\cite{chang2024conformer}など、大規模な学習データを確保することで精度改善を達成した例も報告されている。

上記の研究は英語データを用いたものであったが、VSRにおいては英語以外の言語に焦点を当てた研究や、多言語対応モデルの構築も検討が進んでいる。~\cite{zinonos2023learning}ではRAVEnを利用し、英語に加えてスペイン語、イタリア語、ポルトガル語など計6種類の言語が含まれるデータセット~\cite{ephrat2018looking,salesky2021multilingual,zhao2019cascade}を用いて多言語モデルの構築を検討した。結果として、教師ありデータの少ない英語以外の言語に対する、多言語モデルの有効性が明らかとなった。また、~\cite{kim2023lip_vsr}では英語データで学習されたAVHuBERTを用いつつ、特定の言語ごとに構築した音声認識モデルのデコーダを転移学習することで、特定言語ごとにモデルを構築するアプローチを提案した。さらに、~\cite{yeo2023visual}では音声認識モデルであるWhisperを利用し、教師なしデータへのラベリングによるデータ拡張を行うことで、上記二つのアプローチを超える精度を達成した。

本研究では、近年の主流とも言える英語大規模データセットを用いた実験は計算機のスペックの都合上難しく、世界的に見て日本語での動画音声合成の検討例が少ないことも考慮して、文献~\cite{taguchi,esaki}で収録された日本語データを用いて研究を行うこととした。英語データと比較して小規模なデータである分性能に課題を抱えたが、予備実験として英語データで学習されたAVHuBERTのFineTuningを検討したところ、スクラッチで構築したモデルと比較して、より高い精度を示すことが明らかとなった。これは、英語データを用いた事前学習済みモデルの多言語対応を検討した先行研究の傾向にも一致する結果であり、日本語においても同様に有効なモデルだと考えられる。しかしながら、それでも依然として合成音声の品質は低く、自然音声に迫る合成音は実現されていないことが課題である。

\subsection{目的}
本研究の目的は、動画音声合成によって得られる合成音声の品質を向上させることである。近年高い精度を達成した手法~\cite{choi2023intelligible}では、AVHuBERTの利用および、メルスペクトログラムと音声SSL離散特徴量を利用したマルチタスク学習が採用されている。その他にも近年高い精度を達成したモデルは存在~\cite{hsu2023revise,sahipjohn2023robustl2s,kim2024let}するが、手法~\cite{choi2023intelligible}が採用しているマルチタスク学習の有効性は、テキストを用いた先行研究~\cite{kim2023lip_multitask}でも同様に示されている。これより、このアプローチが現状特に有効そうだと判断し、本研究においてはこの手法をベースラインとして、さらなる改善を狙う形で研究を進めることとした。この手法では、動画を入力としてメルスペクトログラムと音声SSL離散特徴量を推定し、これら両方をMulti-input Vocoderに入力することで音声波形へと変換する。しかし、動画と音声の間には、同様の口の動きであっても声道形状の違いによって生じる発話内容の曖昧さや、話者によるパターンの多様さが存在すると考え、推定を動画のみに依存した先行研究の手法ではこういった側面への対処が難しいと考えた。これに対して本研究では、音声SSLモデルであるHuBERTを利用した動画音声合成モデルを提案し、合成音声の推定残差をHuBERTを利用した後処理によって軽減することで、合成音声の品質改善を狙った。HuBERTは、音声波形を畳み込み層を通すことによってダウンサンプリングしつつ特徴量に変換し、ここでマスクをかけた上でTransformer層を通す。そして、マスクされたフレームにおける予測対象を推定する、Masked Predictionを行うことで学習する。大規模な音声データを用いてこの自己教師あり学習を行うことで、音声のコンテキスト自体をデータそのものから学習することが可能であり、音声認識において有効性が確認されている。本研究では、大規模日本語音声データで事前学習済みのHuBERTを活用し、動画音声合成モデルにおいて生じる推定残差を、音声自体のコンテキストを考慮する形で補うようなアプローチを検討した。

\subsection{本論文の構成}
\clearpage

\section{音声信号処理}
音声にはフォルマントや基本周波数（ピッチ）など、様々な周波数的な特徴が存在している。フォルマントは母音や子音を知覚するため、ピッチはアクセントやイントネーションを表現するために重要なものである。このような音声信号の持つ複雑さから、時間波形のままその特徴を分析することは困難である。これに対し、本節では音声の特徴を捉えやすくするための信号処理について説明する。

\subsection{音声のフーリエ変換}
音声の時間波形に対して、周波数領域での情報を得るためにはフーリエ変換（Fourier Transform）を使用する。特に、音声はマイクロフォンで収録され、コンピュータ内で処理されることが多い。この時、音声はアナログ信号ではなく、サンプリング周波数と量子化ビット数に従って離散化されたデジタル信号として扱われる。このような場合、離散信号に対してのフーリエ変換である離散フーリエ変換（discrete Fourier transform; DFT）が用いられる。また、信号の系列長をゼロパディングして2の冪乗の長さに調整することで、計算量を抑えた高速フーリエ変換（fast Fourier transform; FFT）を用いることができる。

離散信号を$x[n]$、それに対するフーリエ変換を$X[k]$とする。ここで、nはサンプルのインデックス、kは周波数インデックスである。$X[k]$は複素数であり、以下のように極座標表示することができる。
\begin{align}
    X[k] & = \mathrm{Re}(X[k]) + j\mathrm{Im}(X[k]) \\
         & = |X[k]|e^{j\angle\mathrm{X[k]}}
\end{align}
ここで、$|X[k]|$は振幅特性（振幅スペクトル）、$\angle\mathrm{X[k]}$は位相特性（位相スペクトル）であり、以下の式で表される。
\begin{gather}
    |X[k]| = \sqrt{\mathrm{Re}(X[k])^{2} + \mathrm{Im}(X[k])^{2}} \\
    \angle\mathrm{X[k]} = \tan^{-1} \frac{\mathrm{Im}(X[k])}{\mathrm{Re}(X[k])}
\end{gather}
また、$|X[k]|^2$はパワースペクトルと呼ばれる。これにより、信号中にどのような周波数成分がどれくらい含まれているかを調べることができる。しかし、音声はフォルマントやピッチが時々刻々と変化するため、信号全体に対して直接フーリエ変換を適用したとしても有用な結果が得られない。このような音声の持つ非定常性の問題に対して、十分短い時間幅においては信号の定常性が成り立つという仮定のもと、短時間フーリエ変換（short-time Fourier transform; STFT）が用いられる。STFTでは、音声信号に対して窓関数による窓処理を適用し、短時間に区切られた信号それぞれに対してDFTを適用する。ここで、窓処理とはある特定の窓関数と音声信号を時間領域でかけ合わせることであり、窓関数の時間幅を窓長という。また、窓関数を時間方向にシフトするときの時間幅をシフト幅という。STFTには時間分解能と周波数分解能の間に不確定性が存在し、両者の間にトレード・オフの関係がある。窓長が長い場合には周波数分解能が向上する一方、時間分解能が低下する。窓長が短い場合にはその逆となる。音声信号$x[n]$のSTFTを時刻$j$、周波数インデックスを$k$として$X(j, k)$と表すと、$X(j, k)$は時間周波数領域における複素数となる。これを複素スペクトログラムと呼ぶ。また、$|X(j, k)|$を振幅スペクトログラム、$\angle\mathrm{X(j, k)}$を位相スペクトログラム、
$|X(j, k)|^{2}$をパワースペクトログラムと呼ぶ。「小さな鰻屋に、熱気のようなものがみなぎる」と発話した音声に対し、窓関数としてハニング窓を用いた上で、複数の窓長・シフト幅によって計算した対数パワースペクトログラムを、図~\ref{sec2:fig:log_power_spectrograms}に示す。窓長が100msと長い場合には周波数分解能が高いが、時間分解能が低下することでスペクトルの時間変化が滑らかでないことがわかる。一方、窓長が12.5msと短い場合には時間分解能が高いが、周波数分解能が低下することでスペクトルがぼやけていることがわかる。これが窓長に対する時間分解能と周波数分解能とトレード・オフであり、窓長25msや50msが程よいパラメータであることがわかる。
\begin{figure}[tb]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./figure/sec2/spectrogram_1.png}
        \caption{窓長12.5ms、シフト幅5ms}
        \label{sec2:fig:spectrogram1}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./figure/sec2/spectrogram_2.png}
        \caption{窓長25ms、シフト幅10ms}
        \label{sec2:fig:spectrogram2}
    \end{subfigure}

    \vspace{0.5cm}

    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./figure/sec2/spectrogram_4.png}
        \caption{窓長50ms、シフト幅20ms}
        \label{sec2:fig:spectrogram3}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./figure/sec2/spectrogram_8.png}
        \caption{窓長100ms、シフト幅40ms}
        \label{sec2:fig:spectrogram4}
    \end{subfigure}
    \caption{「小さな鰻屋に、熱気のようなものがみなぎる」と発話した音声から計算された対数パワースペクトログラム}
    \label{sec2:fig:log_power_spectrograms}
\end{figure}

\subsection{メルスペクトログラム}
メルスペクトログラムは、振幅スペクトログラムを人間の聴感特性を考慮したメル尺度に変換することによって得られる。周波数軸をメル尺度に変換する際、以下の式を用いる。
\begin{equation}
    Mel(f) = 2595\log_{10} \left(1 + \frac{f}{700}\right)
\end{equation}
メル尺度は、$\SI[]{1000}{Hz}$、$\SI[]{40}{dB}$の純音を$\SI[]{1000}{mel}$とする比率尺度である。メル尺度を用いることにより、低い周波数ほど細かく、高い周波数ほど荒い特徴量になる。メルスペクトログラムは、振幅スペクトログラムに対してメルフィルタバンクを適用することによって得られる。メルフィルタバンクの数は任意に決定できるパラメータであり、メルスペクトログラムの周波数方向の次元はこれに一致する。音声合成においては、音声のサンプリング周波数を$\SI[]{16}{kHz}$とするとき、メルフィルタバンクの数を80とし、$\SI[]{8000}{Hz}$までの帯域に対して適用することが多い。「小さな鰻屋に、熱気のようなものがみなぎる」と発話した音声に対し、窓関数にハニング窓を用い、窓長25ms、シフト幅10msとしてパワースペクトログラムを計算した上で、80次元のメルフィルタバンクを適用して得られた対数メルスペクトログラムを、図~\ref{sec2:fig:melspectrogram}に示す。
\begin{figure}[bt]
    \centering
    \includegraphics[height=90mm]{./figure/sec2/melspectrogram.png}
    \caption{「小さな鰻屋に、熱気のようなものがみなぎる」と発話した音声に対する対数メルスペクトログラム}
    \label{sec2:fig:melspectrogram}
\end{figure}

\clearpage

\section{深層学習}
深層学習とは、人間の神経細胞の仕組みを模擬したニューラルネットワークを用いる機械学習手法のことである。特に近年ではその層を深くしたディープニューラルネットワーク（Deep Neural Network; DNN）が用いられ、大量のパラメータを最適化することによって得られる表現力により、自然言語処理や画像処理、音声認識や音声合成など様々な分野で成果を上げている。本章では、本研究で使用するニューラルネットワーク及び、構築したDNNの学習方法について説明する。

\subsection{ニューラルネットワーク}
\subsubsection{全結合層}
全結合層は、すべての入力ノードがすべての出力ノードに接続される層である。全結合層の出力は、入力に対して学習可能な重みによる線形変換を適用することで得られる。入力$\bm{x} \in \mathbb{R}^{n}$に対し、出力$\bm{y} \in \mathbb{R}^{m}$は、
\begin{align}
    \bm{y} = \bm{W}\bm{x} + \bm{b}
\end{align}
で計算される。ここで、$\bm{W} \in \mathbb{R}^{m \times n}$は重み行列で、各要素$w_{ij}$は入力の$j$番目の成分と、出力の$i$番目の成分の間の重みである。$\bm{b} \in \mathbb{R}^{m}$はバイアスベクトルで、各要素$b_{i}$は出力の$i$番目の成分のバイアス項である。全結合層は特徴量の次元の変換に用いられ、特に最終層において、ネットワークの特徴量を所望の特徴量の次元に変換する場合に便利である。

\subsubsection{畳み込み層}
畳み込み層は、入力に対して畳み込み演算を行う層である。一次元畳み込み層について、入力$\bm{x} \in \mathbb{R}^{C_{in} \times T_{in}}$に対し、出力$\bm{y} \in \mathbb{R}^{C_{out} \times T_{out}}$は、$y_{k}[i]$を出力テンソルの$k$番目のチャネルの$i$番目の成分とすると、
\begin{align}
    y_{k}[i] = b_{k} + \sum_{c=0}^{C_{in} - 1} \sum_{m = 0}^{M - 1} x_{c}\left[i - \left\lfloor \frac{M}{2} \right\rfloor + m\right] w_{k, c}[m]
\end{align}
で計算される。ここで、$x_{c}\left[i - \left\lfloor \frac{M}{2} \right\rfloor + m\right]$が入力テンソルの$c$番目のチャネルの$\left[i - \left\lfloor \frac{M}{2} \right\rfloor + m\right]$番目の成分、$w_{k, c}[m]$が出力チャネル$k$と入力チャネル$c$に対応するカーネルの$m$番目の成分、$M$がカーネルサイズである。上式より、一次元畳み込み層の$i$番目の出力は、$i$番目の入力を中心とし、カーネルサイズの範囲分が考慮されて得られる値だと解釈できる。一次元畳み込みは、自然言語や音声といった一次元系列に対する特徴抽出のために用いられることが多い。

これに加えて、カーネルを二次元配列とすれば二次元畳み込み層、三次元配列とすれば三次元畳み込み層となる。二次元畳み込み層は主に画像に用いられることが多く、三次元畳み込みは動画に用いられることが多い。

畳み込み層における主要なパラメータは三つある。一つ目は、カーネルサイズである。この値が、畳み込み演算におけるフィルタのサイズを決定しており、考慮できる入力特徴量の範囲が定まる。二つ目は、ストライドである。この値はフィルタのシフト幅にあたり、2以上の値を設定すると、出力特徴量のサイズは入力特徴量のサイズよりも小さくなる。実際、特徴量をダウンサンプリングしたい場合には、2以上の値が用いられる。三つ目は、ダイレーションである。これはフィルタ内の要素間の距離を表し、その値を2以上とすれば、入力特徴量の飛び飛びの値を考慮した畳み込み演算が行われる。ダイレーションを大きくすることによって、カーネルサイズが同じでも考慮できる範囲が広がることが特徴であり、系列長の長大な音声波形を扱う場合などに用いられる。また、出力の系列長を入力に対して整数倍に保つためには、上記のパラメータに対して適切なパディング長を指定する必要がある。例えば、カーネルサイズを3、ストライドとダイレーションを1とした場合には、入力の両端に1ずつゼロパディングすることで入出力の系列長が保たれる。図~\ref{sec3:fig:conv_variations}に、ある入出力チャネル間における一次元畳み込み層の様子を示す。

\begin{figure}[tb]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[height=4cm]{./figure/sec3/conv1.drawio.png}
        \caption{(k, s, d) = (3, 1, 1)}
        \label{sec3:fig:conv1}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[height=4cm]{./figure/sec3/conv2.drawio.png}
        \caption{(k, s, d) = (5, 1, 1)}
        \label{sec3:fig:conv2}
    \end{subfigure}

    \vspace{0.5cm}

    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[height=4cm]{./figure/sec3/conv3.drawio.png}
        \caption{(k, s, d) = (3, 2, 1)}
        \label{sec3:fig:conv3}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[height=4cm]{./figure/sec3/conv4.drawio.png}
        \caption{(k, s, d) = (3, 1, 2)}
        \label{sec3:fig:conv4}
    \end{subfigure}
    \caption{ある入出力チャネル間における一次元畳み込み層の様子。kはカーネルサイズ、sはストライド、dはダイレーションを表し、図中の0はパディング部。}
    \label{sec3:fig:conv_variations}
\end{figure}

\subsubsection{転置畳み込み層}
転置畳み込み層は、畳み込み層の逆の演算を行うような層である。図~\ref{sec3:fig:tconv_variations}に、ある入出力チャネル間における一次元畳み込み層の様子を示す。転置畳み込み層では、$i$番目の入力とカーネルの積を計算し、その結果を$i$番目から$i + M - 1$番目までの出力とする。ここで$M$はカーネルサイズを表す。また、$i$番目の入力から計算された出力が、$i - 1$番目までの入力によってすでに得られている出力とオーバーラップする場合、これらは加算される。図~\ref{sec3:fig:tconv1}は、カーネルサイズを4、ストライドを1とした場合の様子である。
転置畳み込み層は特に、入力をアップサンプリングしたい場合に用いられることが多い。その例を図~\ref{sec3:fig:tconv2}に示す。ここでは、カーネルサイズを4、ストライドを2としており、入力系列長が4であるのに対して、出力系列長が10まで拡大されていることがわかる。ここで、出力系列長を入力系列長の整数倍にするためには、パディングの値を適切に設定する必要がある。転置畳み込み層におけるパディングは、出力の両端を何個落とすかを指定するパラメータである。上述の例においては、パディングを1とすることで、出力系列長を入力系列長の2倍である8に調整することができる。

\begin{figure}[tb]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[height=4cm]{./figure/sec3/tconv1.drawio.png}
        \caption{(k, s) = (4, 1)}
        \label{sec3:fig:tconv1}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[height=4cm]{./figure/sec3/tconv2.drawio.png}
        \caption{(k, s) = (4, 2)}
        \label{sec3:fig:tconv2}
    \end{subfigure}
    \caption{ある入出力チャネル間における一次元転置畳み込み層の様子。kはカーネルサイズ、sはストライド。}
    \label{sec3:fig:tconv_variations}
\end{figure}
\subsubsection{活性化関数}
% 各関数について、具体的な用途が挙げられると良い。また、論文を読んで引用できるとさらに良い。
活性化関数は、ニューラルネットワークの各層の出力に非線形性を導入するために用いられる関数である。これにより、ネットワークは単純な線形変換だけでは表現できない複雑な入出力の関係を学習可能になる。以下、代表的な活性化関数を述べる。また、活性化関数とその一階導関数のグラフを図~\ref{sec3:fig:activations_and_their_prime}に示す。

一つ目は、シグモイド関数である。シグモイド関数は
\begin{equation}
    \sigma(x) = \frac{1}{1 + \exp(-x)}
\end{equation}
で与えられ、その一階導関数は
\begin{equation}
    \frac{d}{dx}\sigma(x) = \frac{\exp(-x)}{(1 + \exp(-x))^{2}}
\end{equation}
となる。図~\ref{sec3:fig:activations_prime}より、シグモイド関数の一階導関数の最大値は$x=0$における0.25であり、入力が0から離れるほど微分係数が小さくなることが分かる。DNNの学習では、損失関数の重みに対するの勾配を逆伝播する必要があり、これは合成関数の微分における連鎖律に従って行われる。シグモイド関数は取り得る微分係数の値が小さく、特に層数が深いDNNにおいては勾配が小さくスケーリングされていくことで、浅い層の重みに対する勾配のノルムが小さくなりすぎる可能性がある。この問題は、勾配消失と呼ばれる。

二つ目は、tanh関数である。tanh関数は
\begin{equation}
    \tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}
\end{equation}
で与えられ、その一階導関数は
\begin{align}
    \frac{d}{dx}\tanh(x) & = \frac{4}{(\exp(x) + \exp(-x))^{2}} \\
                         & = \frac{1}{\cosh(x)^{2}}
\end{align}
となる。シグモイド関数と比較すると、シグモイド関数の値域が$[0, 1]$であったのに対し、tanh関数の値域は$[-1, 1]$となっている。シグモイド関数やtanh関数を実際に活性化関数として利用するニューラルネットワークとして、後述するLSTMやGRUが挙げられる。

三つ目は、ReLUである。ReLUは
\begin{equation}
    \text{ReLU}(x) = \max (0, x)
\end{equation}
で与えられ、その一階導関数は
\begin{equation}
    \frac{d}{dx}\text{ReLU}(x) =
    \begin{cases}
        1 & \text{if $x > 0$}  \\
        0 & \text{if $x <= 0$}
    \end{cases}
\end{equation}
となる。ReLUは入力が0以上であれば恒等写像となり、0未満であれば0に写す。一階導関数は0あるいは1のみを取り、特に入力が正の値であれば常に微分係数は1となることから、シグモイド関数よりも勾配消失問題が起こりづらくなっている。一方、ReLUは0以下の入力に対して微分係数が0となるため、勾配も0となる。従って、入力が負の値に偏った場合には勾配が0になり続け、重みの更新が進まなくなる可能性がある。この問題を、Dying ReLU問題と呼ぶ。

四つ目は、LeakyReLUである。LeakyReLUは
\begin{equation}
    \text{LeakyReLU}(x) =
    \begin{cases}
        x  & \text{if $x > 0$}  \\
        ax & \text{if $x <= 0$}
    \end{cases}
\end{equation}
で与えられ、その一階導関数は
\begin{equation}
    \frac{d}{dx}\text{LeakyReLU}(x) =
    \begin{cases}
        1 & \text{if $x > 0$}  \\
        a & \text{if $x <= 0$}
    \end{cases}
\end{equation}
となる。ここで、aはnegative slopeと呼ばれるパラメータである。ReLUと比較すると、0以下の入力に対しても0でない値を取り、また微分係数も0にならない点が異なっている。この工夫により、勾配が0になり続ける恐れがなくなるため、前述したDying ReLU問題に対処することが可能である。具体的な利用ケースとしては、敵対的生成ネットワーク（Generative adversarial network; GAN）におけるGeneratorやDiscriminatorが挙げられる。

五つ目は、PReLUである。これは四つ目に述べたLeakyReLUと非常に似た活性化関数であるが、LeakyReLUのnegative slopeを学習可能なパラメータに変更したという点で異なっている。

六つ目は、GELUである。GELUは
\begin{equation}
    \text{GELU}(x) = x \Phi(x)
\end{equation}
で与えられる。ここで、$\Phi(x)$は標準正規分布の累積分布関数である。GELUの一階導関数は、
\begin{equation}
    \frac{d}{dx}\text{GELU}(x) = \Phi(x) + \frac{x}{\sqrt{2\pi}}\exp(-\frac{x^{2}}{2})
\end{equation}
となる。GELUは入力に対して標準正規分布に基づく確率値を利用したスケーリングを行う活性化関数であり、後述するTransformerなど大規模なモデルにおいて採用される場合がある。

\begin{figure}[tb]
    \centering
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
        \includegraphics[height=7cm]{./figure/sec3/activations.png}
        \caption{活性化関数}
        \label{sec3:fig:activations}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
        \includegraphics[height=7cm]{./figure/sec3/activations_prime.png}
        \caption{活性化関数の一階導関数}
        \label{sec3:fig:activations_prime}
    \end{subfigure}
    \caption{活性化関数の例}
    \label{sec3:fig:activations_and_their_prime}
\end{figure}

\subsubsection{再帰型ニューラルネットワーク}
再帰型ニューラルネットワーク（Recurrent Neural Network; RNN）は、自身の過去の出力を保持し、それをループさせる再帰的な構造を持ったネットワークである。

近年よく用いられるRNNとして、長・短期記憶（Long Short-Time Memory; LSTM）~\cite{hochreiter1997long}がある。LSTMでは入力ゲート、忘却ゲート、出力ゲートの3つを持ち、これらゲートによってネットワーク内部の情報の取捨選択を行うことで、長い系列データからの学習を可能にした。LSTMのネットワーク内部で行われる計算を以下に示す。
\begin{gather}
    \bm{f}_{t} = \sigma(\bm{W}_{f}[\bm{x}_{t}, \bm{h}_{t-1}] + \bm{b}_{f}) \\
    \bm{i}_{t} = \sigma(\bm{W}_{i}[\bm{x}_{t}, \bm{h}_{t-1}] + \bm{b}_{i}) \\
    \tilde{\bm{c}_{t}} = \tanh (\bm{W}_{h}[\bm{x}_{t}, \bm{h}_{t-1}] + \bm{b}_{h}) \\
    \bm{c}_{t} = \bm{f}_{t} \odot \bm{c}_{t-1} + \bm{i}_{t} \odot \tilde{\bm{c}_{t}} \\
    \bm{o}_{t} = \sigma(\bm{W}_{o}[\bm{x}_{t}, \bm{h}_{t-1}] + \bm{b}_{o}) \\
    \bm{h}_{t} = \bm{o}_{t} \odot \tanh(\bm{c}_{t})
\end{gather}
ここで、$\bm{x}_{t}$は時刻$t$の入力、$\bm{f}_{t}$は忘却ゲートの出力、$\bm{i}_{t}$は入力ゲートの出力、$\bm{c}_{t}$が時刻$t$におけるセルの状態、$\bm{o}_{t}$が出力ゲートの出力、$\bm{h}_{t}$が時刻$t$における隠れ状態を表し、それぞれベクトルである。また、$\bm{W}$、$\bm{b}$はそれぞれ学習可能な重み行列、バイアスベクトルである。忘却ゲートの出力$\bm{f}_{t}$は、前時刻のセル状態$\bm{c}_{t-1}$との要素積に用いられ、これによってネットワーク内に保持されていた長期記憶に対して忘却するべき情報を与え、取捨選択を行う。入力ゲートの出力$\bm{i}_{t}$は時刻$t$のセル状態の候補となる$\tilde{\bm{c}_{t}}$との要素積に用いられ、これによって新たにネットワークが長期記憶するべき情報が選択され、セル状態$\bm{c}_{t}$として保持される。出力ゲートの出力$\bm{o}_{t}$は時刻$t$のセル状態$\bm{c}_{t}$に$\tanh$を適用した値との要素積に用いられ、これによって時刻$t$において出力するべき情報を選択する。最後に、隠れ状態$\bm{h}_{t}$は、時刻$t$の出力と$\tanh$を適用したセル状態$\bm{c}_{t}$から決定される。このように、$\bm{h}_{t}$及び$\bm{c}_{t}$を入力に対して適応的に変化させながら情報を取捨選択することで、長い系列長のデータに対しての学習を可能にした。

また、LSTMでは3つのゲートを必要とするが、ゲートを2つに減らすことでネットワークの軽量化を図ったネットワークとして、ゲート付き回帰型ユニット（Gated Recurrent Unit; GRU）\cite{cho2014learning}がある。GRUではリセットゲートと更新ゲートの2つのゲートを用いて隠れ状態$\bm{h}_{t}$を再帰的に更新する。また、LSTMと違ってセル状態$\bm{c}_{t}$を省いており、よりシンプルな構造となっている。GRUのネットワーク内部で行われる計算を以下に示す。
\begin{gather}
    \bm{z}_{t} = \sigma(\bm{W}_{z}[\bm{x}_{t}, \bm{h}_{t-1}] + \bm{b}_{z}) \\
    \bm{r}_{t} = \sigma(\bm{W}_{r}[\bm{x}_{t}, \bm{h}_{t-1}] + \bm{b}_{r}) \\
    \tilde{\bm{h}_{t}} = \tanh(\bm{W}_{h}[\bm{x}_{t}, \bm{r}_{t} \odot \bm{h}_{t-1}] + \bm{b}_{h}) \\
    \bm{h}_{t} = (1 - \bm{z}_{t}) \odot \bm{h}_{t-1} + \bm{z}_{t} \odot \tilde{\bm{h}_{t}}
\end{gather}
ここで、$\bm{x}_{t}$が時刻$t$における入力、$\bm{z}_{t}$が更新ゲートの出力、$\bm{r}_{t}$がリセットゲートの出力、$\bm{h}_{t}$が時刻$t$における隠れ状態を表し、それぞれベクトルである。また、$\bm{W}$は学習可能な重み行列、$\bm{b}$はバイアスベクトルである。更新ゲートの出力$\bm{z}_{t}$はネットワーク内で新たに記憶するべき情報の割合を決定する役割を果たす。一方、リセットゲートの出力$\bm{r}_{t}$は前時刻の隠れ状態$\bm{h}_{t-1}$との要素積に用いられ、これによって忘却するべき情報が選択される。リセットゲートの出力によって処理された内部表現から、時刻$t$の隠れ状態の候補となる$\tilde{\bm{h}_{t}}$が計算される。最後に、前時刻の隠れ状態$\bm{h}_{t-1}$と隠れ状態の候補$\tilde{\bm{h}_{t}}$に対して更新ゲートの出力$\bm{z}_{t}$を用いた重み付け和を計算することで、隠れ状態$\bm{h}_{t}$を決定する。

\subsubsection{Transformer}
Transformer~\cite{vaswani2017attention}は、自己注意機構（Self-Attention）を用いて、入力系列全体に渡る依存関係を捉えることができるニューラルネットワークである。特に、再帰的な計算を必要とするRNNと比較して、Transformerは並列計算のみ行うため、GPUによる計算の高速化が可能である。Transformerの内部構造について説明するため、入力系列$\bm{X} \in \mathbb{R}^{T \times d_{model}}$をとる。ここで、$T$は入力系列の系列長、$d_{model}$は入力系列の次元である。また、Transformer層の構造を図~\ref{sec3:fig:transformer_layer}に示す。

\begin{figure}[bt]
    \centering
    \includegraphics[height=140mm]{./figure/sec3/transformer.drawio.png}
    \caption{Transformer層の構造}
    \label{sec3:fig:transformer_layer}
\end{figure}

まず、Self-Attetionについて、これは以下の二つの要素からなる。
\begin{enumerate}
    \item クエリ、キー、バリューの計算
    \item Attentionスコアの計算
\end{enumerate}
まず、クエリ、キー、バリューの計算は、それぞれを$\bm{Q} \in \mathbb{R}^{T \times d_{k}}$、$\bm{K} \in \mathbb{R}^{T \times d_{k}}$、$\bm{V} \in \mathbb{R}^{T \times d_{v}}$とおくと、
\begin{align}
    \bm{Q} & = \bm{X}\bm{W}_{Q} \\
    \bm{K} & = \bm{X}\bm{W}_{K} \\
    \bm{V} & = \bm{X}\bm{W}_{V}
\end{align}
で与えられる。ここで、$d_{k}$はキーとクエリの次元、$d_{v}$はバリューの次元、$\bm{W_{Q}} \in \mathbb{R}^{d_{model} \times d_{k}}$はクエリに対する重み行列、$\bm{W_{K}} \in \mathbb{R}^{d_{model} \times d_{k}}$はキーに対する重み行列、$\bm{W_{V}} \in \mathbb{R}^{d_{model} \times d_{v}}$はバリューに対する重み行列である。次に、ここで得られたクエリ、キー、バリューを利用したAttentionスコアの計算は、
\begin{equation}
    \text{Attention}(\bm{Q}, \bm{K}, \bm{V}) = \text{softmax}\left(\frac{\bm{Q}\bm{K}^\top}{\sqrt{d_{k}}}\right) \bm{V}
\end{equation}
で与えられる。式中の$\bm{Q}\bm{K}^\top \in \mathbb{R}^{T \times T}$について、この行列の$(i, j)$成分$(\bm{Q}\bm{K}^\top)_{i, j}$は、クエリベクトル$Q_{i} \in \mathbb{R}^{d_{k}}$と、キーベクトル$K_{j} \in \mathbb{R}^{d_{k}}$の内積であり、$Q_{i}$と$K_{i}$の関連度を表していると解釈できる。また、softmax関数は列方向に適用されるため、アテンション重み$\text{softmax}\left(\frac{\bm{Q}\bm{K}^\top}{\sqrt{d_{k}}}\right)$の$(i, j)$成分$\left(\text{softmax}\left(\frac{\bm{Q}\bm{K}^\top}{\sqrt{d_{k}}}\right)\right)_{i, j}$は、
\begin{equation}
    \left(\text{softmax}\left(\frac{\bm{Q}\bm{K}^\top}{\sqrt{d_{k}}}\right)\right)_{i, j} = \frac{\exp\left(\frac{(\bm{Q}\bm{K}^\top)_{i, j}}{\sqrt{d_{k}}}\right)}{\sum_{j = 1}^{T} \exp\left(\frac{(\bm{Q}\bm{K}^\top)_{i, j}}{\sqrt{d_{k}}}\right)}
\end{equation}
となる。softmax関数を適用することでアテンション重みの行ベクトルは確率値として扱うことができるようになり、これは$i$番目のクエリからのキーの全要素に対する注意度が、確率分布として表されていると解釈できる。最後にアテンション重みをバリュー$\bm{V}$にかけることにより、各クエリからの注意度をもとにバリューベクトル$\bm{V}_{j}$の重み付け和が計算され、出力が得られる。

Transformerでは、Self-Attentionをただ行うだけではなく、これを複数のヘッドで並列に計算し、各ヘッドの出力を結合して最終出力を得るMulti-Head Attentionが行われる。ヘッド数を$h$とすると、各ヘッドにおけるクエリ、キー、バリューの計算は、
\begin{align}
    \bm{Q}^{i} & = \bm{X}\bm{W}_{Q}^{i} \\
    \bm{K}^{i} & = \bm{X}\bm{W}_{K}^{i} \\
    \bm{V}^{i} & = \bm{X}\bm{W}_{V}^{i}
\end{align}
で与えられる。ここで、$\bm{W_{Q}}^{i} \in \mathbb{R}^{d_{model} \times \frac{d_{k}}{h}}$は$i$番目のヘッドのクエリに対する重み行列、$\bm{W_{K}}^{i} \in \mathbb{R}^{d_{model} \times \frac{d_{k}}{h}}$は$i$番目のヘッドのキーに対する重み行列、$\bm{W_{V}}^{i} \in \mathbb{R}^{d_{model} \times \frac{d_{v}}{h}}$は$i$番目のヘッドのバリューに対する重み行列である。よって、$i$番目のヘッドのクエリ、キー、バリューの形状はそれぞれ、$\bm{Q}^{i} \in \mathbb{R}^{T \times \frac{d_{k}}{h}}$、$\bm{K}^{i} \in \mathbb{R}^{T \times \frac{d_{k}}{h}}$、$\bm{V}^{i} \in \mathbb{R}^{T \times \frac{d_{v}}{h}}$となる。$i$番目のヘッドにおけるAttentionスコアの計算は
\begin{equation}
    \text{Attention}(\bm{Q}^{i}, \bm{K}^{i}, \bm{V}^{i}) = \text{softmax}\left(\frac{\bm{Q}^{i}(\bm{K}^{i})^\top}{\sqrt{d_{k}} / h}\right) \bm{V}^{i}
\end{equation}
で行われる。その後、すべてのヘッドからのAttentionスコアを結合し、さらに線形変換を適用することでMulti-Head Attetionの出力が得られる。これは、
\begin{equation}
    \text{MultiHead}(\bm{Q}, \bm{K}, \bm{V}) = \text{Concat}(\text{head}^{1}, \ldots, \text{head}^{h})\bm{W}_{o}
\end{equation}
と表される。ここで、$\text{head}^{i} \in \mathbb{R}^{T \times \frac{d_{v}}{h}}$が各ヘッドから出力されたAttentionスコア、$\text{Concat}(\text{head}^{1}, \\ \ldots, \text{head}^{h}) \in \mathbb{R}^{T \times d_{v}}$が全ヘッドのAttentionスコアを結合した特徴量、$\bm{W}_{o} \in \mathbb{R}^{d_{v} \times d_{model}}$が最終的な線形変換に用いられる重み行列である。ヘッドを分割することで複数パターンのアテンションが可能になり、これが入出力間の複雑な関係性を考慮するのに役立っていると考えられる。Multi-Head Attention後には、残差結合と後述するレイヤー正規化を適用する。
これは、出力系列を$\bm{Y} \in \mathbb{R}^{T \times d_{model}}$とすると、
\begin{equation}
    \bm{Y} = \text{LayerNorm}\left(\text{MultiHead}(\bm{Q}, \bm{K}, \bm{V}) + \bm{X}\right)
\end{equation}
で与えられる。

Multi-Head Attentionによって系列全体の依存関係を考慮した後は、各フレームごとに独立して全結合層を適用する。これは、
\begin{equation}
    \text{FFN}(\bm{Y}) = \text{ReLU}(\bm{Y}\bm{W}_{1} + \bm{b}_{1})\bm{W}_{2} + \bm{b}_{2}
\end{equation}
で与えられる。
ここで、$\bm{W}_{1} \in \mathbb{R}^{d_{model} \times d_{ff}}$、$\bm{W}_{2} \in \mathbb{R}^{d_{1} \times d_{model}}$は学習可能な重み行列、$\bm{b}_{1} \in \mathbb{R}^{d_{ff}}$、$\bm{b}_{2} \in \mathbb{R}^{d_{model}}$はバイアスベクトルである。$d_{ff}$は$d_{model}$の4倍とされることが多い。全結合層適用後は、Multi-Head Attention後と同様に、残差結合とレイヤー正規化を適用する。

上述したMulti-Head Attentionとフレームごとに適用される全結合層を合わせて、Transformer層と呼ぶ。実際には、Transformer層を多層積み重ねて用いることが多い。

最後に、TransformerではRNNと違い、並列計算によって系列全体を一度に処理することが可能であるが、それと引き換えに入力の順序情報を考慮することができなくなる。この問題に対し、入力系列に対して位置情報を与えるために行われるのが、Positional Encodingである。Positional Encodingは$\sin$関数と$\cos$関数に基づいて計算される値であり、
\begin{align}
    \text{PE}_{pos, 2i}     & = \sin \left(\frac{pos}{10000^{2i / d_{model}}}\right) \\
    \text{PE}_{pos, 2i + 1} & = \cos \left(\frac{pos}{10000^{2i / d_{model}}}\right)
\end{align}
で与えられる。ここで、$pos$は入力系列の位置を表し、$i$は入力系列の次元を表す。

\subsection{学習方法}
\subsubsection{損失関数}

\subsubsection{確率的勾配降下法}
\subsubsection{誤差逆伝播法}
\subsubsection{最適化手法}
% adam, adamw
\subsubsection{学習率のスケジューリング}
% exp, cosine annealing with warmup
\subsubsection{特徴量の正規化}
DNNの学習過程では学習の進行に伴って重みが変化するため、その度に各層への入力の分布が変わってしまう。これは内部共変量シフト（Internal Covariate Shift）と呼ばれ、ネットワークの学習を不安定にする原因となる。これに対し、バッチ正規化（Batch Normalization）が有効である。バッチ正規化は、ミニバッチ学習を行う際にそのミニバッチ内における平均と標準偏差を計算し、特徴量を標準化する。
\subsubsection{正則化}
% dropout, weight decay, early stopping
\subsubsection{学習の安定化}
% gradient clipping, gradient accumulation
\subsubsection{自己教師あり学習}
% hubert, avhubert

\clearpage

\section{動画音声合成モデルの検討}
\subsection{音声合成法}
提案手法の構築手順は3段階に分かれる。ネットワークの構造を図~\ref{sec4:fig:network}に示す。
一段階目では、動画を入力として、メルスペクトログラムとHuBERT離散特徴量、HuBERT中間特徴量を推定するネットワークAを学習する（図~\ref{sec4:fig:network}のA）。ここで、HuBERT離散特徴量はHuBERT Transformer層から得られる特徴量を k-means法によってクラスタリングすることで離散化した値、HuBERT中間特徴量はHuBERTにおける畳み込み層出力で、HuBERTの事前学習時にマスク対象となる値のことを指す。図~\ref{sec4:fig:hubert}にこれらの取得位置を示す。第一段階では、AVHuBERTを動画からの特徴抽出に利用した。これにより、動画の空間情報は完全に圧縮され、768次元の一次元系列となる。その後、事前学習済みの話者識別モデル~\cite{wan2018generalized}によって音声波形から得られる256次元の話者Embeddingを、各フレームでチャンネル方向に結合する。これによって特徴量は1024次元に拡張され、全結合層によって再度768次元に圧縮する。その後、畳み込み層と全結合層からなるConvDecoder（図~\ref{sec4:fig:network}のConvDecoder）を通すことによって、話者Embeddingを結合した特徴量に対する変換を施した。これにより、特にメルスペクトログラムにおいて話者性が正しく反映されることを狙った。複数話者モデルであっても、入力である動画の見た目から話者性を判別できる可能性があったが、将来的な未知話者対応への拡張性も考慮して、本研究では補助特徴量として入力することとした。ConvDecocerは残差結合を利用したブロック単位で構成され、各ブロックに2層の畳み込み層を設けた。各畳み込み層のチャンネル数は768、カーネルサイズは3であり、3ブロック積み重ねた。最後に全結合層を通し、所望の次元に変換することで予測対象を得た。ネットワークAの役割は、続くネットワークBの入力であるHuBERT中間特徴量を提供することである。これに対し、メルスペクトログラムとHuBERT離散特徴量の推定を同時に行った理由は、先行研究においてマルチタスク学習の有効性が確認されていることを考慮し、ネットワークAでもマルチタスク学習を採用しておこうと考えたからである。

二段階目では、一段階目に学習されたネットワークAの重みを固定した状態でHuBERT中間特徴量を推定し、それを入力としてメルスペクトログラムとHuBERT離散特徴量を推定する、HuBERT Transformer層を中心としたネットワークBの学習を行う（図~\ref{sec4:fig:network}のB）。HuBERT Transformer層出力はAVHuBERT出力と同じ768次元の特徴量となるため、これに対してネットワークAと同様に話者Embeddingを結合し、ConvDecoderを通すことで予測値を得た。ネットワークBの役割は、音声波形への変換に必要となるメルスペクトログラムとHuBERT離散特徴量の予測である。HuBERT Transformer層の転移学習を検討した狙いについて、HuBERTは自己教師あり学習時、畳み込み層出力にマスクを適用し、Transformer層を通すことによってマスクされた部分を推定しようとする。これにより、音声の文脈を考慮するのに長けた学習済み重みが、特にTransformer層で獲得されると仮定した。これに基づき、本研究ではHuBERT Transformer層を動画音声合成にFine Tuningすることにより、動画を入力としたAVHuBERTを中心とするネットワークAにおける推定残差を、音声自体の文脈を考慮することによって軽減し、動画から直接推定しきれなかった部分を補うことでの精度改善を狙った。

\begin{figure}[bt]
    \centering
    \includegraphics[height=90mm]{./figure/sec4/model/network.png}
    \caption{提案するネットワークの構造}
    \label{sec4:fig:network}
\end{figure}

\begin{figure}[bt]
    \centering
    \includegraphics[height=90mm]{./figure/sec4/model/hubert.png}
    \caption{HuBERT中間特徴量とHuBERT離散特徴量の取得位置}
    \label{sec4:fig:hubert}
\end{figure}

三段階目では、二段階目までに学習されたネットワークAとネットワークBの重みを固定した状態で、AVHuBERTから得られる特徴量と、HuBERT Transformer層から得られる特徴量の二つを結合し、それらを入力として再びメルスペクトログラムとHuBERT離散特徴量の予測を行うネットワークCを学習した（図~\ref{sec4:fig:network}のC）。ネットワークCでは、はじめに前述した二つの特徴量をチャンネル方向に結合することで、1536次元の入力特徴量を得る。これに対して全結合層を施すことで再度768次元に圧縮し、4層のTransformer層を通すことで系列全体を考慮した特徴抽出を改めて行った。その後、ネットワークA,Bと同様に話者Embeddingを結合し、ConvDecoderを通すことによって予測値を得た。ここで、ネットワークCのTransformer層におけるパラメータについては、AVHuBERTやHuBERTと同様にチャンネル数を768、ヘッド数は12とした。ネットワークCの役割は、ネットワークBと同様に音声波形への変換に必要な特徴量の予測である。ここでの狙いについて、まず、AVHuBERTから得られる特徴量とHuBERT Transformer層から得られる特徴量は、どちらもConvDecoderへの入力となる点で同じである。一方、AVHuBERTは動画を入力、HuBERT Transformer層はHuBERT中間特徴量を入力とするため、これら特徴量の元となる入力は異なっている。ここでは、概ね同じ予測対象のために利用される二つの特徴量（ネットワークAではHuBERT中間特徴量の予測も行っているため、全く同じではない）が、入力の違いに依存して内部のSelf Attentionにより注意される部分が変化し、何らかの異なった情報を持っている可能性があると仮定した。この仮定に基づき、両方の特徴量を考慮して単一特徴量への依存を解消することで、汎化性能向上による予測精度の改善を狙った。

以上が提案手法の全体像であるが、今回ベースラインとする先行研究\cite{choi2023intelligible}に基づいたマルチタスク学習手法は、本研究におけるネットワークAで、HuBERT中間特徴量を推定しないものに当たる。AVHuBERT以降のConvDecoderについて、これは先行研究と異なる構成である。先行研究では、メルスペクトログラムに対して畳み込みをベースとしたデコーダ、HuBERT離散特徴量に対して全結合層を利用したデコーダを別々に設けており、話者Embeddingはメルスペクトログラムの推定のみに用いていた。この構成についても検討したが、ConvDecoderの方がベースライン、提案手法問わず全体的に性能が改善したため、本研究ではこちらを採用した。

以上のモデルにより、動画からメルスペクトログラムとHuBERT離散特徴量が推定可能となる。その後、先行研究~\cite{choi2023intelligible}に基づくMulti-input Vocoderを用い、メルスペクトログラムとHuBERT離散特徴量を入力として音声波形に変換することで、最終的な合成音声を得た。Multi-input VocoderはHiFi-GAN~\cite{kong2020hifi}をベースとしたモデルであり、音声波形を生成するGeneratorと、Multi-Period Discriminator（MPD）およびMulti-Scale Discriminator（MSD）という二つのDiscriminatorによって構成される。

Generatorの構造を図~\ref{sec4:fig:multi-input_vocoder}に示す。左の特徴量予測モデルは、動画からメルスペクトログラムとHuBERT離散特徴量を予測する、本研究において主な検討対象となる部分を表す。Generatorの内部構造について、まず、前処理層はメルスペクトログラムとHuBERT離散特徴量を入力として受け取り、その後のレイヤーに入力するための形状に変換する役割を持つ。メルスペクトログラムに対しては、時間方向に隣接したフレームを次元方向に縦積みすることによって、100 Hz・80次元の特徴量から50 Hz・160次元の特徴量に変換した後、全結合層によって128次元の特徴量に変換する。一方、HuBERT離散特徴量は50 Hzのインデックス系列であり、インデックスから128次元のベクトルへと変換する。その後、これらをチャンネル方向に結合することで256次元の特徴量を構成し、これをその後のレイヤーへの入力とする。この特徴量は、転置畳み込み層と複数種類の畳み込み層から構成されるブロックを通過していく。各ブロックについて、まず、転置畳み込み層は特徴量を時間方向にアップサンプリングする役割を果たす。実際、本研究では50 Hzの入力特徴量から16 kHzの音声波形まで、時間方向に320倍のアップサンプリングを行う必要がある。Generatorでは、これを複数のブロックを通して段階的に行っている。また、各ブロックの転置畳み込み層に積まれた複数種類の畳み込み層は、そのカーネルサイズとダイレーションが全て異なっている。複数の時間的な受容野を持つ畳み込み層からの出力をすべて加算することで、アップサンプリング後の特徴量からの特徴抽出を行う仕組みとなっている。表~\ref{sec4:tab:multi-input_vocoder_parameter}に、Generatorの各ブロックにおけるパラメータとブロックごとの出力特徴量の形状を示す。カラム名の右にある括弧がきが数値の意味を表しており、Kはカーネルサイズ、Sはストライド、Dはダイレーション、Cは次元（チャンネル数）、Tは系列長である。畳み込み層についてはカーネルサイズとダイレーションを集合として表記しているが、実際はこれらの直積の元、すなわち(3, 1)や(3, 3)、(3, 5)をパラメータとする畳み込み層が存在することを表す。
\begin{figure}[bt]
    \centering
    \includegraphics[height=120mm]{./figure/sec4/model/multi-input_vocoder.png}
    \caption{Multi-input Vocoderの構造}
    \label{sec4:fig:multi-input_vocoder}
\end{figure}
\begin{table*}[bt]
    \centering
    \caption{Generatorの各ブロックにおけるパラメータ}
    \label{sec4:tab:multi-input_vocoder_parameter}
    \begin{center}
        \renewcommand{\arraystretch}{0.9} % 行の高さ調整
        \setlength{\tabcolsep}{8pt}      % 列の幅調整
        \scalebox{1.0}{
            \begin{tabular}{|c|c|c|c|}
                \hline
                  & \multicolumn{1}{c|}{転置畳み込み層 (K, S)} & \multicolumn{1}{c|}{畳み込み層 (K, D)}     & \multicolumn{1}{c|}{出力特徴量の形状 (C, T)} \\
                \hline
                1 & (11, 5)                             & ($\{3, 5, 7, 9, 11\}$, $\{1, 3, 5\}$) & (1024, 250)                          \\
                2 & (8, 4)                              & ($\{3, 5, 7, 9, 11\}$, $\{1, 3, 5\}$) & (512, 1000)                          \\
                3 & (4, 2)                              & ($\{3, 5, 7, 9, 11\}$, $\{1, 3, 5\}$) & (256, 2000)                          \\
                4 & (4, 2)                              & ($\{3, 5, 7, 9, 11\}$, $\{1, 3, 5\}$) & (128, 4000)                          \\
                5 & (4, 2)                              & ($\{3, 5, 7, 9, 11\}$, $\{1, 3, 5\}$) & (64, 8000)                           \\
                6 & (4, 2)                              & ($\{3, 5, 7, 9, 11\}$, $\{1, 3, 5\}$) & (32, 16000)                          \\
                \hline
            \end{tabular}
        }
    \end{center}
\end{table*}

Generatorの学習時に用いられるのが、MPDおよびMSDという二つのDiscriminatorである。これらの概要を図~\ref{sec4:fig:multi-input_vocoder_mpd_msd}に示す。MPDでは、一次元の音声波形を指定した周期をもとにReshapeすることで二次元に変換し、これに対して二次元畳み込みを適用することで、入力された音声波形の原音声らしさを判定するDiscriminatorである。異なる周期を設定したDiscriminatorを複数利用することで、時間的な特徴を考慮できるように構成されている。一方、MSDでは、音声波形に対してAverage Poolingを適用することでダウンサンプリングし、これに一次元畳み込みを適用することで入力された音声波形の原音声らしさを判定するDiscriminatorである。MSDはAverage Poolingによって系列長を抑えつつ、時間方向の連続的な特徴を考慮する狙いがある。これら二つのDiscriminatorについては、HiFi-GANと同様のパラメータで用いた。
\begin{figure}[bt]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{./figure/sec4/model/mpd.png}
        \caption{Multi-Period Discriminator（MPD）}
        \label{sec4:fig:multi-input_vocoder_mpd}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{./figure/sec4/model/msd.png}
        \caption{Multi-Scale Discriminator（MSD）}
        \label{sec4:fig:multi-input_vocoder_msd}
    \end{subfigure}
    \caption{Multi-Period Discriminator（MPD）とMulti-Scale Discriminator（MSD）の概要}
    \label{sec4:fig:multi-input_vocoder_mpd_msd}
\end{figure}

\subsection{実験方法}
\subsubsection{利用したデータセット}
動画音声データセットには、男女二人ずつから収録した合計4人分のデータセット~\cite{taguchi,esaki}を用いた。これはATR音素バランス文~\cite{atr}から構成され、全話者共通でAからHセットを学習データ、Iセットを検証データ、Jセットをテストデータとして利用した。各分割ごとの文章数を表~\ref{sec4:tab:dataset_info}に示す。

Multi-input Vocoderの学習に利用する音声データセットには、Hi-Fi-Captain（日本語話者二名分）~\cite{okamoto2023hi}とJVS（parallel100とnonpara30）~\cite{takamichi2019jvs}を利用した。Hi-Fi-Captainはtrain-parallelおよびtrain-non-parallelを学習データ、valを検証データ、evalをテストデータとして分割した。各分割ごとの文章数を表~\ref{sec4:tab:dataset_info}に示す。JVSには話者に対して1から100まで番号が割り振られており、本実験では1から80番の話者を学習データ、81番から90番の話者を検証データ、91番から100番までの話者をテストデータとした。各分割ごとの文章数を表~\ref{sec4:tab:dataset_info}に示す。また、JVSには読み上げ音声のparallel100およびnonpara30と、裏声のfalset10、囁き声のwhisper10が含まれる。本研究では、parallel100とnonpara30のみを利用した。

\begin{table*}[bt]
    \centering
    \caption{利用したデータセットの文章数}
    \label{sec4:tab:dataset_info}
    \begin{center}
        \renewcommand{\arraystretch}{0.9} % 行の高さ調整
        \setlength{\tabcolsep}{8pt}      % 列の幅調整
        \scalebox{1.0}{
            \begin{tabular}{|l|r|r|r|}
                \hline
                              & \multicolumn{1}{c|}{学習} & \multicolumn{1}{c|}{検証} & \multicolumn{1}{c|}{テスト} \\
                \hline
                動画音声データセット    & 1598                    & 200                     & 212                      \\
                Hi-Fi-Captain & 37714                   & 200                     & 200                      \\
                JVS           & 10398                   & 1299                    & 1300                     \\
                \hline
            \end{tabular}
        }
    \end{center}
\end{table*}

\subsubsection{データの前処理}
動画データは60 FPSで収録されたものをffmpegにより25 FPSに変換して用いた。その後、手法~\cite{bulat2017far}により動画に対してランドマーク検出を適用した。このランドマークを利用することで口元のみを切り取り、画像サイズを（96, 96）にリサイズした。モデル入力時は動画をグレースケールに変換し、各フレームに対する正規化および標準化を適用した。正規化では、グレースケールが0から255までの値を取るため、最大値の255で割り、標準化では、AVHuBERTのプログラムで利用されていた平均値0.421と、標準偏差0.165をそのまま利用して、平均値を引いた後に標準偏差で割った。全体として、今回は事前学習済みのAVHuBERTの転移学習を行うため、そこでの前処理に合わせている。学習時は、ランダムクロップ、左右反転、Time Masking（一時停止）をデータ拡張として適用した。ランダムクロップは、（96, 96）で与えられる画像から（88, 88）をランダムに切り取る処理である。検証およびテスト時は、必ず画像中央を切り取るよう実装した。左右反転は、50\%の確率で左右が反転されるよう実装した。Time Maskingは、連続する画像の時間平均値を利用することによって、一時停止させるような効果を与えるデータ拡張手法である。動画1秒あたり0から0.5秒の間でランダムに停止区間を定め、その区間における動画の時間方向平均値を計算し、区間内のすべてのフレームをこの平均値で置換した。

音声データは16 kHzにダウンサンプリングして用いた。それから、窓長25 msのハニング窓を用いて、シフト幅10 msでSTFTを適用することでフレームレート100 Hzのスペクトログラムに変換した。さらに、振幅スペクトログラムに対して80次のメルフィルタバンクを適用し、メルスペクトログラムを得た上で対数スケールに変換した。
また、Multi-input Vocoderの学習に利用したHi-Fi-CaptainとJVSについては、無音区間のトリミング（-40 dBFS未満かつ500 ms継続する区間を100 msまでカット）を適用した。なぜなら、Multi-input Vocoderの学習時は、全体から1秒分をランダムサンプリングするように実装しており、元データに存在する無音区間を除去することによって、学習の安定化および得られる音声の品質改善に繋がったからである。
また、話者Embeddingの取得には事前学習済みの話者識別モデル~\cite{wan2018generalized}を利用した。動画音声データセット、Hi-Fi-Captain、JVSともに、各話者学習データの中から100文章をランダムサンプリングし、各発話に対して得られたベクトルの平均値を用いた。この値を学習・検証・テストで一貫して用いるため、学習外の検証データやテストデータには非依存な値となっている。モデルに入力する際には、ベクトルの大きさで割って正規化した。

HuBERTは、HuggingFaceに公開されているReazonSpeechというデータセットによって学習されたモデル~\cite{rinna-japanese-hubert-base,sawada2024release}を利用した。ReazonSpeechは約19000時間の日本語音声からなるデータセットであり、日本語音声のコンテキストを大量のデータから学習したモデルである。今回用いるデータセットが日本語であることから、本研究の検討対象としては日本語音声に関する事前知識を豊富に有するモデルが適していると考え、このモデルを選択した。本研究で用いるHuBERT中間特徴量およびHuBERT離散特徴量について、HuBERT中間特徴量は、音声波形に対して畳み込み層を適用した出力で、HuBERTの事前学習時にはマスクの対象となる特徴量を利用した。一方、HuBERT離散特徴量は、HuBERT Transformer層の8層目出力に、k-means法によるクラスタリングを適用することで得た。あえて8層目出力を選択した理由は、HuBERTのレイヤーごとの特徴量について、音素のOne-hotベクトルおよび単語のOne-hotベクトルとの相関を、Canonical Correlation Analysis（CCA）によって調べた先行研究\cite{pasad2023comparative}より、8層目出力がそのどちらとも相関が高く言語的な情報に近いと判断したからである。クラスタ数については決め打ちとなるが、今回は離散化した結果が言語的な情報を持つことを狙い、比較的少ない100とした。k-means法の学習には動画音声データセットにおける学習用データ全てを利用し、これを用いて動画音声データ、Hi-Fi-Captain、JVSに対するクラスタリングを実施した。

\subsubsection{学習方法}
一段階目について、損失関数はメルスペクトログラムのMAE Loss $L_{mel}$ とHuBERT離散特徴量のCross Entropy Loss $L_{ssl^{d}}$ 、HuBERT中間特徴量のMAE Loss $L_{ssl^{i}}$ の重み付け和とした。それぞれの重み係数を$\lambda_{mel}, \lambda_{ssl^{d}}, \lambda_{ssl^{i}}$とすると、
\begin{equation}
    \label{sec4:eq:loss}
    L = \lambda_{mel} * L_{mel} + \lambda_{ssl^{d}} * L_{ssl^{d}} + \lambda_{ssl^{i}} * L_{ssl^{i}}
\end{equation}
となる。最適化手法にはAdamW~\cite{loshchilov2017decoupled}を利用し、$\beta_{1} = 0.9$、$\beta_{2} = 0.98$、$\lambda = 0.01$とした。学習率に対するスケジューラには、Cosine Annealing with Warmupを利用した。開始時の学習率は\num{1.0e-6}として、最大エポック数の10\%に至るまでは学習率を\num{1.0e-3}まで線形に増加させ、その後のエポックではcosine関数に基づいて\num{10.e-6}まで減少させた。バッチサイズはメモリの都合上4としたが、学習の安定化のため、Gradient Accumulationによって各イテレーションにおける勾配を累積させ、8イテレーションに一回重みを更新するようにした。モデルに入力する動画の秒数は10秒を上限とし、それを超える場合はランダムにトリミング、それに満たない場合はゼロパディングした。ゼロパディングした部分は損失の計算からは除外した。勾配のノルムは3.0を上限としてクリッピングすることで、過度に大きくなることを防止した。最大エポック数は50とし、10エポック連続して検証データに対する損失が小さくならない場合には、学習を中断するようにした（Early Stopping）。また、学習終了時には検証データに対する損失が最も小さかったエポックにおけるチェックポイントを保存し、これをテストデータに対する評価に用いた。

第二段階について、損失関数はメルスペクトログラムのMAE LossとHuBERT離散特徴量のCross Entropy Lossの重み付け和とした。これは式~\eqref{sec4:eq:loss}において、$\lambda_{mel} = 1.0, \lambda_{ssl^{i}} = 0.0$と固定した場合に相当する。最適化手法にはAdamWを利用し、$\beta_{1} = 0.9$、$\beta_{2} = 0.98$、$\lambda = 0.01$とした。学習率に対するスケジューラには、Cosine Annealing with Warmupを利用した。開始時の学習率は\num{1.0e-6}として、最大エポック数の10\%に至るまでは学習率を\num{5.0e-4}まで線形に増加させ、その後のエポックではcosine関数に基づいて\num{10.e-6}まで減少させた。学習率の最大値は第一段階の$1/2$となっているが、これは値を半減させることによって学習を安定させることができたためである。その他のパラメータは、第一段階における値と同じである。

第三段階について、学習率のみ第一段階と同様に\num{1.0e-3}としたが、それ以外は第二段階と同様である。

Multi-input Vocoderの学習では、Hi-Fi-CaptainとJVSを用いた。はじめにHi-Fi-Captainのみを用いて学習させ、その後学習済みモデルをJVSによって再学習した。損失関数はHiFi-GANと同様である。Hi-Fi-Captainは男女一人ずつの文章数が豊富なデータセットであるため、高品質なモデルを構築可能であった。しかし、学習できる話者数が少ない分、学習外話者に対する合成音声の品質が低かった。そのため、一人当たりの文章数は100文章程度と少ないながらも、100人分の話者からなるJVSを利用して再学習することによって、学習外話者に対する合成音声の品質を向上させた。最適化手法にはAdamWを利用し、$\beta_{1} = 0.8$、$\beta_{2} = 0.99$、$\lambda = \num{1.0e-5}$とした。学習率は\num{2.0e-4}から開始し、1エポック経過するごとに0.99かけて徐々に減衰させた。バッチサイズは16とし、ここではGradient Accumulationは利用しなかった。モデルへの入力は1秒を上限とし、それを超える場合はランダムにトリミング、それに満たない場合はゼロパディングした。勾配のノルムは3.0を上限としてクリッピングすることで、過度に大きくなることを防止した。最大エポック数は30とし、ここではEarly Stoppingは適用しなかった。また、学習終了時には検証データに対する損失（メルスペクトログラムに対するL1 Loss）が最も小さかったエポックにおけるチェックポイントを保存し、これをテストデータに対する評価に用いた。また、Multi-input Vocoderの提案された先行研究~\cite{choi2023intelligible}においては、学習時にあえてメルスペクトログラムにノイズをかけることによって、合成音声に対する汎化性能を向上させる学習方法が提案されている。本研究では、動画から推定されるメルスペクトログラムとHuBERT離散特徴量の推定精度向上に焦点を当てたため、Multi-input Vocoderの学習は原音声から計算される特徴量そのもので行い、ボコーダ自体の汎化性能向上による精度改善は追求しなかった。

実装に用いた深層学習ライブラリはPyTorchおよびPyTorch Lightningである。GPUにはNVIDIA RTX A4000を利用し、計算の高速化のためAutomatic Mixed Precisionを適用した。

\subsubsection{比較手法}
\label{sec4:subsubsection:methods}
比較手法は、以下の五つである。
\begin{enumerate}
    \item メルスペクトログラムとHuBERT離散特徴量のマルチタスク学習手法（ベースライン）
    \item 提案手法のネットワークBで、事前学習済み重みを読み込まずにHuBERT Transformer層を用いる手法
    \item 提案手法のネットワークCで、事前学習済み重みを読み込まずにHuBERT Transformer層を用いる手法
    \item 提案手法のネットワークBで、事前学習済み重みを読み込んでHuBERT Transformer層を用いる手法
    \item 提案手法のネットワークCで、事前学習済み重みを読み込んでHuBERT Transformer層を用いる手法
\end{enumerate}
手法1が先行研究において有効性が確認された手法であり、今回の実験においてベースラインとなる。これに対する改善案として、手法2から手法5が提案手法である。手法2と手法3及び手法4と手法5の違いは事前学習済み重みを読み込むか否かであり、これらを比較することで初期値を事前学習済み重みとして転移学習することの有効性を調べた。

\subsubsection{客観評価}
合成音声の客観評価では、二種類の指標を用いた。
一つ目は、音声認識の結果から算出したWord Error Rate（WER）である。音声認識にはWhisper~\cite{radford2023robust}を利用し、出力される漢字仮名交じり文に対してMeCabを用いて分かち書きを行った上で、jiwerというライブラリを用いて算出した。WhisperはLargeモデルを利用し、MeCabの辞書にはunidicを利用した。WERの値は0\%から100\%であり、この値が低いほど音声認識の誤りが少ないため、より聞き取りやすい音声であると判断した。
二つ目は、話者Embeddingから計算したコサイン類似度である。話者Embeddingの計算は、モデルへの入力値を計算したものと同様の話者識別モデルを利用し、対象音声と原音声のペアでコサイン類似度を計算した。今回構築するモデルは4人の話者に対応するモデルとなるため、原音声に似た声質の合成音声が得られているかをこの指標で評価した。値は0から1であり、高いほど原音声と類似した合成音声だと判断できる。

% \subsubsection{主観評価}
% 合成音声の主観評価では、音声の明瞭性と類似性の二点を評価した。今回はクラウドワークスというクラウドソーシングサービスおよび、自作の実験用Webサイトを利用してオンラインで実験を実施した。被験者の条件は、日本語母語話者であること、聴覚に異常がないこと、イヤホンあるいはヘッドホンを用いて静かな環境で実験を実施可能であることとした。被験者の方に行っていただいた項目は、以下の五つである。
% \begin{enumerate}
%     \item アンケート
%     \item 練習試行（明瞭性）
%     \item 本番試行（明瞭性）
%     \item 練習試行（類似性）
%     \item 本番試行（類似性）
% \end{enumerate}

% 一つ目のアンケートでは、被験者についての基本的な統計を取ることを目的として、性別・年齢・実験に利用した音響機器について回答してもらった。性別は、男性、女性、無回答の三つからの選択式とした。年齢は被験者の方に直接数値を入力してもらう形式とした。実験に使用した音響機器は、イヤホン、ヘッドホンの二つからの選択式とした。

% 二つ目の練習試行（明瞭性）および三つ目の本番試行（明瞭性）では、音声の明瞭性の評価を実施した。初めに練習試行を行っていただくことで実験内容を把握してもらい、その後本番施行を行っていただく流れとした。ここで、練習施行は何度でも実施可能とし、本番試行は一回のみ実施可能とした。
% 評価項目について、明瞭性は「話者の意図した発話内容をその通り聞き取ることができるか」を評価するものとした。実際の評価プロセスは以下の三段階で構成した。
% \begin{enumerate}
%     \item 音声サンプルのみを聞いてもらい、その音声の発話内容を聞き取ってもらう。
%     \item 発話内容を聞き取ることができた、あるいはこれ以上聞き取ることはできないと判断したら、本来の発話内容を確認してもらう。
%     \item 聴取者が想定していた発話内容と本来の発話内容を照らし合わせ、音声の聞き取りやすさを5段階評価してもらう。
% \end{enumerate}
% 5段階評価の回答項目は以下のようにした。
% \begin{enumerate}
%     \item 全く聞き取れなかった
%     \item ほとんど聞き取れなかった
%     \item ある程度聞き取れた
%     \item ほとんど聞き取れた
%     \item 完全に聞き取れた
% \end{enumerate}

% 実験に利用した音声サンプルについて、練習試行では検証データ、本番試行ではテストデータを用いた。評価対象とした音声の種類は、\ref{sec4:subsubsection:methods}節における五つの手法と、原音声、分析合成を加えた7種類である。被験者ごとの評価サンプルの割り当て方法について、初めに評価に用いる文章を比較手法の総数である7つのグループにランダムに分割した。具体的には、練習試行では検証データであるATR音素バランス文のIセットから7種類、本番試行ではテストデータであるATR音素バランス文のJセットのすべて、すなわち53種類の文章を7つの文章グループにランダムに分割した。次に、各文章グループに対して7種類の手法から一つを割り当てることで、文章一つ一つに対する手法の割り当てを行った。残る話者の決定については、ランダムに割り当てるようにした。文章グループに対して割り当てる手法は、被験者ごとにずらすよう実装した。この過程を表~\ref{sec4:tab:sbj_selection_relation}に示す。これは、7つの文章グループに対し、割り当てる手法が一つずつずれていく過程を表している。これにより、文章と手法の組み合わせを効率よく網羅できる\cite{king2008blizzard}。またこの選択方法により、各話者はすべての文章を一回ずつ評価する機会が与えられ、その中で各手法がなるべく均等な回数含まれることとなる。同じ発話内容の音声を二回以上提示しないことで、聴取者の集中力を保つことを狙った。また、各手法をなるべく均等な回数提示するようにした理由は、手法の比較が主観評価の最終的な目的であったため、各被験者がすべての手法を評価する機会を与えたかったからである。
% 加えて、サンプル選択の過程では、以前に選択された回数をカウントしておくことで、サンプルの選択にランダム性を持たせつつ、すべてのサンプルが等しい回数評価されるようにした。例えば、一回選択されたサンプルと未選択のサンプルが存在する場合、一回選択されたサンプルは選択の候補から除外する。これにより、未選択のサンプルのみを対象としたランダムサンプリングを行うことで、最終的な評価回数が等しくなるよう実装した。本実験では手法が七種類、話者が四人存在するため、28回の実験によって、手法・話者・文章のすべての組み合わせが一回ずつ評価される、すなわち全サンプルが一回ずつ評価されることになる。

% \begin{table*}[bt]
%     \centering
%     \caption{主観評価実験のサンプル選択における文章グループと手法の対応関係}
%     \label{sec4:tab:sbj_selection_relation}
%     \begin{center}
%         \renewcommand{\arraystretch}{1.0} % 行の高さ調整
%         \setlength{\tabcolsep}{8pt}      % 列の幅調整
%         \scalebox{0.9}{
%             \begin{tabular}{|c|ccccccc|}
%                 \hline
%                 \multirow{2}{*}{}         & \multicolumn{7}{c|}{文章グループインデックス}                         \\
%                                           & 1                                 & 2 & 3 & 4 & 5 & 6 & 7 \\
%                 \hline
%                 \multirow{7}{*}{手法インデックス} & 1                                 & 2 & 3 & 4 & 5 & 6 & 7 \\
%                                           & 2                                 & 3 & 4 & 5 & 6 & 7 & 1 \\
%                                           & 3                                 & 4 & 5 & 6 & 7 & 1 & 2 \\
%                                           & 4                                 & 5 & 6 & 7 & 1 & 2 & 3 \\
%                                           & 5                                 & 6 & 7 & 1 & 2 & 3 & 4 \\
%                                           & 6                                 & 7 & 1 & 2 & 3 & 4 & 5 \\
%                                           & 7                                 & 1 & 2 & 3 & 4 & 5 & 6 \\
%                 \hline
%             \end{tabular}
%         }
%     \end{center}
% \end{table*}

% 四つ目の練習試行（類似性）および五つ目の本番試行（類似性）では、評価対象の音声と同一話者の原音声の類似性の評価を実施した。ここでも初めに練習試行を行っていただくことで実験内容を把握してもらい、その後本番施行を行っていただく流れとした。ここで、練習施行は何度でも実施可能とし、本番試行は一回のみ実施可能とした。
% 評価項目について、類似性は「評価対象の音声が同一話者の原音声とどれくらい似ているか」を評価するものとした。実際の評価プロセスは以下の二段階で構成した。
% \begin{enumerate}
%     \item 評価対象の音声と原音声を聞き比べてもらう。
%     \item 評価対象の音声が原音声にどれくらい似ていたかを五段階評価してもらう。
% \end{enumerate}
% 5段階評価の回答項目は以下のようにした。
% \begin{enumerate}
%     \item 全く似ていなかった
%     \item あまり似ていなかった
%     \item やや似ていた
%     \item かなり似ていた
%     \item 同じ話者に聞こえた
% \end{enumerate}
% 実験に利用した音声サンプルおよび、被験者ごとの評価サンプルの割り当て方法は明瞭性の評価実験と同様である。ただし、類似性評価においては同一話者の原音声を発話内容についてランダムに選択し、評価対象となるサンプルとペアで提示できるようにした。評価時は、明瞭性評価と同様に音声サンプルを何度でも聞けるようにしたが、発話文章については提示しなかった。なぜなら、類似性評価では評価が発話文章に依存しないからである。実際、評価サンプルのペアとなる原音声サンプルは発話文章をランダムに選択しているため、一致する場合も異なる場合も存在する。

% また、オンラインでの評価は効率よく数多くの方に評価していただけるという点でメリットがあるが、オフラインでの評価と比較して実験環境を制御することが難しく、評価品質が低下する恐れがある。これに対して、本実験では先行研究\cite{kirkland2023stuck}を参考に、評価サンプル中にダミー音声を混入させることで対策を講じた。ダミー音声は本研究で得られた合成音声とは無関係に、gTTSというライブラリを用いて生成したサンプルである。具体例として、明瞭性評価では
% \begin{quote}
%     これはダミー音声です。明瞭性は「3: ある程度聞き取れた」を選択してください。
% \end{quote}
% のような発話内容の音声を、類似性評価では
% \begin{quote}
%     これはダミー音声です。類似性は「1: 全く同じ話者には聞こえなかった」を選択してください。
% \end{quote}
% のような発話内容の音声を提示した。この時、その音声自体の明瞭性や類似性とは無関係に、必ずこの音声によって指定された評価値を選択するよう説明を与えた。本番試行においてダミー音声で指定された評価値を誤って選んだ場合は、すべての回答を無効にする旨を被験者に伝えた。実際、実験終了後にはそのようにデータを処理した。

% 被験者数および各手法の評価回数に関して、先行研究\cite{wester2015we}では主観評価実験の結果に対する統計処理について、そこで用いる被験者数や手法ごとの評価回数を変数とし、実験条件に対してどれほどの被験者数とサンプル数が必要そうであるかを検討している。今回はこの研究を参考にしつつ、オンラインで実験を実施するのであれば総被験者数が100人以上、各手法に対する総評価回数が200回以上となることが望ましいと判断した。前述した各被験者に対するサンプルの選択方法により、28回の実験によって全てのサンプルが一回ずつ評価される。これを1セットとすると、セットあたり被験者数は28人、各手法に対する評価回数は212回となる。従って、今回は4セット行うことで、総被験者数112人、各手法に対しての総評価回数が848回となるようにした。実験は30分程度で終わると見積もって、一人当たりの報酬は500円とした。

\subsection{結果}
\subsubsection{客観評価}
まず、損失関数~\eqref{sec4:eq:loss}の重み係数$\lambda_{ssl^{d}}$を変化させた時の、客観評価指標の全テストデータに渡る平均値を表~\ref{sec4:tab:obj_weights}に示す。各手法ごとに0.0001から1.0まで10倍刻みで5段階検討し、各手法の客観指標ごとに最も優れた値を下線で示している。最良エポックは検証データに対する損失が最小となったエポックであり、テストデータの合成にはこのエポックにおけるチェックポイントを利用した。また、$L_{mel}$、$L_{ssl^{d}}$、$L$は最良エポックにおける検証データに対する損失の平均値である。また、これ以降の比較のために、最適だと考えられる$\lambda_{ssl^{d}}$の値を選択しており、選択された行を太字で表している。

手法1では、$\lambda_{ssl^{d}}$の値が0.0001の時にWERが最も低く、0.01の時に話者類似度が最も高くなった。現状WERの高さが特に課題であり、話者類似度は0.004とわずかな違いでもあるため、今回はWERが最小であることを優先して0.0001が最適であると判断した。$\lambda_{ssl^{d}}$の値による評価指標の変化について、WERは$\lambda_{ssl^{d}}$の値を大きくするのに伴って単調に増加していることがわかる。一方、話者類似度は$\lambda_{ssl^{d}}$の値が0.1以上となった時に、0.01以下であった場合と比較して顕著に低下することがわかる。次に、図~\ref{sec4:fig:learning_curve_method_1_val_losses}に手法1における学習曲線の結果を示す。横軸がエポック数、縦軸が損失の値を表す。損失の値は各エポックにおける平均値である。実線は検証データに対する損失、点線は学習データに対する損失を表しており、線の色は$\lambda_{ssl^{d}}$の違いを表す。また、丸いマーカーは表~\ref{sec4:tab:obj_method_comp}に示した最良エポック時における損失の値を表す。学習曲線より、$\lambda_{ssl^{d}}$の値を変化させることによって、特に$L_{ssl^{d}}$の傾向が変化していることがわかる。具体的には、$\lambda_{ssl^{d}}$の値を0.0001から1.0へと増加させるのに伴って、学習初期における損失の下がり方が急峻になっており、達する最小値自体が小さくなっていることがわかる。また、$\lambda_{ssl^{d}}$の値が0.1以上の場合、検証データに対する$L_{ssl^{d}}$は早いうちから増加傾向に転じている。これに伴い、今回は検証データに対する$L$の値を監視し、Early Stoppingの適用と最良エポックの決定を行なったため、$L_{mel}$が下がり切らない状態で学習が中断される結果となった。客観評価指標において、特に$\lambda_{ssl^{d}}$の値が0.1以上となるとき、0.01以下の場合と比較して話者類似度の低下や、WERの上昇傾向が見られていたが、学習曲線の挙動より、$L_{mel}$を下げきれなくなっていたことが原因として考えられる。最適な$\lambda_{ssl^{d}}$の値は客観評価指標から0.0001としたが、0.01以下ではそれと概ね同程度の品質であったことを考えると、手法1では検証データに対する$L_{mel}$の値を十分小さくすることのできる$\lambda_{ssl^{d}}$が適していると考えられる。

手法2では、$\lambda_{ssl^{d}}$の値が0.1の時にWERが最も低く、0.0001の時に話者類似度が最も高くなった。ここでは$\lambda_{ssl^{d}}$の値が0.1の時に、ベースラインで選択された最適なケースと比較してWERが9.1\%低下しており、話者類似度についても0.003高くなっていることから、0.1が最適だと判断した。$\lambda_{ssl^{d}}$の値による評価指標の変化について、$\lambda_{ssl^{d}}$を0.1以上とすることで、0.01以下の場合と比較してWERが低下する傾向が見られた。しかし、1.0まで大きくすると0.1の場合よりもWERが大きくなっており、単調に減少していないこともわかる。話者類似度については、$\lambda_{ssl^{d}}$の値を0.1としたときに、0.01以下の場合と比較して値が少し低下し、さらに1.0まで大きくすることで0.1以下の場合と比較して顕著に低下していることがわかる。次に、図~\ref{sec4:fig:learning_curve_method_2_val_losses}に手法2における学習曲線の結果を示す。手法1と同様に、$\lambda_{ssl^{d}}$の値を0.0001から1.0へと増加させるのに伴って、学習初期における$L_{ssl^{d}}$の下がり方が急峻になっており、達する最小値自体が小さくなっていることがわかる。また、$\lambda_{ssl^{d}}$が1.0の場合に、$L_{mel}$を下げきれなくなる傾向が見られる。加えて、手法2では$\lambda_{ssl^{d}}$が0.1の場合における$L_{ssl^{d}}$の増加が緩やかであり、$L_{mel}$も十分下げられていることがわかる（ただし、表~\ref{sec4:tab:obj_weights}の$L_{ssl^{d}}$より、手法1の$\lambda_{ssl^{d}}$が0.1の場合と比較して、損失の値自体は大きい）。さらに、$\lambda_{ssl^{d}}$が0.1の場合、$L_{mel}$が達する最小値自体が、$\lambda_{ssl^{d}}$が0.01以下の場合と比較して小さくなっていることがわかる（具体的な値は表~\ref{sec4:tab:obj_weights}の$L_{mel}$の値を参照）。これは手法1では見られなかった新たな傾向であった。最適な$\lambda_{ssl^{d}}$の値は客観評価指標から0.1としたが、学習曲線の挙動より、この時他の値の場合と比較して$L_{mel}$と$L_{ssl^{d}}$の両方をバランスよく下げられていたことがわかった。よって、手法2においては$L_{mel}$と$L_{ssl^{d}}$の両方をバランスよく下げられるような、程よい大きさの重みが適していると考えられる。

手法3における最適値の選択理由は手法2と同様であり、$\lambda_{ssl^{d}}$の値による評価指標の変化についても同様であった。次に、図~\ref{sec4:fig:learning_curve_method_3_val_losses}に手法3における学習曲線の結果を示す。傾向は手法2と概ね同様であるが、手法3では$\lambda_{ssl^{d}}$の値が0.1の場合における$L_{ssl^{d}}$の増加が早く、学習が早期に中断されている点は異なる。しかし、この時$L_{mel}$も十分下げられており、$L_{mel}$が達する最小値自体が、$\lambda_{ssl^{d}}$が0.01以下の場合と比較して小さくなっていることがわかる（具体的な値は表~\ref{sec4:tab:obj_weights}の$L_{mel}$の値を参照）。以上より、細かな違いはあるものの手法3は手法2と同様の挙動を示し、$\lambda_{ssl^{d}}$は$L_{mel}$と$L_{ssl^{d}}$の両方をバランスよく下げられるような、程よい大きさの重みが適していると考えられる。

手法4及び手法5についても、最適値の選択理由は手法2と同様である。しかし、これらは$\lambda_{ssl^{d}}$の値を0.1としたときの話者類似度の低下の度合いが手法2、手法3と比較して大きい点で、傾向が異なっていた。次に、図~\ref{sec4:fig:learning_curve_method_4_val_losses}に手法4、図~\ref{sec4:fig:learning_curve_method_5_val_losses}に手法5における学習曲線の結果を示す。傾向については、$L_{ssl^{d}}$の増加の程度について細かな違いはあるが、概ね手法2と同様であり、$L_{mel}$と$L_{ssl^{d}}$の両方をバランスよく下げられるような、程よい大きさの重みが適していると考えられる。また、手法4と手法5において、$\lambda_{ssl^{d}}$が0.1の場合における話者類似度は、手法2および手法3と比較して低くなっていたが、$L_{mel}$や$L_{ssl^{d}}$の値からは一貫した傾向が見られない。これより、平均値からこの違いを分析することは難しいが、実際の細かな誤差の出方が異なっていて、これが影響を与えているのではないかと考える。

次に、最適なチューニングをした場合における、手法ごとの客観評価指標の全テストデータに渡る平均値を表~\ref{sec4:tab:obj_method_comp}に示す。分析合成は、原音声から計算した特徴量を入力として、Multi-input Vocoderで逆変換した合成音声であり、本実験下において合成音声により達成され得る上限値を表す。手法1から手法5については、表~\ref{sec4:tab:obj_weights}において太字としたもの、すなわち最適なチューニングだと判断されたものを選択している。また、分析合成、原音声を除いた合成音声（手法1から手法5）の中で、最も優れた値を下線で示している。
これより、WER、話者類似度ともに手法3が最も優れていることが分かる。特に、今回のベースラインである手法1と比較すると、WERが9.5\%低下し、話者類似度は0.01高くなっていることから、提案手法がベースラインよりもより聞き取りやすく、話者性を反映した音声を合成できたと考えられる。
また、HuBERTの事前学習済み重みを初期値とすることの効果について、手法2と手法4を比較すると、手法2の方がWERが1.2\%低く、話者類似度が0.017高いことがわかる。よって、本研究ではHuBERTの事前学習済み重みを初期値とした転移学習が有効である可能性を検討したが、むしろ事前学習済み重みを初期値とせず、ランダム初期化したモデルの方が優れていたと考えられる。これは仮説に反した結果であったが、事前学習済み重みによって与えられる初期値が、より良い局所解への収束には繋がらなかったことが原因だと考えられる。一方、アンサンブル手法の有効性について、手法2と手法3を比較すると、アンサンブル手法を導入することによってWERが0.4\%低下し、話者類似度が0.007高くなっていることから、わずかではあるが改善していることがわかる。しかし、手法4と手法5を比較すると、WERは変化せず、話者類似度は0.017低下していることから、特に話者類似度の観点で悪化したことがわかる。よって、アンサンブル手法は必ずしも改善につながるわけではなく、改善するとしても顕著な変化はもたらさないと考えられる。

次に、最適なチューニングをした場合における手法ごとに、発話文章ごとのWERの比較を行った結果を図~\ref{sec4:fig:wer_sample_wise_comparison}に示す。ここで、縦軸は発話文章を表し、横軸はベースラインである手法1とその他手法の間でWERの差を計算し、発話文章ごとに平均した値を表す。負の値を取っているとき、手法1に対してより低いWERを達成したと解釈できる。図より、表~\ref{sec4:tab:obj_method_comp}における平均値のみを見れば、手法2から手法5の全てが手法1に対してより低いWERを達成していたが、発話文章ごとに見れば、手法1に対してWERが高くなっているサンプルもあることがわかる。例えば、「ATR503\_j11」では、手法2から手法5の全てがベースラインよりも20\%前後高いWERとなっていることがわかる。また、全体的な傾向として手法ごとにWERはばらけており、いかなる場合においても最良となるようなモデルは構築できていないことがわかる。これより、現状のいかなるモデルも発話文章に対する汎化性能が不十分であり、さらなる改善が必要だと考えられる。

次に、話者ごとのWERの比較を行った結果を図~\ref{sec4:fig:wer_speaker_wise_comparison}に示す。ここで縦軸は話者を表し、横軸はベースラインである手法1とその他手法の間でWERの差を計算し、話者ごとに平均した値を表す。負の値を取っているとき、手法1に対してより低いWERを達成したと解釈できる。これより、手法2から手法5は平均的にいかなる話者に対しても手法1より低いWERを達成していることがわかるが、一方でその改善の度合いは話者によって異なることがわかる。特に、「F02\_kablab」はその他三人の話者と比較して改善の度合いが小さい。また、例えば「F01\_kablab」では手法2と手法3がより有効である一方、「F02\_kablab」では手法4と手法5がより有効となっており、有効な手法が話者によって異なることもわかる。これより、話者に対する性能の依存があると考えられ、さらなる汎化性能の向上が必要だと言える。

次に、話者ごとの話者類似度の比較を行った結果を図~\ref{sec4:fig:spk_sim_speaker_wise_comparison}に示す。ここで縦軸は話者を表し、横軸はベースラインである手法1とその他手法の間で話者類似度の差を計算し、話者ごとに平均した値を表す。正の値を取っているとき、手法1に対してより高い話者類似度を達成したと解釈できる。これを見ると、手法4と手法5はいかなる話者に対しても話者類似度を悪化させたことがわかる。手法4と手法5は事前学習済み重みを初期値としたモデルであったため、この影響が出ている可能性がある。今回検討したHuBERTは音声の言語情報を考慮する点で強みがあり、実際音声認識ではその転移学習の有効性が確認されている。しかし、話者性を考慮するという点で特に有効でなく、話者性を軽視するような局所解への収束に繋がった可能性が考えられる。ただ、表~\ref{sec4:tab:obj_weights}より手法4と手法5においても$\lambda_{ssl^{d}}$が0.01以下であれば、比較的高い話者類似度を達成していたため、損失の重み付けにも依存していると言える。また、手法3はいかなる話者に対しても類似度を向上させており、この点で優れていたと言える。手法2では「F02\_kablab」のみ話者類似度が悪化しており、手法3と比較して話者に対する汎化性能が低かったと考えられる。

以上のことから、提案手法である手法2から手法5はいずれも手法1に対して平均的にWERを低下させ、特に手法2及び手法3は話者類似度についても若干の改善を達成したことから、提案手法によるベースラインからの改善が達成できたと考える。また、HuBERTの事前学習済み重みを用いることは仮説に反して有効ではなく、アンサンブル手法についてもその効果は顕著なものではなかった。さらに、特に手法2から手法5は損失関数の重み係数である$\lambda_{ssl^{d}}$による性能の変化が著しく、ここで最適値を選択できなければベースラインである手法1から改善しないこともわかった。よって、提案手法におけるベースラインからの改善に寄与した可能性のあるポイントは、以下でまとめられる。
\begin{enumerate}
    \item HuBERT中間特徴量を入力とし、メルスペクトログラムとHuBERT離散特徴量を推定するネットワークを導入したこと
    \item 上記のネットワーク構造をHuBERT Transformerとしたこと
    \item ネットワークをランダム初期化した上で学習させたこと
    \item 最適な$\lambda_{ssl^{d}}$の値を発見できたこと
\end{enumerate}
本実験からは、HuBERT Transformerをランダム初期化する必要性と、$\lambda_{ssl^{d}}$の値のチューニングを行う必要性が明らかとなった。しかし、入力特徴量やネットワーク構造についてはHuBERTを用いることを前提とした実験条件であったことから、検討できていない。実験結果から、もはやHuBERTに依存する必要性は無くなっているため、入力がHuBERT中間特徴量でなくても良いし、ネットワークも任意に選択できる。入力については、例えば最終予測値であるメルスペクトログラムやHuBERT離散特徴量を採用しても実装は可能であるが、これによって性能が変化するのであれば、HuBERT中間特徴量が良い入力特徴量であると考えられる。ネットワークについては、Transformer自体が多くの場合有効であるため現状で適切なものとなっている可能性もあるが、検討の余地はある。こういった点が、今後の検討課題として挙げられる。

% \begin{table*}[bt]
%     \centering
%     \caption{損失関数の重み係数による客観評価指標の比較}
%     \label{sec4:tab:obj_weights}
%     \begin{center}
%         \renewcommand{\arraystretch}{1.0} % 行の高さ調整
%         \setlength{\tabcolsep}{8pt}      % 列の幅調整
%         \scalebox{0.85}{
%             \begin{tabular}{|l|l|l|rrc|}
%                 \hline
%                 \multicolumn{1}{|c|}{手法} & \multicolumn{1}{c|}{詳細}      & \multicolumn{1}{c|}{$\lambda_{ssl^{d}}$} & \multicolumn{1}{c}{WER [\%]} & \multicolumn{1}{c}{話者類似度} & \multicolumn{1}{c|}{選択フラグ} \\
%                 \hline
%                 1                        & ベースライン                       & 0.0001                                   & \underline{55.3}             & 0.841                     & True                       \\
%                 1                        & ベースライン                       & 0.001                                    & 55.6                         & 0.841                     &                            \\
%                 1                        & ベースライン                       & 0.01                                     & 56.7                         & \underline{0.845}         &                            \\
%                 1                        & ベースライン                       & 0.1                                      & 57.7                         & 0.764                     &                            \\
%                 1                        & ベースライン                       & 1.0                                      & 62.1                         & 0.699                     &                            \\
%                 \hline
%                 2                        & 事前学習なしTransformer            & 0.0001                                   & 58.9                         & \underline{0.860}         &                            \\
%                 2                        & 事前学習なしTransformer            & 0.001                                    & 58.3                         & 0.857                     &                            \\
%                 2                        & 事前学習なしTransformer            & 0.01                                     & 56.7                         & 0.859                     &                            \\
%                 2                        & 事前学習なしTransformer            & 0.1                                      & \underline{46.2}             & 0.844                     & True                       \\
%                 2                        & 事前学習なしTransformer            & 1.0                                      & 50.3                         & 0.706                     &                            \\
%                 \hline
%                 3                        & {事前学習なしTransformer + アンサンブル} & 0.0001                                   & 58.8                         & 0.860                     &                            \\
%                 3                        & {事前学習なしTransformer + アンサンブル} & 0.001                                    & 58.1                         & 0.854                     &                            \\
%                 3                        & {事前学習なしTransformer + アンサンブル} & 0.01                                     & 56.5                         & \underline{0.865}         &                            \\
%                 3                        & {事前学習なしTransformer + アンサンブル} & 0.1                                      & \underline{45.8}             & 0.851                     & True                       \\
%                 3                        & {事前学習なしTransformer + アンサンブル} & 1.0                                      & 48.6                         & 0.755                     &                            \\
%                 \hline
%                 4                        & 事前学習ありTransformer            & 0.0001                                   & 58.2                         & 0.849                     &                            \\
%                 4                        & 事前学習ありTransformer            & 0.001                                    & 57.4                         & 0.847                     &                            \\
%                 4                        & 事前学習ありTransformer            & 0.01                                     & 56.0                         & \underline{0.853}         &                            \\
%                 4                        & 事前学習ありTransformer            & 0.1                                      & \underline{47.4}             & 0.827                     & True                       \\
%                 4                        & 事前学習ありTransformer            & 1.0                                      & 48.4                         & 0.720                     &                            \\
%                 \hline
%                 5                        & {事前学習ありTransformer + アンサンブル} & 0.0001                                   & 57.8                         & 0.857                     &                            \\
%                 5                        & {事前学習ありTransformer + アンサンブル} & 0.001                                    & 57.5                         & 0.853                     &                            \\
%                 5                        & {事前学習ありTransformer + アンサンブル} & 0.01                                     & 58.1                         & \underline{0.865}         &                            \\
%                 5                        & {事前学習ありTransformer + アンサンブル} & 0.1                                      & \underline{47.4}             & 0.810                     & True                       \\
%                 5                        & {事前学習ありTransformer + アンサンブル} & 1.0                                      & 47.7                         & 0.748                     &                            \\
%                 \hline
%             \end{tabular}
%         }
%     \end{center}
% \end{table*}

\begin{table*}[bt]
    \centering
    \caption{損失関数の重み係数$\lambda_{ssl^{d}}$による客観評価指標の比較}
    \label{sec4:tab:obj_weights}
    \begin{center}
        \renewcommand{\arraystretch}{1.0} % 行の高さ調整
        \setlength{\tabcolsep}{8pt}      % 列の幅調整
        \scalebox{1.0}{
            \begin{tabular}{|c|l|rrrrrr|}
                \hline
                \multicolumn{1}{|c|}{手法} & \multicolumn{1}{c|}{$\lambda_{ssl^{d}}$} & \multicolumn{1}{c}{WER [\%]} & \multicolumn{1}{c}{話者類似度} & \multicolumn{1}{c}{最良エポック} & \multicolumn{1}{c}{$L_{mel}$} & \multicolumn{1}{c}{$L_{ssl^{d}}$} & \multicolumn{1}{c|}{$L$} \\
                \hline
                \textbf{1}               & \textbf{0.0001}                          & \textbf{\underline{55.3}}    & \textbf{0.841}            & \textbf{47}                & \textbf{0.632}                & \textbf{2.147}                    & \textbf{0.633}           \\
                1                        & 0.001                                    & 55.6                         & 0.841                     & 45                         & 0.629                         & 2.049                             & 0.631                    \\
                1                        & 0.01                                     & 56.7                         & \underline{0.845}         & 47                         & 0.631                         & 1.990                             & 0.651                    \\
                1                        & 0.1                                      & 57.7                         & 0.764                     & 14                         & 0.645                         & 1.793                             & 0.824                    \\
                1                        & 1.0                                      & 62.1                         & 0.699                     & 9                          & 0.672                         & 1.760                             & 2.431                    \\
                \hline
                2                        & 0.0001                                   & 58.9                         & \underline{0.860}         & 46                         & 0.630                         & 2.521                             & 0.630                    \\
                2                        & 0.001                                    & 58.3                         & 0.857                     & 45                         & 0.631                         & 2.382                             & 0.634                    \\
                2                        & 0.01                                     & 56.7                         & 0.859                     & 48                         & 0.631                         & 2.104                             & 0.652                    \\
                \textbf{2}               & \textbf{0.1}                             & \textbf{\underline{46.2}}    & \textbf{0.844}            & \textbf{32}                & \textbf{0.620}                & \textbf{1.942}                    & \textbf{0.814}           \\
                2                        & 1.0                                      & 50.3                         & 0.706                     & 9                          & 0.651                         & 1.713                             & 2.364                    \\
                \hline
                3                        & 0.0001                                   & 58.8                         & 0.860                     & 22                         & 0.632                         & 2.584                             & 0.632                    \\
                3                        & 0.001                                    & 58.1                         & 0.854                     & 22                         & 0.633                         & 2.448                             & 0.636                    \\
                3                        & 0.01                                     & 56.5                         & \underline{0.865}         & 45                         & 0.630                         & 2.077                             & 0.651                    \\
                \textbf{3}               & \textbf{0.1}                             & \textbf{\underline{45.8}}    & \textbf{0.851}            & \textbf{18}                & \textbf{0.622}                & \textbf{2.041}                    & \textbf{0.826}           \\
                3                        & 1.0                                      & 48.6                         & 0.755                     & 14                         & 0.634                         & 1.735                             & 2.370                    \\
                \hline
                4                        & 0.0001                                   & 58.2                         & 0.849                     & 20                         & 0.632                         & 2.593                             & 0.632                    \\
                4                        & 0.001                                    & 57.4                         & 0.847                     & 24                         & 0.633                         & 2.496                             & 0.635                    \\
                4                        & 0.01                                     & 56.0                         & \underline{0.853}         & 30                         & 0.630                         & 2.106                             & 0.651                    \\
                \textbf{4}               & \textbf{0.1}                             & \textbf{\underline{47.4}}    & \textbf{0.827}            & \textbf{17}                & \textbf{0.622}                & \textbf{1.929}                    & \textbf{0.815}           \\
                4                        & 1.0                                      & 48.4                         & 0.720                     & 10                         & 0.648                         & 1.746                             & 2.394                    \\
                \hline
                5                        & 0.0001                                   & 57.8                         & 0.857                     & 22                         & 0.632                         & 2.547                             & 0.632                    \\
                5                        & 0.001                                    & 57.5                         & 0.853                     & 27                         & 0.635                         & 2.449                             & 0.637                    \\
                5                        & 0.01                                     & 58.1                         & \underline{0.865}         & 44                         & 0.632                         & 2.100                             & 0.653                    \\
                \textbf{5}               & \textbf{0.1}                             & \textbf{\underline{47.4}}    & \textbf{0.810}            & \textbf{6}                 & \textbf{0.633}                & \textbf{1.981}                    & \textbf{0.831}           \\
                5                        & 1.0                                      & 47.7                         & 0.748                     & 16                         & 0.637                         & 1.786                             & 2.423                    \\
                \hline
            \end{tabular}
        }
    \end{center}
\end{table*}

\begin{figure}[bt]
    \centering
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[height=55mm]{./figure/sec4/learning_curve/impact_of_loss_weights_across_methods/0/mel_loss.png}
        \caption{メルスペクトログラムのMAE Loss（式~\eqref{sec4:eq:loss}の$L_{mel}$）}
        \label{sec4:fig:learning_curve_method_1_val_mel_loss}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[height=55mm]{./figure/sec4/learning_curve/impact_of_loss_weights_across_methods/0/ssl_feature_cluster_loss.png}
        \caption{HuBERT離散特徴量のCross Entropy Loss（式~\eqref{sec4:eq:loss}の$L_{ssl^{d}}$）}
        \label{sec4:fig:learning_curve_method_1_val_ssl_feature_cluster_loss}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[height=55mm]{./figure/sec4/learning_curve/impact_of_loss_weights_across_methods/0/total_loss.png}
        \caption{損失の合計値（式~\eqref{sec4:eq:loss}の$L$）}
        \label{sec4:fig:learning_curve_method_1_val_total_loss}
    \end{subfigure}
    \caption{手法1における学習曲線}
    \label{sec4:fig:learning_curve_method_1_val_losses}
\end{figure}

\begin{figure}[bt]
    \centering
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[height=55mm]{./figure/sec4/learning_curve/impact_of_loss_weights_across_methods/2/mel_loss.png}
        \caption{メルスペクトログラムのMAE Loss（式~\eqref{sec4:eq:loss}の$L_{mel}$）}
        \label{sec4:fig:learning_curve_method_2_val_mel_loss}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[height=55mm]{./figure/sec4/learning_curve/impact_of_loss_weights_across_methods/2/ssl_feature_cluster_loss.png}
        \caption{HuBERT離散特徴量のCross Entropy Loss（式~\eqref{sec4:eq:loss}の$L_{ssl^{d}}$）}
        \label{sec4:fig:learning_curve_method_2_val_ssl_feature_cluster_loss}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[height=55mm]{./figure/sec4/learning_curve/impact_of_loss_weights_across_methods/2/total_loss.png}
        \caption{損失の合計値（式~\eqref{sec4:eq:loss}の$L$）}
        \label{sec4:fig:learning_curve_method_2_val_total_loss}
    \end{subfigure}
    \caption{手法2における学習曲線}
    \label{sec4:fig:learning_curve_method_2_val_losses}
\end{figure}

\begin{figure}[bt]
    \centering
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[height=55mm]{./figure/sec4/learning_curve/impact_of_loss_weights_across_methods/3/mel_loss.png}
        \caption{メルスペクトログラムのMAE Loss（式~\eqref{sec4:eq:loss}の$L_{mel}$）}
        \label{sec4:fig:learning_curve_method_3_val_mel_loss}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[height=55mm]{./figure/sec4/learning_curve/impact_of_loss_weights_across_methods/3/ssl_feature_cluster_loss.png}
        \caption{HuBERT離散特徴量のCross Entropy Loss（式~\eqref{sec4:eq:loss}の$L_{ssl^{d}}$）}
        \label{sec4:fig:learning_curve_method_3_val_ssl_feature_cluster_loss}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[height=55mm]{./figure/sec4/learning_curve/impact_of_loss_weights_across_methods/3/total_loss.png}
        \caption{損失の合計値（式~\eqref{sec4:eq:loss}の$L$）}
        \label{sec4:fig:learning_curve_method_3_val_total_loss}
    \end{subfigure}
    \caption{手法3における学習曲線}
    \label{sec4:fig:learning_curve_method_3_val_losses}
\end{figure}

\begin{figure}[bt]
    \centering
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[height=55mm]{./figure/sec4/learning_curve/impact_of_loss_weights_across_methods/4/mel_loss.png}
        \caption{メルスペクトログラムのMAE Loss（式~\eqref{sec4:eq:loss}の$L_{mel}$）}
        \label{sec4:fig:learning_curve_method_4_val_mel_loss}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[height=55mm]{./figure/sec4/learning_curve/impact_of_loss_weights_across_methods/4/ssl_feature_cluster_loss.png}
        \caption{HuBERT離散特徴量のCross Entropy Loss（式~\eqref{sec4:eq:loss}の$L_{ssl^{d}}$）}
        \label{sec4:fig:learning_curve_method_4_val_ssl_feature_cluster_loss}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[height=55mm]{./figure/sec4/learning_curve/impact_of_loss_weights_across_methods/4/total_loss.png}
        \caption{損失の合計値（式~\eqref{sec4:eq:loss}の$L$）}
        \label{sec4:fig:learning_curve_method_4_val_total_loss}
    \end{subfigure}
    \caption{手法4における学習曲線}
    \label{sec4:fig:learning_curve_method_4_val_losses}
\end{figure}

\begin{figure}[bt]
    \centering
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[height=55mm]{./figure/sec4/learning_curve/impact_of_loss_weights_across_methods/5/mel_loss.png}
        \caption{メルスペクトログラムのMAE Loss（式~\eqref{sec4:eq:loss}の$L_{mel}$）}
        \label{sec4:fig:learning_curve_method_5_val_mel_loss}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[height=55mm]{./figure/sec4/learning_curve/impact_of_loss_weights_across_methods/5/ssl_feature_cluster_loss.png}
        \caption{HuBERT離散特徴量のCross Entropy Loss（式~\eqref{sec4:eq:loss}の$L_{ssl^{d}}$）}
        \label{sec4:fig:learning_curve_method_5_val_ssl_feature_cluster_loss}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[height=55mm]{./figure/sec4/learning_curve/impact_of_loss_weights_across_methods/5/total_loss.png}
        \caption{損失の合計値（式~\eqref{sec4:eq:loss}の$L$）}
        \label{sec4:fig:learning_curve_method_5_val_total_loss}
    \end{subfigure}
    \caption{手法5における学習曲線}
    \label{sec4:fig:learning_curve_method_5_val_losses}
\end{figure}

\begin{table*}[bt]
    \centering
    \caption{最適なチューニングをした場合における手法ごとの比較}
    \label{sec4:tab:obj_method_comp}
    \begin{center}
        \renewcommand{\arraystretch}{1.0} % 行の高さ調整
        \setlength{\tabcolsep}{8pt}      % 列の幅調整
        \scalebox{1.0}{
            \begin{tabular}{|c|l|rr|}
                \hline
                \multicolumn{1}{|c|}{手法} & \multicolumn{1}{c|}{詳細}     & \multicolumn{1}{c}{WER [\%]} & \multicolumn{1}{c|}{話者類似度} \\
                \hline
                1                        & ベースライン                      & 55.3                         & 0.841                      \\
                2                        & ランダム初期化Transformer          & 46.2                         & 0.844                      \\
                3                        & ランダム初期化Transformer + アンサンブル & \underline{45.8}             & \underline{0.851}          \\
                4                        & 事前学習ありTransformer           & 47.4                         & 0.827                      \\
                5                        & 事前学習ありTransformer + アンサンブル  & 47.4                         & 0.810                      \\
                \hline
                6                        & 分析合成                        & 4.8                          & 0.944                      \\
                7                        & 原音声                         & 4.5                          & 1.000                      \\
                \hline
            \end{tabular}
        }
    \end{center}
\end{table*}

\begin{figure}[bt]
    \centering
    \includegraphics[height=190mm]{./figure/sec4/obj_metrics/wer_sample_wise_comparison.png}
    \caption{テストデータにおける発話文章ごとのWERの比較}
    \label{sec4:fig:wer_sample_wise_comparison}
\end{figure}

\begin{figure}[bt]
    \centering
    \includegraphics[height=40mm]{./figure/sec4/obj_metrics/wer_speaker_wise_comparison.png}
    \caption{テストデータにおける話者ごとのWERの比較}
    \label{sec4:fig:wer_speaker_wise_comparison}
\end{figure}

\begin{figure}[bt]
    \centering
    \includegraphics[height=40mm]{./figure/sec4/obj_metrics/spk_sim_speaker_wise_comparison.png}
    \caption{テストデータにおける話者ごとの話者類似度の比較}
    \label{sec4:fig:spk_sim_speaker_wise_comparison}
\end{figure}

\clearpage

\subsubsection{主観評価}

\clearpage

\subsection{まとめ}

\clearpage

\section{結論}

\clearpage

\section*{謝辞}
\addcontentsline{toc}{section}{謝辞}

\clearpage

\bibliographystyle{junsrt}
\addcontentsline{toc}{section}{参考文献}
\bibliography{library}

\end{document}