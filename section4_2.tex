\subsection{実験方法}
\subsubsection{利用したデータセット}
動画音声データセットには，先行研究\cite{taguchi,esaki}で収録されたもののうち，ATR音素バランス文\cite{atr}を読み上げたデータを利用した．このデータセットは、男女二人ずつから収録された合計4人分のデータから構成される。分割は，AからHセットを学習データ，Iセットを検証データ，Jセットをテストデータとした．各分割ごとの文章数を表~\ref{sec4:tab:dataset_info}に示す．

ボコーダの学習には，Hi-Fi-Captain\cite{okamoto2023hi}とJVS\cite{takamichi2019jvs}を利用した．Hi-Fi-Captainは，日本語話者2名と英語話者2名からなるデータセットであるが，本実験では日本語話者2名分のデータのみを利用した．分割は，train-parallelおよびtrain-non-parallelを学習データ，valを検証データ，evalをテストデータとした．各分割ごとの文章数を表~\ref{sec4:tab:dataset_info}に示す．JVSは100人の日本語話者からなるデータであり，各話者に対して1から100まで番号が割り振られている．読み上げ音声のparallel100およびnonpara30と，裏声のfalset10，囁き声のwhisper10が含まれるが，本研究ではparallel100とnonpara30のみ利用した．分割は，1から80番の話者を学習データ，81番から90番の話者を検証データ，91番から100番までの話者をテストデータとした．各分割ごとの文章数を表~\ref{sec4:tab:dataset_info}に示す．

\begin{table*}[bt]
    \centering
    \caption{利用したデータセットの文章数}
    \label{sec4:tab:dataset_info}
    \begin{center}
        \renewcommand{\arraystretch}{0.9} % 行の高さ調整
        \setlength{\tabcolsep}{8pt}      % 列の幅調整
        \scalebox{1.0}{
            \begin{tabular}{|l|r|r|r|}
                \hline
                              & \multicolumn{1}{c|}{学習} & \multicolumn{1}{c|}{検証} & \multicolumn{1}{c|}{テスト} \\
                \hline
                動画音声データセット    & 1598                    & 200                     & 212                      \\
                Hi-Fi-Captain & 37714                   & 200                     & 200                      \\
                JVS           & 10398                   & 1299                    & 1300                     \\
                \hline
            \end{tabular}
        }
    \end{center}
\end{table*}

\subsubsection{データの前処理}
動画データは60 FPSで収録されたものをffmpegにより25 FPSに変換して用いた．その後，手法\cite{bulat2017far}により動画に対してランドマーク検出を適用した\footnote{\url{https://github.com/1adrianb/face-alignment}}．このランドマークを利用することで口元のみを切り取り，画像サイズを$\lr{96, 96}$にリサイズした．モデル入力時は動画をグレースケールに変換し，各フレームに対する正規化および標準化を適用した．全体として，今回はAVHuBERTの転移学習を行うため，そこでの前処理に合わせている．学習時のデータ拡張は，ランダムクロップ，左右反転，Time Maskingを適用した．ランダムクロップは，$\lr{96, 96}$で与えられる画像から$\lr{88, 88}$をランダムに切り取る処理である．検証およびテスト時は，必ず画像中央を切り取るよう実装した．左右反転は，50\%の確率で左右が反転されるよう実装した．Time Maskingは，動画1秒あたり0から0.5秒の間でランダムに停止区間を定め，その区間における動画の時間方向平均値を計算し，区間内のすべてのフレームをこの平均値で置換した．これにより，動画が一時停止されるような効果が得られる．

音声データは16 kHzにダウンサンプリングして用いた．窓長25 msのハニング窓を用いてシフト幅10 msでSTFTを適用することで，フレームレート100 Hzのスペクトログラムに変換し，パワースペクトログラムに対して80次のメルフィルタバンクを適用した後，対数スケールに変換することで対数メルスペクトログラムを得た．また，Hi-Fi-CaptainとJVSには，ボコーダの学習安定化のため，無音区間のトリミング（-40 dBFS未満かつ500 ms継続する区間を100 msまでカット）を適用した．

モデルへの入力とした話者ベクトルは，動画音声データセット，Hi-Fi-Captain，JVSともに，各話者に対し学習データの中から100文章をランダムサンプリングし，各発話に対して得られたベクトルの平均値を用いた．これを学習・検証・テストで一貫して用いるため，検証データやテストデータには非依存な値となっている．

HuBERT離散特徴量の計算に利用するHuBERT Transformer層出力は，8層目出力を利用した．HuBERTの層ごとの特徴量について，音素のOne-hotベクトルおよび単語のOne-hotベクトルとの相関をCanonical Correlation Analysis（CCA）によって調べた先行研究\cite{pasad2023comparative}より，8層目出力がそのどちらとも相関が高いことが示されている．本実験では，HuBERT離散特徴量は言語的な情報を持つものとして扱いたかったため，この層からの出力を利用した．k-means法のクラスタ数は100とし，動画音声データセットの学習用データを利用して学習した後，全データセットに対してクラスタリングを実施した．また，学習時はゼロパディングされる区間のためにクラスを一つ追加したため，合計101クラスとして扱った．

\subsubsection{本実験で利用した事前学習済みモデルについて}
話者ベクトルの計算に用いた話者識別モデルの事前学習済み重みには，VoxCeleb1\cite{nagrani2020voxceleb}とVoxCeleb2，LibriSpeech\cite{panayotov2015librispeech}のotherセットで学習されたものを用いた\footnote{\url{https://github.com/resemble-ai/Resemblyzer}}．学習データセットの記述は，このモデルを利用して複数話者TTSを検討した先行研究\cite{jia2018transfer}のGitHubリポジトリにある\footnote{\url{https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training}}．話者識別モデルは，音声波形をメルスペクトログラムに変換した後，これを1.6秒ごとに0.77秒のオーバーラップを持つよう分割し，各区間ごとに一つのベクトル表現を獲得した後，区間ごとの出力ベクトルを平均して正規化することで最終出力を得る．モデル構造は3層のLSTMと全結合層からなり，得られる話者ベクトルの次元は256次元である．

AVHuBERTの事前学習済み重みには，LRS3とVoxCeleb2の英語データを利用し，データ拡張として音声データにノイズを付与した場合\cite{shi2022robust}の重みを利用した\footnote{\url{https://github.com/facebookresearch/av_hubert}}．脚注のリンク先では，「Model: Noise-Augmented AV-HuBERT Base」，「Pretraining Data: LRS3 + VoxCeleb2 (En)」，「Finetuning Data: No finetuning」と示されている．ノイズ付与の場合を利用した理由は，音声にノイズが含まれることで動画からの特徴抽出が促進され，より動画タスクに適した重みになっているのではないかと考えたからである．AVHuBERTは3次元畳み込み層と2次元畳み込み層を中心としたResNetと，12層のTransformer層から構成される．ResNetによって形状が$\lr{\dimUpper \times \timeUpper \times \widthUpper \times \heightUpper}$である動画からの特徴抽出および空間情報の圧縮が行われ，出力特徴量の形状は$\lr{\dimUpper \times \timeUpper}$となる．これに対してTransformer層を適用することで，時系列全体を考慮した特徴抽出を行う．最終的に得られる特徴量は768次元25 Hzである．

HuBERTの事前学習済み重みには，HuggingFaceに公開されているReazonSpeechによって学習されたもの\cite{rinna-japanese-hubert-base,sawada2024release}を利用した\footnote{\url{https://huggingface.co/rinna/japanese-hubert-base}}．ReazonSpeechは約19000時間の日本語音声からなるデータセットであり，今回用いる動画音声データセットが日本語であることから，このモデルが検討対象に適すると判断した．HuBERTは一次元畳み込み層を中心とした畳み込みエンコーダと，12層のTransformer層から構成される．畳み込みエンコーダによって音声波形の系列長を削減しつつ次元を上げた後，Transformer層を適用することで，時系列全体を考慮した特徴抽出を行う．最終的に得られる特徴量は768次元50 Hzである．

\subsubsection{本実験で独自に構築したモデルについて}
本実験で独自に構築したモデルは，ネットワークAとBにおける$\myNetworkPost$と，ボコーダ$\vocoder$である．

$\myNetworkPost$について，ConvBlockにおける畳み込み層の次元は768次元とし，カーネルサイズは3とした．Dropoutは$p = 0.1$として用いた．ConvBlockから構成されるResBlockは3層積み重ねた．

ボコーダについて，メルスペクトログラムの前処理層$\vocoderPreMel$では，入力される80次元100 Hzのメルスペクトログラムに対し，時間方向に隣接した2フレームを次元方向に積むことで160次元50 Hzの特徴量に変換し，全結合層を適用して128次元まで次元を削減した．HuBERT離散特徴量の前処理層$\vocoderPreHub$では，初めに各時刻$t$におけるロジットに対して$\argmax$を適用することで，最も確率の高いクラスを選択する．ここで，学習時など原音声から計算されたインデックス系列が初めから入力される場合は，この処理をスキップする．その後，各時刻$t$におけるインデックスを128次元のベクトルに変換することで，128次元50 Hzの特徴量が得られる．前処理後の二つの特徴量を次元方向に結合することで，256次元50 Hzの特徴量が得られる．次に，メインの処理層$\vocoderMain$では，初めにカーネルサイズ7の畳み込み層により，次元を1024次元まで拡大する．これに対し，転置畳み込み層によるアップサンプリングと複数種類の畳み込み層による特徴抽出を繰り返し行うことで，16 kHzの特徴量を獲得する．各UpsamplingBlockにおけるパラメータを表\ref{sec4:tab:vocoder_main_params}に示す．ここで，$K$はカーネルサイズ，$S$はストライド，$R$はダイレーション，$D$は次元，$T$は系列長である．畳み込み層についてはカーネルサイズとダイレーションを集合として表記しているが，実際はこれらの直積の元，すなわち$\lr{3, 1}$や$\lr{3, 3}$，$\lr{3, 5}$をパラメータとする層が存在することを表す．すなわち，転置畳み込み層一層に対し，その後の特徴量抽出は15種類の異なるカーネルサイズ，ダイレーションを設定した畳み込み層によって行われる．最後に，16次元16 kHzの特徴量を畳み込み層に通して1次元まで次元を削減することで，16 kHzの音声波形が得られる．
\begin{table}[bt]
    \centering
    \caption{$\vocoderMain$の各UpsamplingBlockにおけるパラメータ}
    \label{sec4:tab:vocoder_main_params}
    \begin{center}
        \renewcommand{\arraystretch}{0.9}
        \setlength{\tabcolsep}{8pt}
        \scalebox{1.0}{
            \begin{tabular}{|c|c|c|c|}
                \hline
                  & \multicolumn{1}{c|}{転置畳み込み層 $\lr{\kernelSizeUpper, \strideUpper}$} & \multicolumn{1}{c|}{畳み込み層 $\lr{\kernelSizeUpper, \dilationUpper}$} & \multicolumn{1}{c|}{出力特徴量の形状 $\lr{\dimUpper, \timeUpper}$} \\
                \hline
                1 & $\lr{11, 5}$                                                       & $\lr{\lrc{3, 5, 7, 9, 11}, \lrc{1, 3, 5}}$                         & $\lr{512, 250}$                                            \\
                2 & $\lr{8, 4}$                                                        & $\lr{\lrc{3, 5, 7, 9, 11}, \lrc{1, 3, 5}}$                         & $\lr{256, 1000}$                                           \\
                3 & $\lr{4, 2}$                                                        & $\lr{\lrc{3, 5, 7, 9, 11}, \lrc{1, 3, 5}}$                         & $\lr{128, 2000}$                                           \\
                4 & $\lr{4, 2}$                                                        & $\lr{\lrc{3, 5, 7, 9, 11}, \lrc{1, 3, 5}}$                         & $\lr{64, 4000}$                                            \\
                5 & $\lr{4, 2}$                                                        & $\lr{\lrc{3, 5, 7, 9, 11}, \lrc{1, 3, 5}}$                         & $\lr{32, 8000}$                                            \\
                6 & $\lr{4, 2}$                                                        & $\lr{\lrc{3, 5, 7, 9, 11}, \lrc{1, 3, 5}}$                         & $\lr{16, 16000}$                                           \\
                \hline
            \end{tabular}
        }
    \end{center}
\end{table}

\subsubsection{学習方法}
ネットワークAについて，最適化手法はAdamW\cite{loshchilov2017decoupled}を利用し，$\beta_{1} = 0.9, \beta_{2} = 0.98, \lambda = 0.01$とした．スケジューラはCosine Annealing with Warmupを利用し，$\learningRate_{\text{min}} = 1.0 \times 10^{-6}, \learningRate_{\text{max}} = 1.0 \times 10^{-3}, \text{WarmupSteps} = 5, \epoch_{\text{max}} = 50$とした．バッチサイズは4とし，8イテレーションに一回重みを更新するようGradient Accumulationを適用した．モデルに入力する動画の秒数は10秒を上限とし，10秒を超える場合はランダムにトリミング，10秒未満の場合はゼロパディングした．また，ゼロパディングした部分は損失の計算から除外した．勾配のノルムは3.0を上限としてクリッピングした．10エポック連続して検証データに対する損失関数$\lossA$の値が小さくならない場合には学習を中断するようにし（Early Stopping），学習終了時には検証データに対する損失が最も小さかったエポックにおけるチェックポイントを保存して，これをテストデータに対する評価に用いた．$\lossA$の重み係数は，$\lossWeightMel = 1.0, \lossWeightHubInt = 1.0$に固定した上で，$\lossWeightHubDisc$を$0.0001, 0.001, 0.01, 0.1, 1.0$の五段階でグリッドサーチした．

ネットワークBについて，ここではネットワークAの重みは固定した．最適化手法はAdamWを利用し，$\beta_{1} = 0.9, \beta_{2} = 0.98, \lambda = 0.01$とした．スケジューラはCosine Annealing with Warmupを利用し，$\learningRate_{\text{min}} = 1.0 \times 10^{-6}, \learningRate_{\text{max}} = 5.0 \times 10^{-4}, \text{WarmupSteps} = 5, \epoch_{\text{max}} = 50$とした．$\learningRate_{\text{max}}$をネットワークAから半減したのは，学習の安定化のためである．その他の設定はネットワークAと同様で，Early Stoppingにおいて監視したのは検証データに対する$\lossFuncUpper_{B}$の値である．$\lossB$の重み係数は，$\lossWeightMel = 1.0$に固定した上で，$\lossWeightHubDisc$を$0.0001, 0.001, 0.01, 0.1, 1.0$の五段階でグリッドサーチした．ただし，ネットワークBにおいて用いるネットワークAの学習済み重みは，等しい$\lossWeightHubDisc$で学習されたものとした．例えば，ネットワークBにおいて$\lossWeightHubDisc = 0.0001$で学習させる時，ネットワークAも$\lossWeightHubDisc = 0.0001$で学習されたものを用いた．

ボコーダについて，ここでははじめにHi-Fi-Captainのみを用いて学習させ，その後JVSによって再学習した．最適化手法はAdamWを利用し，$\beta_{1} = 0.8$，$\beta_{2} = 0.99$，$\lambda = \num{1.0e-5}$とした．スケジューラはExponentialLRSchedulerを利用し，$\gamma = 0.99$とした．また，最大エポック数は30とした．バッチサイズは16とした．モデルへの入力は1秒を上限とし，1秒を超える場合はランダムにトリミング，1秒未満の場合はゼロパディングした．ここではEarly Stoppingは適用せず，学習終了時に検証データに対する損失（メルスペクトログラムに対するMAE Loss）が最も小さかったエポックにおけるチェックポイントを保存し，これをテストデータに対する評価に用いた．
% また，先行研究\cite{choi2023intelligible}においては学習時に，メルスペクトログラムにノイズを付与するデータ拡張手法が提案されている．本研究では，動画から推定されるメルスペクトログラムとHuBERT離散特徴量の推定精度向上に焦点を当てたため，ボコーダの学習は原音声から計算される特徴量で行い，ボコーダ自体の汎化性能向上による精度改善は追求しなかった．

実装に用いた深層学習ライブラリはPyTorchおよびPyTorch Lightningである．GPUにはNVIDIA RTX A4000を利用し，計算の高速化のためAutomatic Mixed Precisionを適用した．

\subsubsection{客観評価}
合成音声の客観評価には，二種類の指標を用いた．
一つ目は，音声認識の結果から算出した単語誤り率（Word Error Rate; WER）である．WERの計算方法について，まず，正解文字列$s_{1}$と音声認識モデルによる予測文字列$s_{2}$に対し，レーベンシュタイン距離によってその差分を測る．レーベンシュタイン距離は，二つの文字列を一致させるために必要なトークンの挿入数$I$，削除数$D$，置換数$R$の和の最小値として定義される．WERは，レーベンシュタイン距離を測ることによって得られた$I, D, R$を利用し，
\begin{equation}
    \text{WER}\lr{s_{1}, s_{2}} = \frac{I + D + R}{|s_{1}|}
\end{equation}
で与えられる．ここで，$|s_{1}|$は正解文字列$s_{1}$のトークン数を表す．実際には，音声認識モデルにWhisper\cite{radford2023robust}を利用し\footnote{\url{https://github.com/openai/whisper}}，出力される漢字仮名交じり文に対してMeCabを用いて分かち書きを行った上で，jiwerというライブラリを用いて算出した．WhisperはLargeモデルを利用し，MeCabの辞書にはunidicを利用した．WERの値は0\%以上であり，この値が低いほど音声認識の誤りが少ないため，より聞き取りやすい音声であると判断した．

二つ目は，話者ベクトルから計算したコサイン類似度である．モデルに入力する話者ベクトルを計算するのに用いた話者識別モデルを利用して，各サンプルごとに合成音声の話者ベクトルと原音声の話者ベクトルを計算し，これらのコサイン類似度を計算した．今回構築するモデルは4人の話者に対応するモデルとなるため，原音声に似た声質の合成音声が得られているかをこの指標で評価した．値は$-1$以上$1$以下であり，高いほど原音声と似た合成音声だと判断した．

\subsubsection{主観評価}
\label{sec4:sec:sbj_explanation}
合成音声の主観評価では，音声の明瞭性と類似性の二点を評価した．今回はクラウドワークスというクラウドソーシングサービスおよび，自作の実験用Webサイトを利用してオンラインで実験を実施した．被験者の条件は，
\begin{enumerate}
    \item 日本語話者である方
    \item 聴覚に異常のない方
    \item イヤホンあるいはヘッドホンでの実施が可能な方
    \item 静かな環境での実施が可能な方
    \item 以前に行った主観評価実験に参加されなかった方
\end{enumerate}
とした，実験項目は，
\begin{enumerate}
    \item アンケート
    \item 練習試行（明瞭性）
    \item 本番試行（明瞭性）
    \item 練習試行（類似性）
    \item 本番試行（類似性）
\end{enumerate}
である．

一つ目のアンケートでは，被験者についての基本的な統計を取ることを目的として，性別・年齢・実験に利用した音響機器について回答してもらった．性別は，男性，女性，無回答の三つからの選択式とした．年齢は被験者の方に直接数値を入力してもらう形式とした．実験に利用した音響機器は，イヤホン，ヘッドホンの二つからの選択式とした．

二つ目の練習試行（明瞭性）および三つ目の本番試行（明瞭性）では，音声の明瞭性の評価を実施した．初めに練習試行で実験内容を把握してもらい，その後本番施行を行う流れとした．練習施行は何度でも実施可能とし，本番試行は一回のみ実施可能とした．評価項目について，明瞭性は「話者の意図した発話内容を，一回の発話でどの程度聞き取ることができたか」を評価するものとした．実際の評価プロセスは以下の三段階で構成した．
\begin{enumerate}
    \item 提示された音声サンプルを一回再生し，発話内容を聞き取る．
    \item 「発話内容を表示」ボタンを押し、本来の発話内容を表示する。
    \item 聴取者が想定していた発話内容と本来の発話内容を照らし合わせ，音声の聞き取りやすさを5段階評価する．
\end{enumerate}
5段階評価の回答項目は以下のようにした．
\begin{enumerate}
    \item 全く聞き取れなかった
    \item ほとんど聞き取れなかった
    \item ある程度聞き取れた
    \item ほとんど聞き取れた
    \item 完全に聞き取れた
\end{enumerate}
実験に利用した音声サンプルについて，練習試行では検証データであるATR音素バランス文のIセット，本番試行ではテストデータであるATR音素バランス文のJセットを用いた．被験者ごとの評価サンプルの割り当て方法をアルゴリズム\ref{sec4:algo:sample-assignment-int}に示す．$\text{sentences}$は文章のリスト，$\text{methods}$が手法のリスト，$\text{speakers}$が話者のリスト，$\numUpper_{\text{p}}$が被験者総数である．各被験者について，初めに$\text{sentences}$と$\text{methods}$をランダムにシャッフルし，それから順に$\text{sentence}, \text{method}, \text{speaker}$を決めて，対応した音声サンプルを選択した．この方法では，二つのことに注意した．一つ目は，各被験者がユニークな発話文章を評価することである．各音声サンプルの評価の際に本来の発話内容が分かるため，これを知った上で同じ発話内容のサンプルを評価すると，音声自体の明瞭性に関わらず発話内容がわかってしまい，評価に影響を与える可能性がある．これを避けるため，評価サンプルは全て異なる発話内容にした．二つ目は，各手法がなるべく均等な回数出現することである．今回の実験は手法の比較を目的としたため，各被験者がすべての手法をなるべく均等な回数評価できるようにした．また，評価時に音声サンプルを一回だけ聞けるようにしたことについて，代用音声をコミュニケーションツールとして利用する場面を想定したとき，会話において何度も聞き返されることは利用者のストレスになると考えられる．よって，本実験では一回の発話で意図した発話内容をどの程度聞き取ってもらえるかを聞き取りやすさとして評価したいと考え，この制限を設けた．
\begin{algorithm}
    \caption{被験者に対する評価サンプル割り当て方法（明瞭性）}
    \label{sec4:algo:sample-assignment-int}
    \begin{algorithmic}[1]
        \State \textbf{Input:} $\text{sentences, methods, speakers}, \numUpper_{\text{p}}$
        \State $\text{allAssignments} = \{\}$
        \For{$\text{participantId} = 0 \text{ to } \numUpper_{\text{p}} - 1$}
        \State $\text{assignments} = []$
        \State $\text{Randomly shuffle sentences}$
        \State $\text{Randomly shuffle methods}$
        \For{$i = 0 \text{ to } \text{len}\lr{\text{sentences}} - 1$}
        \State $\text{sentence} = \text{sentences}\lrsq{i}$
        \State $\text{method} = \text{methods}\lrsq{i \bmod \text{len}\lr{\text{methods}}}$
        \State $\text{speaker} = \text{Randomly select from speakers}$
        \State $\text{Append } \lrsq{\text{sentence}, \text{method}, \text{speaker}} \text{ to assignments}$
        \EndFor
        \State $\text{allAssignments}\lrsq{\text{participantId}} = \text{assignments}$
        \EndFor
        \State \Return $\text{allAssignments}$
    \end{algorithmic}
\end{algorithm}

四つ目の練習試行（類似性）および五つ目の本番試行（類似性）では，評価対象の音声と同一話者の原音声の類似性の評価を実施した．初めに練習試行で実験内容を把握してもらい，その後本番施行を行う流れとした．練習施行は何度でも実施可能とし，本番試行は一回のみ実施可能とした．評価項目について，類似性は「評価対象の音声が同一話者の原音声とどれくらい似ているか」を評価するものとした．実際の評価プロセスは以下の二段階で構成した．
\begin{enumerate}
    \item 評価対象の音声と原音声を聞き比べる．
    \item 評価対象の音声が原音声にどれくらい似ていたかを五段階評価する．
\end{enumerate}
5段階評価の回答項目は以下のようにした．
\begin{enumerate}
    \item 全く似ていなかった
    \item あまり似ていなかった
    \item やや似ていた
    \item かなり似ていた
    \item 同じ話者に聞こえた
\end{enumerate}
実験に利用した音声サンプルは明瞭性評価と同じである．被験者ごとの評価サンプルの割り当て方法をアルゴリズム\ref{sec4:algo:sample-assignment-sim}に示す．明瞭性実験と概ね同様であるが，評価サンプル用の$\text{sentenceEval}$に対して比較対象となる原音声用の$\text{sentenceGT}$が追加された点が異なる．類似性評価は発話内容に依存しない評価だと考えて，$\text{sentenceGT}$はランダムに選択した．また，類似性評価では音声サンプルを何度でも聞けるようにした．明瞭性におけるコミュニケーションを想定した評価と異なり，単に原音声とどの程度似ているかを評価したいと考えたからである．ここで，類似性評価を明瞭性評価の後に行うようにした理由は，類似性評価と明瞭性評価に用いるデータが同一だったからである．類似性評価は発話内容に依存しない評価だと考えて，明瞭性評価を完了し，発話内容を知った上で評価しても問題ないと判断した．
\begin{algorithm}
    \caption{被験者に対する評価サンプル割り当て方法（類似性）}
    \label{sec4:algo:sample-assignment-sim}
    \begin{algorithmic}[1]
        \State \textbf{Input:} $\text{sentences, methods, speakers}, \numUpper_{\text{p}}$
        \State $\text{allAssignments} = \{\}$
        \For{$\text{participantId} = 0 \text{ to } \numUpper_{\text{p}} - 1$}
        \State $\text{assignments} = []$
        \State $\text{Randomly shuffle sentences}$
        \State $\text{Randomly shuffle methods}$
        \For{$i = 0 \text{ to } \text{len}\lr{\text{sentences}} - 1$}
        \State $\text{sentenceEval} = \text{sentences}\lrsq{i}$
        \State $\text{method} = \text{methods}\lrsq{i \bmod \text{len}\lr{\text{methods}}}$
        \State $\text{speaker} = \text{Randomly select from speakers}$
        \State $\text{sentenceGT} = \text{Randomly select from sentences}$
        \State $\text{Append } \lrsq{\text{sentenceEval}, \text{method}, \text{speaker}, \text{sentenceGT}} \text{ to assignments}$
        \EndFor
        \State $\text{allAssignments}\lrsq{\text{participantId}} = \text{assignments}$
        \EndFor
        \State \Return $\text{allAssignments}$
    \end{algorithmic}
\end{algorithm}

また，オンラインでの評価は効率よく数多くの方に評価していただけるという点でメリットがあるが，オフラインでの評価と比較して実験環境を制御することが難しく，評価品質が低下する恐れがある．これに対して，本実験では先行研究\cite{kirkland2023stuck}を参考に，評価サンプル中にダミー音声を混入させることで対策を講じた．ダミー音声は本研究で得られた合成音声とは無関係に，gTTSというライブラリを用いて生成したサンプルである．明瞭性評価では
\begin{quote}
    これはダミー音声です．明瞭性は「3: ある程度聞き取れた」を選択してください．
\end{quote}
のような発話内容の音声を，類似性評価では
\begin{quote}
    これはダミー音声です．類似性は「3: やや似ていた」を選択してください．
\end{quote}
のような発話内容の音声を提示した．「3: ある程度聞き取れた」や「3: やや似ていた」の部分は，5段階評価の回答項目からランダムに選択した．被験者には，ダミー音声が提示された場合はその音声自体の明瞭性や類似性とは無関係に，必ず音声で指定された評価値を選択するよう伝えた．練習試行では一つ、本番試行では二つ混入させ、本番試行でダミー音声に対する評価値を誤った被験者については，適当に回答した恐れがあると判断し，最終的な統計処理には用いなかった．

総被験者数は75人とし，謝礼は実験一回あたり40分程度要すると考えて，750円とした．